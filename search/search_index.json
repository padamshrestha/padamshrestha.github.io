{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83c\udfe1 Welcome \u00b6 Welcome to my blog. It's a collection of tutorials, how tos and ideas from everyday while solving problem at work. Blog post/content are grouped into following categories: \u00b6 Dotnet Web API Azure Functions Kusto AKS Azure Cloud Angular React Python Tip Try searching the post your are looking for using key /","title":"Home"},{"location":"#welcome","text":"Welcome to my blog. It's a collection of tutorials, how tos and ideas from everyday while solving problem at work.","title":"\ud83c\udfe1 Welcome"},{"location":"#blog-postcontent-are-grouped-into-following-categories","text":"Dotnet Web API Azure Functions Kusto AKS Azure Cloud Angular React Python Tip Try searching the post your are looking for using key /","title":"Blog post/content are grouped into following categories:"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: git \u00b6 Create Git PAT token with limited access. Multiple remote git configuration for repository","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#git","text":"Create Git PAT token with limited access. Multiple remote git configuration for repository","title":"git"},{"location":"about/","text":"About \u00b6 Thanks for visiting my notes \u00b6 You can find all my notes here...","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#thanks-for-visiting-my-notes","text":"You can find all my notes here...","title":"Thanks for visiting my notes"},{"location":"azure/ev2-arm-deployment/","text":"Ev2 ARM Template Deployment for MS \u00b6 1. Create Azure Function App \u00b6 Powershell Azure Function Developer Guide 2. Create ARM Template \u00b6 3. Create EV2 template for deployment \u00b6 Resources: Ev2 Quickstart : Clone the repo and run the rollout which should deploy sample node app to WestUS and EastUS with 3 mins wait policy from rollout spec EV2 Cmdlet : Use AzureServiceDeployClient shortcut from the package to execute rollout Validate rollout spec at ARM Template Visualizer Rollout command PowerShell .\\ AzureServiceDeployQuickStart . ps1 -subscriptionId \"568651df-cbff-4e65-8206-b0982bafb4db\" -resourceGroupNamePrefix \"testquickdemo\" -email \"v-padshr@microsoft.com\" Operating on subscription 568651df-cbff - 4e65 - 8206-b0982bafb4db Delete Resource group PowerShell Remove-AzResourceGroup -Name testquickdemoContosoWest Remove-AzResourceGroup -Name testquickdemoContosoEast","title":"Ev2 ARM Template Deployment for MS"},{"location":"azure/ev2-arm-deployment/#ev2-arm-template-deployment-for-ms","text":"","title":"Ev2 ARM Template Deployment for MS"},{"location":"azure/ev2-arm-deployment/#1-create-azure-function-app","text":"Powershell Azure Function Developer Guide","title":"1. Create Azure Function App"},{"location":"azure/ev2-arm-deployment/#2-create-arm-template","text":"","title":"2. Create ARM Template"},{"location":"azure/ev2-arm-deployment/#3-create-ev2-template-for-deployment","text":"Resources: Ev2 Quickstart : Clone the repo and run the rollout which should deploy sample node app to WestUS and EastUS with 3 mins wait policy from rollout spec EV2 Cmdlet : Use AzureServiceDeployClient shortcut from the package to execute rollout Validate rollout spec at ARM Template Visualizer Rollout command PowerShell .\\ AzureServiceDeployQuickStart . ps1 -subscriptionId \"568651df-cbff-4e65-8206-b0982bafb4db\" -resourceGroupNamePrefix \"testquickdemo\" -email \"v-padshr@microsoft.com\" Operating on subscription 568651df-cbff - 4e65 - 8206-b0982bafb4db Delete Resource group PowerShell Remove-AzResourceGroup -Name testquickdemoContosoWest Remove-AzResourceGroup -Name testquickdemoContosoEast","title":"3. Create EV2 template for deployment"},{"location":"azure/links/","text":"\ud83d\udd17 Azure links \u00b6 Export App Insight query data","title":"\ud83d\udd17 Azure links"},{"location":"azure/links/#azure-links","text":"Export App Insight query data","title":"\ud83d\udd17 Azure links"},{"location":"courses-projects/coursera/","text":"Coursera Courses \u00b6 Machine Learning Specialization - Stanford | DeepLearning.AI \u00b6 Supervised Machine Learning: Regression and Classification Advanced Learning Algorithms Unsupervised Learning, Recommenders, Reinforcement Learning Deep Learning Specialization - DeepLearning.AI \u00b6 Neural Networks and Deep Learning Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization Structuring Machine Learning Projects Convolutional Neural Networks Sequence Models DeepLearning.AI \u00b6 Sequences, Time Series and Prediction AI \u00b6 Machine Learning with Apache Spark Generative AI with Large Language Models Machine Learning Operations (MLOps) with Vertex AI: Manage Features Introduction to Trading, Machine Learning & GCP Using Machine Learning in Trading and Finance Reinforcement Learning for Trading Strategies Applied AI with DeepLearning Projects \u00b6 Building Systems with the ChatGPT API LangChain Chat with Your Data Github \u00b6 Large Language Model Course The System Design Primer IBKR - Derivatives - Options & Futures - Microeconomics: The Power of Markets -","title":"Coursera Courses"},{"location":"courses-projects/coursera/#coursera-courses","text":"","title":"Coursera Courses"},{"location":"courses-projects/coursera/#machine-learning-specialization-stanford-deeplearningai","text":"Supervised Machine Learning: Regression and Classification Advanced Learning Algorithms Unsupervised Learning, Recommenders, Reinforcement Learning","title":"Machine Learning Specialization - Stanford | DeepLearning.AI"},{"location":"courses-projects/coursera/#deep-learning-specialization-deeplearningai","text":"Neural Networks and Deep Learning Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization Structuring Machine Learning Projects Convolutional Neural Networks Sequence Models","title":"Deep Learning Specialization - DeepLearning.AI"},{"location":"courses-projects/coursera/#deeplearningai","text":"Sequences, Time Series and Prediction","title":"DeepLearning.AI"},{"location":"courses-projects/coursera/#ai","text":"Machine Learning with Apache Spark Generative AI with Large Language Models Machine Learning Operations (MLOps) with Vertex AI: Manage Features Introduction to Trading, Machine Learning & GCP Using Machine Learning in Trading and Finance Reinforcement Learning for Trading Strategies Applied AI with DeepLearning","title":"AI"},{"location":"courses-projects/coursera/#projects","text":"Building Systems with the ChatGPT API LangChain Chat with Your Data","title":"Projects"},{"location":"courses-projects/coursera/#github","text":"Large Language Model Course The System Design Primer IBKR - Derivatives - Options & Futures - Microeconomics: The Power of Markets -","title":"Github"},{"location":"courses-projects/github_project/","text":"Github repo and projects \u00b6 Apple mlx \u00b6 Federated Learning :: :: / github / Flower Example using MLX voice assistant :: :: Re-implementation of GPT-2 with mlx Training/finetuning medium-sized GPTs Linear Regression using mlx MLX Transformer - Implementation of high performance transformer modules in MLX by apple : discord channel HopeNet (head pose estimation network) in mlx Llama v1 7B model implemented in MLX - Blog with video code Open AI \u00b6 Cookbook - Open AI blog Gemini \u00b6 open source implementation of Gemini GeminiProChat - Github project with docker support \u00b6 Deep Learning and Machine Learning for Stock Predictions | Lil'Log - Blog AI \u00b6 OpenAI Docs Hugging Face - Models, Datasets, Spaces, Docs Whisper asr web service Download whisper models Whisper cpp Desktop convert Whisper cpp llama cpp live transcription Auto-subtitle RAG-Survey : Retrieval-Augmented Generation Run GPT locally \u00b6 GPT PILOT - Creates app using GPT in Developer's supervision MetaGPT: The Multi-Agent Framework GPT-Engineer DB-GPT Unstructured Chatbot UI Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit crewAI : framework for orchestrating role-playing, autonomous AI agents Tools \u00b6 Playwright is a framework for Web Testing and Automation Spotube: Spotify client without premium movie-web: Watch movies Twenty: CRM - good for dockers and e2e applications","title":"Github repo and projects"},{"location":"courses-projects/github_project/#github-repo-and-projects","text":"","title":"Github repo and projects"},{"location":"courses-projects/github_project/#apple-mlx","text":"Federated Learning :: :: / github / Flower Example using MLX voice assistant :: :: Re-implementation of GPT-2 with mlx Training/finetuning medium-sized GPTs Linear Regression using mlx MLX Transformer - Implementation of high performance transformer modules in MLX by apple : discord channel HopeNet (head pose estimation network) in mlx Llama v1 7B model implemented in MLX - Blog with video code","title":"Apple mlx"},{"location":"courses-projects/github_project/#open-ai","text":"Cookbook - Open AI blog","title":"Open AI"},{"location":"courses-projects/github_project/#gemini","text":"open source implementation of Gemini GeminiProChat - Github project with docker support","title":"Gemini"},{"location":"courses-projects/github_project/#_1","text":"Deep Learning and Machine Learning for Stock Predictions | Lil'Log - Blog","title":""},{"location":"courses-projects/github_project/#ai","text":"OpenAI Docs Hugging Face - Models, Datasets, Spaces, Docs Whisper asr web service Download whisper models Whisper cpp Desktop convert Whisper cpp llama cpp live transcription Auto-subtitle RAG-Survey : Retrieval-Augmented Generation","title":"AI"},{"location":"courses-projects/github_project/#run-gpt-locally","text":"GPT PILOT - Creates app using GPT in Developer's supervision MetaGPT: The Multi-Agent Framework GPT-Engineer DB-GPT Unstructured Chatbot UI Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit crewAI : framework for orchestrating role-playing, autonomous AI agents","title":"Run GPT locally"},{"location":"courses-projects/github_project/#tools","text":"Playwright is a framework for Web Testing and Automation Spotube: Spotify client without premium movie-web: Watch movies Twenty: CRM - good for dockers and e2e applications","title":"Tools"},{"location":"csharp/csharp-10/","text":"What's new in C# 10 \u00b6 C# 10 adds the following features and enhancements to the C# language: What's new in C# 10 Record structs Improvements of structure types Interpolated string handler Global using directives File-scoped namespace declaration Extended property patterns Lambda expression improvements Constant interpolated strings Record types can seal ToString Assignment and declaration in same deconstruction Improved definite assignment Allow AsyncMethodBuilder attribute on methods CallerArgumentExpression attribute diagnostics Enhanced #line pragma C# 10 is supported on .NET 6 . For more information, see C# language versioning . You can download the latest .NET 6 SDK from the .NET downloads page . You can also download Visual Studio 2022 , which includes the .NET 6 SDK. Record structs \u00b6 You can declare value type records using the record struct or readonly record struct declarations . You can now clarify that a record is a reference type with the record class declaration. Improvements of structure types \u00b6 C# 10 introduces the following improvements related to structure types: You can declare an instance parameterless constructor in a structure type and initialize an instance field or property at its declaration. For more information, see the Struct initialization and default values section of the Structure types article. A left-hand operand of the with expression can be of any structure type or an anonymous (reference) type. Interpolated string handler \u00b6 You can create a type that builds the resulting string from an interpolated string expression . The .NET libraries use this feature in many APIs. You can build one by following this tutorial . Global using directives \u00b6 You can add the global modifier to any using directive to instruct the compiler that the directive applies to all source files in the compilation. This is typically all source files in a project. File-scoped namespace declaration \u00b6 You can use a new form of the namespace declaration to declare that all declarations that follow are members of the declared namespace: C# namespace MyNamespace ; This new syntax saves both horizontal and vertical space for namespace declarations. Extended property patterns \u00b6 Beginning with C# 10, you can reference nested properties or fields within a property pattern. For example, a pattern of the form C# { Prop1 . Prop2 : pattern } is valid in C# 10 and later and equivalent to C# { Prop1 : { Prop2 : pattern } } valid in C# 8.0 and later. For more information, see the Extended property patterns feature proposal note. For more information about a property pattern, see the Property pattern section of the Patterns article. Lambda expression improvements \u00b6 C# 10 includes many improvements to how lambda expressions are handled: Lambda expressions may have a natural type , where the compiler can infer a delegate type from the lambda expression or method group. Lambda expressions may declare a return type when the compiler can't infer it. Attributes can be applied to lambda expressions. These features make lambda expressions more similar to methods and local functions. They make it easier to use lambda expressions without declaring a variable of a delegate type, and they work more seamlessly with the new ASP.NET Core Minimal APIs. Constant interpolated strings \u00b6 In C# 10, const strings may be initialized using string interpolation if all the placeholders are themselves constant strings. String interpolation can create more readable constant strings as you build constant strings used in your application. The placeholder expressions can't be numeric constants because those constants are converted to strings at run time. The current culture may affect their string representation. Learn more in the language reference on const expressions . Record types can seal ToString \u00b6 In C# 10, you can add the sealed modifier when you override ToString in a record type. Sealing the ToString method prevents the compiler from synthesizing a ToString method for any derived record types. A sealed ToString ensures all derived record types use the ToString method defined in a common base record type. You can learn more about this feature in the article on records . Assignment and declaration in same deconstruction \u00b6 This change removes a restriction from earlier versions of C#. Previously, a deconstruction could assign all values to existing variables, or initialize newly declared variables: C# // Initialization: ( int x , int y ) = point ; // assignment: int x1 = 0 ; int y1 = 0 ; ( x1 , y1 ) = point ; C# 10 removes this restriction: C# int x = 0 ; ( x , int y ) = point ; Improved definite assignment \u00b6 Prior to C# 10, there were many scenarios where definite assignment and null-state analysis produced warnings that were false positives. These generally involved comparisons to boolean constants, accessing a variable only in the true or false statements in an if statement, and null coalescing expressions. These examples generated warnings in previous versions of C#, but don't in C# 10: C# string representation = \"N/A\" ; if (( c != null && c . GetDependentValue ( out object obj )) == true ) { representation = obj . ToString (); // undesired error } // Or, using ?. if ( c ?. GetDependentValue ( out object obj ) == true ) { representation = obj . ToString (); // undesired error } // Or, using ?? if ( c ?. GetDependentValue ( out object obj ) ?? false ) { representation = obj . ToString (); // undesired error } The main impact of this improvement is that the warnings for definite assignment and null-state analysis are more accurate. Allow AsyncMethodBuilder attribute on methods \u00b6 In C# 10 and later, you can specify a different async method builder for a single method, in addition to specifying the method builder type for all methods that return a given task-like type. A custom async method builder enables advanced performance tuning scenarios where a given method may benefit from a custom builder. To learn more, see the section on AsyncMethodBuilder in the article on attributes read by the compiler. CallerArgumentExpression attribute diagnostics \u00b6 You can use the to specify a parameter that the compiler replaces with the text representation of another argument. This feature enables libraries to create more specific diagnostics. The following code tests a condition. If the condition is false, the exception message contains the text representation of the argument passed to condition : C# public static void Validate ( bool condition , [ CallerArgumentExpression ( \"condition\" )] string? message = null ) { if ( ! condition ) { throw new InvalidOperationException ( $\"Argument failed validation: <{message}>\" ); } } You can learn more about this feature in the article on Caller information attributes in the language reference section. Enhanced #line pragma \u00b6 C# 10 supports a new format for the #line pragma. You likely won't use the new format, but you'll see its effects. The enhancements enable more fine-grained output in domain-specific languages (DSLs) like Razor. The Razor engine uses these enhancements to improve the debugging experience. You'll find debuggers can highlight your Razor source more accurately. To learn more about the new syntax, see the article on Preprocessor directives in the language reference. You can also read the feature specification for Razor based examples.","title":"What's new in C# 10 - C# Guide"},{"location":"csharp/csharp-10/#whats-new-in-c-10","text":"C# 10 adds the following features and enhancements to the C# language: What's new in C# 10 Record structs Improvements of structure types Interpolated string handler Global using directives File-scoped namespace declaration Extended property patterns Lambda expression improvements Constant interpolated strings Record types can seal ToString Assignment and declaration in same deconstruction Improved definite assignment Allow AsyncMethodBuilder attribute on methods CallerArgumentExpression attribute diagnostics Enhanced #line pragma C# 10 is supported on .NET 6 . For more information, see C# language versioning . You can download the latest .NET 6 SDK from the .NET downloads page . You can also download Visual Studio 2022 , which includes the .NET 6 SDK.","title":"What's new in C# 10"},{"location":"csharp/csharp-10/#record-structs","text":"You can declare value type records using the record struct or readonly record struct declarations . You can now clarify that a record is a reference type with the record class declaration.","title":"Record structs"},{"location":"csharp/csharp-10/#improvements-of-structure-types","text":"C# 10 introduces the following improvements related to structure types: You can declare an instance parameterless constructor in a structure type and initialize an instance field or property at its declaration. For more information, see the Struct initialization and default values section of the Structure types article. A left-hand operand of the with expression can be of any structure type or an anonymous (reference) type.","title":"Improvements of structure types"},{"location":"csharp/csharp-10/#interpolated-string-handler","text":"You can create a type that builds the resulting string from an interpolated string expression . The .NET libraries use this feature in many APIs. You can build one by following this tutorial .","title":"Interpolated string handler"},{"location":"csharp/csharp-10/#global-using-directives","text":"You can add the global modifier to any using directive to instruct the compiler that the directive applies to all source files in the compilation. This is typically all source files in a project.","title":"Global using directives"},{"location":"csharp/csharp-10/#file-scoped-namespace-declaration","text":"You can use a new form of the namespace declaration to declare that all declarations that follow are members of the declared namespace: C# namespace MyNamespace ; This new syntax saves both horizontal and vertical space for namespace declarations.","title":"File-scoped namespace declaration"},{"location":"csharp/csharp-10/#extended-property-patterns","text":"Beginning with C# 10, you can reference nested properties or fields within a property pattern. For example, a pattern of the form C# { Prop1 . Prop2 : pattern } is valid in C# 10 and later and equivalent to C# { Prop1 : { Prop2 : pattern } } valid in C# 8.0 and later. For more information, see the Extended property patterns feature proposal note. For more information about a property pattern, see the Property pattern section of the Patterns article.","title":"Extended property patterns"},{"location":"csharp/csharp-10/#lambda-expression-improvements","text":"C# 10 includes many improvements to how lambda expressions are handled: Lambda expressions may have a natural type , where the compiler can infer a delegate type from the lambda expression or method group. Lambda expressions may declare a return type when the compiler can't infer it. Attributes can be applied to lambda expressions. These features make lambda expressions more similar to methods and local functions. They make it easier to use lambda expressions without declaring a variable of a delegate type, and they work more seamlessly with the new ASP.NET Core Minimal APIs.","title":"Lambda expression improvements"},{"location":"csharp/csharp-10/#constant-interpolated-strings","text":"In C# 10, const strings may be initialized using string interpolation if all the placeholders are themselves constant strings. String interpolation can create more readable constant strings as you build constant strings used in your application. The placeholder expressions can't be numeric constants because those constants are converted to strings at run time. The current culture may affect their string representation. Learn more in the language reference on const expressions .","title":"Constant interpolated strings"},{"location":"csharp/csharp-10/#record-types-can-seal-tostring","text":"In C# 10, you can add the sealed modifier when you override ToString in a record type. Sealing the ToString method prevents the compiler from synthesizing a ToString method for any derived record types. A sealed ToString ensures all derived record types use the ToString method defined in a common base record type. You can learn more about this feature in the article on records .","title":"Record types can seal ToString"},{"location":"csharp/csharp-10/#assignment-and-declaration-in-same-deconstruction","text":"This change removes a restriction from earlier versions of C#. Previously, a deconstruction could assign all values to existing variables, or initialize newly declared variables: C# // Initialization: ( int x , int y ) = point ; // assignment: int x1 = 0 ; int y1 = 0 ; ( x1 , y1 ) = point ; C# 10 removes this restriction: C# int x = 0 ; ( x , int y ) = point ;","title":"Assignment and declaration in same deconstruction"},{"location":"csharp/csharp-10/#improved-definite-assignment","text":"Prior to C# 10, there were many scenarios where definite assignment and null-state analysis produced warnings that were false positives. These generally involved comparisons to boolean constants, accessing a variable only in the true or false statements in an if statement, and null coalescing expressions. These examples generated warnings in previous versions of C#, but don't in C# 10: C# string representation = \"N/A\" ; if (( c != null && c . GetDependentValue ( out object obj )) == true ) { representation = obj . ToString (); // undesired error } // Or, using ?. if ( c ?. GetDependentValue ( out object obj ) == true ) { representation = obj . ToString (); // undesired error } // Or, using ?? if ( c ?. GetDependentValue ( out object obj ) ?? false ) { representation = obj . ToString (); // undesired error } The main impact of this improvement is that the warnings for definite assignment and null-state analysis are more accurate.","title":"Improved definite assignment"},{"location":"csharp/csharp-10/#allow-asyncmethodbuilder-attribute-on-methods","text":"In C# 10 and later, you can specify a different async method builder for a single method, in addition to specifying the method builder type for all methods that return a given task-like type. A custom async method builder enables advanced performance tuning scenarios where a given method may benefit from a custom builder. To learn more, see the section on AsyncMethodBuilder in the article on attributes read by the compiler.","title":"Allow AsyncMethodBuilder attribute on methods"},{"location":"csharp/csharp-10/#callerargumentexpression-attribute-diagnostics","text":"You can use the to specify a parameter that the compiler replaces with the text representation of another argument. This feature enables libraries to create more specific diagnostics. The following code tests a condition. If the condition is false, the exception message contains the text representation of the argument passed to condition : C# public static void Validate ( bool condition , [ CallerArgumentExpression ( \"condition\" )] string? message = null ) { if ( ! condition ) { throw new InvalidOperationException ( $\"Argument failed validation: <{message}>\" ); } } You can learn more about this feature in the article on Caller information attributes in the language reference section.","title":"CallerArgumentExpression attribute diagnostics"},{"location":"csharp/csharp-10/#enhanced-line-pragma","text":"C# 10 supports a new format for the #line pragma. You likely won't use the new format, but you'll see its effects. The enhancements enable more fine-grained output in domain-specific languages (DSLs) like Razor. The Razor engine uses these enhancements to improve the debugging experience. You'll find debuggers can highlight your Razor source more accurately. To learn more about the new syntax, see the article on Preprocessor directives in the language reference. You can also read the feature specification for Razor based examples.","title":"Enhanced #line pragma"},{"location":"csharp/csharp-11/","text":"What's new in C# 11 \u00b6 [!IMPORTANT] These are currently preview features. You must set <LangVersion> to preview to enable these features. Any feature may change before its final release. These features may not all be released in C# 11. Some may remain in a preview phase for longer based on feedback on the feature. The following feature is available in Visual Studio 2022 version 17.4: File-scoped types The following features are available in Visual Studio 2022 version 17.3: What's new in C# 11 Generic attributes Generic math support Numeric IntPtr and UIntPtr Newlines in string interpolations List patterns Improved method group conversion to delegate Raw string literals Auto-default struct Pattern match Span<char> or ReadOnlySpan<char> on a constant string Extended nameof scope UTF-8 string literals Required members ref fields and ref scoped variables File scoped types The following features are available in Visual Studio 2022 version 17.2: Raw string literals Improved method group conversion to delegate Warning wave 7 The following features are available in Visual Studio 2022 version 17.1: Generic attributes Newlines in string interpolation expressions List patterns You can download the latest Visual Studio 2022 . You can also try all these features with the preview release of the .NET 7 SDK, which can be downloaded from the all .NET downloads page. Generic attributes \u00b6 You can declare a generic class whose base class is . This feature provides a more convenient syntax for attributes that require a parameter. Previously, you'd need to create an attribute that takes a Type as its constructor parameter: C# // Before C# 11: public class TypeAttribute : Attribute { public TypeAttribute ( Type t ) => ParamType = t ; public Type ParamType { get ; } } And to apply the attribute, you use the typeof operator: C# [TypeAttribute(typeof(string))] public string Method () => default ; Using this new feature, you can create a generic attribute instead: C# // C# 11 feature: public class GenericAttribute < T > : Attribute { } Then, specify the type parameter to use the attribute: C# [GenericAttribute<string>()] public string Method () => default ; You must supply all type parameters when you apply the attribute. In other words, the generic type must be fully constructed . C# public class GenericType < T > { [GenericAttribute<T>()] // Not allowed! generic attributes must be fully constructed types. public string Method () => default ; } The type arguments must satisfy the same restrictions as the typeof operator. Types that require metadata annotations aren't allowed. For example, the following types aren't allowed as the type parameter: dynamic string? (or any nullable reference type) (int X, int Y) (or any other tuple types using C# tuple syntax). These types aren't directly represented in metadata. They include annotations that describe the type. In all cases, you can use the underlying type instead: object for dynamic . string instead of string? . ValueTuple<int, int> instead of (int X, int Y) . Generic math support \u00b6 There are several language features that enable generic math support: static virtual members in interfaces checked user defined operators relaxed shift operators unsigned right-shift operator You can add static abstract or static virtual members in interfaces to define interfaces that include overloadable operators, other static members, and static properties. The primary scenario for this feature is to use mathematical operators in generic types. For example, you can implement the System.IAdditionOperators<TSelf, TOther, TResult> interface in a type that implements operator + . Other interfaces define other mathematical operations or well-defined values. You can learn about the new syntax in the article on interfaces . Interfaces that include static virtual methods are typically generic interfaces . Furthermore, most will declare a constraint that the type parameter implements the declared interface . You can learn more and try the feature yourself in the tutorial Explore static abstract interface members , or the Preview features in .NET 6 \u2013 generic math blog post. Generic math created other requirements on the language. unsigned right shift operator : Before C# 11, to force an unsigned right-shift, you would need to cast any signed integer type to an unsigned type, perform the shift, then cast the result back to a signed type. Beginning in C# 11, you can use the >>> , the unsigned shift operator . relaxed shift operator requirements : C# 11 removes the requirement that the second operand must be an int or implicitly convertible to int . This change allows types that implement generic math interfaces to be used in these locations. checked and unchecked user defined operators : Developers can now define checked and unchecked arithmetic operators. The compiler generates calls to the correct variant based on the current context. You can read more about checked operators in the article on Arithmetic operators . Numeric IntPtr and UIntPtr \u00b6 The nint and nuint types now alias and , respectively. Newlines in string interpolations \u00b6 The text inside the { and } characters for a string interpolation can now span multiple lines. The text between the { and } markers is parsed as C#. Any legal C#, including newlines, is allowed. This feature makes it easier to read string interpolations that use longer C# expressions, like pattern matching switch expressions, or LINQ queries. You can learn more about the newlines feature in the string interpolations article in the language reference. List patterns \u00b6 List patterns extend pattern matching to match sequences of elements in a list or an array. For example, sequence is [1, 2, 3] is true when the sequence is an array or a list of three integers (1, 2, and 3). You can match elements using any pattern, including constant, type, property and relational patterns. The discard pattern ( _ ) matches any single element, and the new range pattern ( .. ) matches any sequence of zero or more elements. You can learn more details about list patterns in the pattern matching article in the language reference. Improved method group conversion to delegate \u00b6 The C# standard on Method group conversions now includes the following item: The conversion is permitted (but not required) to use an existing delegate instance that already contains these references. Previous versions of the standard prohibited the compiler from reusing the delegate object created for a method group conversion. The C# 11 compiler caches the delegate object created from a method group conversion and reuses that single delegate object. This feature is first available in Visual Studio 17.2 as a preview feature. It's first available in .NET 7 preview 2. Raw string literals \u00b6 Raw string literals are a new format for string literals. Raw string literals can contain arbitrary text, including whitespace, new lines, embedded quotes, and other special characters without requiring escape sequences. A raw string literal starts with at least three double-quote (\"\"\") characters. It ends with the same number of double-quote characters. Typically, a raw string literal uses three double quotes on a single line to start the string, and three double quotes on a separate line to end the string. The newlines following the opening quote and preceding the closing quote aren't included in the final content: C# string longMessage = \"\"\" This is a long message . It has several lines . Some are indented more than others . Some should start at the first column . Some have \"quoted text\" in them . \"\"\"; Any whitespace to the left of the closing double quotes will be removed from the string literal. Raw string literals can be combined with string interpolation to include braces in the output text. Multiple $ characters denote how many consecutive braces start and end the interpolation: C# var location = $ $\"\"\" You are at {{{ Longitude }}, {{ Latitude }}} \"\"\"; The preceding example specifies that two braces start and end an interpolation. The third repeated opening and closing brace are included in the output string. You can learn more about raw string literals in the article on strings in the programming guide , and the language reference articles on string literals and interpolated strings . Auto-default struct \u00b6 The C# 11 compiler ensures that all fields of a struct type are initialized to their default value as part of executing a constructor. This change means any field or auto property not initialized by a constructor is automatically initialized by the compiler. Structs where the constructor doesn't definitely assign all fields now compile, and any fields not explicitly initialized are set to their default value. You can read more about how this change affects struct initialization in the article on structs . Pattern match Span<char> or ReadOnlySpan<char> on a constant string \u00b6 You've been able to test if a string had a specific constant value using pattern matching for several releases. Now, you can use the same pattern matching logic with variables that are Span<char> or ReadOnlySpan<char> . Extended nameof scope \u00b6 Type parameter names and parameter names are now in scope when used in a nameof expression in an attribute declaration on that method. This feature means you can use the nameof operator to specify the name of a method parameter in an attribute on the method or parameter declaration. This feature is most often useful to add attributes for nullable analysis . UTF-8 string literals \u00b6 You can specify the u8 suffix on a string literal to specify UTF-8 character encoding. If your application needs UTF-8 strings, for HTTP string constants or similar text protocols, you can use this feature to simplify the creation of UTF-8 strings. You can learn more about UTF-8 string literals in the string literal section of the article on builtin reference types . Required members \u00b6 You can add the required modifier to properties and fields to enforce constructors and callers to initialize those values. The can be added to constructors to inform the compiler that a constructor initializes all required members. For more information on required members, See the init-only section of the properties article. ref fields and ref scoped variables \u00b6 You can declare ref fields inside a ref struct . This supports types such as without special attributes or hidden internal types. You can add the scoped modifier to any ref declaration. This limits the scope where the reference can escape to. File scoped types \u00b6 Beginning in C# 11, you can use the file access modifier to create a type whose visibility is scoped to the source file in which it is declared. This feature helps source generator authors avoid naming collisions. You can learn more about this feature in the article on file-scoped types in the language reference.","title":"What's new in C# 11 - C# Guide"},{"location":"csharp/csharp-11/#whats-new-in-c-11","text":"[!IMPORTANT] These are currently preview features. You must set <LangVersion> to preview to enable these features. Any feature may change before its final release. These features may not all be released in C# 11. Some may remain in a preview phase for longer based on feedback on the feature. The following feature is available in Visual Studio 2022 version 17.4: File-scoped types The following features are available in Visual Studio 2022 version 17.3: What's new in C# 11 Generic attributes Generic math support Numeric IntPtr and UIntPtr Newlines in string interpolations List patterns Improved method group conversion to delegate Raw string literals Auto-default struct Pattern match Span<char> or ReadOnlySpan<char> on a constant string Extended nameof scope UTF-8 string literals Required members ref fields and ref scoped variables File scoped types The following features are available in Visual Studio 2022 version 17.2: Raw string literals Improved method group conversion to delegate Warning wave 7 The following features are available in Visual Studio 2022 version 17.1: Generic attributes Newlines in string interpolation expressions List patterns You can download the latest Visual Studio 2022 . You can also try all these features with the preview release of the .NET 7 SDK, which can be downloaded from the all .NET downloads page.","title":"What's new in C# 11"},{"location":"csharp/csharp-11/#generic-attributes","text":"You can declare a generic class whose base class is . This feature provides a more convenient syntax for attributes that require a parameter. Previously, you'd need to create an attribute that takes a Type as its constructor parameter: C# // Before C# 11: public class TypeAttribute : Attribute { public TypeAttribute ( Type t ) => ParamType = t ; public Type ParamType { get ; } } And to apply the attribute, you use the typeof operator: C# [TypeAttribute(typeof(string))] public string Method () => default ; Using this new feature, you can create a generic attribute instead: C# // C# 11 feature: public class GenericAttribute < T > : Attribute { } Then, specify the type parameter to use the attribute: C# [GenericAttribute<string>()] public string Method () => default ; You must supply all type parameters when you apply the attribute. In other words, the generic type must be fully constructed . C# public class GenericType < T > { [GenericAttribute<T>()] // Not allowed! generic attributes must be fully constructed types. public string Method () => default ; } The type arguments must satisfy the same restrictions as the typeof operator. Types that require metadata annotations aren't allowed. For example, the following types aren't allowed as the type parameter: dynamic string? (or any nullable reference type) (int X, int Y) (or any other tuple types using C# tuple syntax). These types aren't directly represented in metadata. They include annotations that describe the type. In all cases, you can use the underlying type instead: object for dynamic . string instead of string? . ValueTuple<int, int> instead of (int X, int Y) .","title":"Generic attributes"},{"location":"csharp/csharp-11/#generic-math-support","text":"There are several language features that enable generic math support: static virtual members in interfaces checked user defined operators relaxed shift operators unsigned right-shift operator You can add static abstract or static virtual members in interfaces to define interfaces that include overloadable operators, other static members, and static properties. The primary scenario for this feature is to use mathematical operators in generic types. For example, you can implement the System.IAdditionOperators<TSelf, TOther, TResult> interface in a type that implements operator + . Other interfaces define other mathematical operations or well-defined values. You can learn about the new syntax in the article on interfaces . Interfaces that include static virtual methods are typically generic interfaces . Furthermore, most will declare a constraint that the type parameter implements the declared interface . You can learn more and try the feature yourself in the tutorial Explore static abstract interface members , or the Preview features in .NET 6 \u2013 generic math blog post. Generic math created other requirements on the language. unsigned right shift operator : Before C# 11, to force an unsigned right-shift, you would need to cast any signed integer type to an unsigned type, perform the shift, then cast the result back to a signed type. Beginning in C# 11, you can use the >>> , the unsigned shift operator . relaxed shift operator requirements : C# 11 removes the requirement that the second operand must be an int or implicitly convertible to int . This change allows types that implement generic math interfaces to be used in these locations. checked and unchecked user defined operators : Developers can now define checked and unchecked arithmetic operators. The compiler generates calls to the correct variant based on the current context. You can read more about checked operators in the article on Arithmetic operators .","title":"Generic math support"},{"location":"csharp/csharp-11/#numeric-intptr-and-uintptr","text":"The nint and nuint types now alias and , respectively.","title":"Numeric IntPtr and UIntPtr"},{"location":"csharp/csharp-11/#newlines-in-string-interpolations","text":"The text inside the { and } characters for a string interpolation can now span multiple lines. The text between the { and } markers is parsed as C#. Any legal C#, including newlines, is allowed. This feature makes it easier to read string interpolations that use longer C# expressions, like pattern matching switch expressions, or LINQ queries. You can learn more about the newlines feature in the string interpolations article in the language reference.","title":"Newlines in string interpolations"},{"location":"csharp/csharp-11/#list-patterns","text":"List patterns extend pattern matching to match sequences of elements in a list or an array. For example, sequence is [1, 2, 3] is true when the sequence is an array or a list of three integers (1, 2, and 3). You can match elements using any pattern, including constant, type, property and relational patterns. The discard pattern ( _ ) matches any single element, and the new range pattern ( .. ) matches any sequence of zero or more elements. You can learn more details about list patterns in the pattern matching article in the language reference.","title":"List patterns"},{"location":"csharp/csharp-11/#improved-method-group-conversion-to-delegate","text":"The C# standard on Method group conversions now includes the following item: The conversion is permitted (but not required) to use an existing delegate instance that already contains these references. Previous versions of the standard prohibited the compiler from reusing the delegate object created for a method group conversion. The C# 11 compiler caches the delegate object created from a method group conversion and reuses that single delegate object. This feature is first available in Visual Studio 17.2 as a preview feature. It's first available in .NET 7 preview 2.","title":"Improved method group conversion to delegate"},{"location":"csharp/csharp-11/#raw-string-literals","text":"Raw string literals are a new format for string literals. Raw string literals can contain arbitrary text, including whitespace, new lines, embedded quotes, and other special characters without requiring escape sequences. A raw string literal starts with at least three double-quote (\"\"\") characters. It ends with the same number of double-quote characters. Typically, a raw string literal uses three double quotes on a single line to start the string, and three double quotes on a separate line to end the string. The newlines following the opening quote and preceding the closing quote aren't included in the final content: C# string longMessage = \"\"\" This is a long message . It has several lines . Some are indented more than others . Some should start at the first column . Some have \"quoted text\" in them . \"\"\"; Any whitespace to the left of the closing double quotes will be removed from the string literal. Raw string literals can be combined with string interpolation to include braces in the output text. Multiple $ characters denote how many consecutive braces start and end the interpolation: C# var location = $ $\"\"\" You are at {{{ Longitude }}, {{ Latitude }}} \"\"\"; The preceding example specifies that two braces start and end an interpolation. The third repeated opening and closing brace are included in the output string. You can learn more about raw string literals in the article on strings in the programming guide , and the language reference articles on string literals and interpolated strings .","title":"Raw string literals"},{"location":"csharp/csharp-11/#auto-default-struct","text":"The C# 11 compiler ensures that all fields of a struct type are initialized to their default value as part of executing a constructor. This change means any field or auto property not initialized by a constructor is automatically initialized by the compiler. Structs where the constructor doesn't definitely assign all fields now compile, and any fields not explicitly initialized are set to their default value. You can read more about how this change affects struct initialization in the article on structs .","title":"Auto-default struct"},{"location":"csharp/csharp-11/#pattern-match-spanchar-or-readonlyspanchar-on-a-constant-string","text":"You've been able to test if a string had a specific constant value using pattern matching for several releases. Now, you can use the same pattern matching logic with variables that are Span<char> or ReadOnlySpan<char> .","title":"Pattern match Span&lt;char&gt; or ReadOnlySpan&lt;char&gt; on a constant string"},{"location":"csharp/csharp-11/#extended-nameof-scope","text":"Type parameter names and parameter names are now in scope when used in a nameof expression in an attribute declaration on that method. This feature means you can use the nameof operator to specify the name of a method parameter in an attribute on the method or parameter declaration. This feature is most often useful to add attributes for nullable analysis .","title":"Extended nameof scope"},{"location":"csharp/csharp-11/#utf-8-string-literals","text":"You can specify the u8 suffix on a string literal to specify UTF-8 character encoding. If your application needs UTF-8 strings, for HTTP string constants or similar text protocols, you can use this feature to simplify the creation of UTF-8 strings. You can learn more about UTF-8 string literals in the string literal section of the article on builtin reference types .","title":"UTF-8 string literals"},{"location":"csharp/csharp-11/#required-members","text":"You can add the required modifier to properties and fields to enforce constructors and callers to initialize those values. The can be added to constructors to inform the compiler that a constructor initializes all required members. For more information on required members, See the init-only section of the properties article.","title":"Required members"},{"location":"csharp/csharp-11/#ref-fields-and-ref-scoped-variables","text":"You can declare ref fields inside a ref struct . This supports types such as without special attributes or hidden internal types. You can add the scoped modifier to any ref declaration. This limits the scope where the reference can escape to.","title":"ref fields and ref scoped variables"},{"location":"csharp/csharp-11/#file-scoped-types","text":"Beginning in C# 11, you can use the file access modifier to create a type whose visibility is scoped to the source file in which it is declared. This feature helps source generator authors avoid naming collisions. You can learn more about this feature in the article on file-scoped types in the language reference.","title":"File scoped types"},{"location":"csharp/csharp-7/","text":"What's new in C# 7.0 through C# 7.3 \u00b6 C# 7.0 through C# 7.3 brought a number of features and incremental improvements to your development experience with C#. This article provides an overview of the new language features and compiler options. The descriptions describe the behavior for C# 7.3, which is the most recent version supported for .NET Framework-based applications. The language version selection configuration element was added with C# 7.1, which enables you to specify the compiler language version in your project file. C# 7.0-7.3 adds these features and themes to the C# language: What's new in C# 7.0 through C# 7.3 Tuples and discards Pattern matching Async main Local functions More expression-bodied members Throw expressions Default literal expressions Numeric literal syntax improvements out variables Non-trailing named arguments private protected access modifier Improved overload candidates Enabling more efficient safe code Ref locals and returns Conditional ref expressions in parameter modifier More types support the fixed statement Indexing fixed fields does not require pinning stackalloc arrays support initializers Enhanced generic constraints Generalized async return types New compiler options Reference assembly generation Public or Open Source signing pathmap Finally, the compiler has new options: -refout and -refonly that control reference assembly generation . -publicsign to enable Open Source Software (OSS) signing of assemblies. -pathmap to provide a mapping for source directories. The remainder of this article provides an overview of each feature. For each feature, you'll learn the reasoning behind it and the syntax. You can explore these features in your environment using the dotnet try global tool: Install the dotnet-try global tool. Clone the dotnet/try-samples repository. Set the current directory to the csharp7 subdirectory for the try-samples repository. Run dotnet try . Tuples and discards \u00b6 C# provides a rich syntax for classes and structs that is used to explain your design intent. But sometimes that rich syntax requires extra work with minimal benefit. You may often write methods that need a simple structure containing more than one data element. To support these scenarios tuples were added to C#. Tuples are lightweight data structures that contain multiple fields to represent the data members. The fields aren't validated, and you can't define your own methods. C# tuple types support == and != . For more information. [!NOTE] Tuples were available before C# 7.0, but they were inefficient and had no language support. This meant that tuple elements could only be referenced as Item1 , Item2 and so on. C# 7.0 introduces language support for tuples, which enables semantic names for the fields of a tuple using new, more efficient tuple types. You can create a tuple by assigning a value to each member, and optionally providing semantic names to each of the members of the tuple: [!code-csharp NamedTuple ] The namedLetters tuple contains fields referred to as Alpha and Beta . Those names exist only at compile time and aren't preserved, for example when inspecting the tuple using reflection at run time. In a tuple assignment, you can also specify the names of the fields on the right-hand side of the assignment: [!code-csharp ImplicitNamedTuple ] There may be times when you want to unpackage the members of a tuple that were returned from a method. You can do that by declaring separate variables for each of the values in the tuple. This unpackaging is called deconstructing the tuple: [!code-csharp CallingWithDeconstructor ] You can also provide a similar deconstruction for any type in .NET. You write a Deconstruct method as a member of the class. That Deconstruct method provides a set of out arguments for each of the properties you want to extract. Consider this Point class that provides a deconstructor method that extracts the X and Y coordinates: [!code-csharp PointWithDeconstruction ] You can extract the individual fields by assigning a Point to a tuple: [!code-csharp DeconstructPoint ] Many times when you initialize a tuple, the variables used for the right side of the assignment are the same as the names you'd like for the tuple elements: The names of tuple elements can be inferred from the variables used to initialize the tuple: C# int count = 5 ; string label = \"Colors used in the map\" ; var pair = ( count , label ); // element names are \"count\" and \"label\" You can learn more about this feature in the Tuple types article. Often when deconstructing a tuple or calling a method with out parameters, you're forced to define a variable whose value you don't care about and don't intend to use. C# adds support for discards to handle this scenario. A discard is a write-only variable whose name is _ (the underscore character); you can assign all of the values that you intend to discard to the single variable. A discard is like an unassigned variable; apart from the assignment statement, the discard can't be used in code. Discards are supported in the following scenarios: When deconstructing tuples or user-defined types. When calling methods with out parameters. In a pattern matching operation with the is and switch statements. As a standalone identifier when you want to explicitly identify the value of an assignment as a discard. The following example defines a QueryCityDataForYears method that returns a 6-tuple that contains data for a city for two different years. The method call in the example is concerned only with the two population values returned by the method and so treats the remaining values in the tuple as discards when it deconstructs the tuple. [!code-csharp Tuple-discard ] For more information, see Discards . Pattern matching \u00b6 Pattern matching is a set of features that enable new ways to express control flow in your code. You can test variables for their type, values or the values of their properties. These techniques create more readable code flow. Pattern matching supports is expressions and switch expressions. Each enables inspecting an object and its properties to determine if that object satisfies the sought pattern. You use the when keyword to specify additional rules to the pattern. The is pattern expression extends the familiar is operator to query an object about its type and assign the result in one instruction. The following code checks if a variable is an int , and if so, adds it to the current sum: C# if ( input is int count ) sum += count ; The preceding small example demonstrates the enhancements to the is expression. You can test against value types as well as reference types, and you can assign the successful result to a new variable of the correct type. The switch match expression has a familiar syntax, based on the switch statement already part of the C# language. The updated switch statement has several new constructs: The governing type of a switch expression is no longer restricted to integral types, Enum types, string , or a nullable type corresponding to one of those types. Any type may be used. You can test the type of the switch expression in each case label. As with the is expression, you may assign a new variable to that type. You may add a when clause to further test conditions on that variable. The order of case labels is now important. The first branch to match is executed; others are skipped. The following code demonstrates these new features: C# public static int SumPositiveNumbers ( IEnumerable < object > sequence ) { int sum = 0 ; foreach ( var i in sequence ) { switch ( i ) { case 0 : break ; case IEnumerable < int > childSequence : { foreach ( var item in childSequence ) sum += ( item > 0 ) ? item : 0 ; break ; } case int n when n > 0 : sum += n ; break ; case null : throw new NullReferenceException ( \"Null found in sequence\" ); default : throw new InvalidOperationException ( \"Unrecognized type\" ); } } return sum ; } case 0: is the familiar constant pattern. case IEnumerable<int> childSequence: is a type pattern. case int n when n > 0: is a type pattern with an additional when condition. case null: is the null pattern. default: is the familiar default case. Beginning with C# 7.1, the pattern expression for is and the switch type pattern may have the type of a generic type parameter. This can be most useful when checking types that may be either struct or class types, and you want to avoid boxing. You can learn more about pattern matching in Pattern Matching in C# . Async main \u00b6 An async main method enables you to use await in your Main method. Previously you would need to write: C# static int Main () { return DoAsyncWork (). GetAwaiter (). GetResult (); } You can now write: C# static async Task < int > Main () { // This could also be replaced with the body // DoAsyncWork, including its await expressions: return await DoAsyncWork (); } If your program doesn't return an exit code, you can declare a Main method that returns a : C# static async Task Main () { await SomeAsyncMethod (); } You can read more about the details in the async main article in the programming guide. Local functions \u00b6 Many designs for classes include methods that are called from only one location. These additional private methods keep each method small and focused. Local functions enable you to declare methods inside the context of another method. Local functions make it easier for readers of the class to see that the local method is only called from the context in which it is declared. There are two common use cases for local functions: public iterator methods and public async methods. Both types of methods generate code that reports errors later than programmers might expect. In iterator methods, any exceptions are observed only when calling code that enumerates the returned sequence. In async methods, any exceptions are only observed when the returned Task is awaited. The following example demonstrates separating parameter validation from the iterator implementation using a local function: [!code-csharp 22_IteratorMethodLocal ] The same technique can be employed with async methods to ensure that exceptions arising from argument validation are thrown before the asynchronous work begins: [!code-csharp TaskExample ] This syntax is now supported: C# [field: SomeThingAboutFieldAttribute] public int SomeProperty { get ; set ; } The attribute SomeThingAboutFieldAttribute is applied to the compiler generated backing field for SomeProperty . For more information, see attributes in the C# programming guide. [!NOTE] Some of the designs that are supported by local functions can also be accomplished using lambda expressions . For more information, see Local functions vs. lambda expressions . More expression-bodied members \u00b6 C# 6 introduced expression-bodied members for member functions and read-only properties. C# 7.0 expands the allowed members that can be implemented as expressions. In C# 7.0, you can implement constructors , finalizers , and get and set accessors on properties and indexers . The following code shows examples of each: [!code-csharp ExpressionBodiedMembers ] [!NOTE] This example does not need a finalizer, but it is shown to demonstrate the syntax. You should not implement a finalizer in your class unless it is necessary to release unmanaged resources. You should also consider using the class instead of managing unmanaged resources directly. These new locations for expression-bodied members represent an important milestone for the C# language: These features were implemented by community members working on the open-source Roslyn project. Changing a method to an expression bodied member is a binary compatible change . Throw expressions \u00b6 In C#, throw has always been a statement. Because throw is a statement, not an expression, there were C# constructs where you couldn't use it. These included conditional expressions, null coalescing expressions, and some lambda expressions. The addition of expression-bodied members adds more locations where throw expressions would be useful. So that you can write any of these constructs, C# 7.0 introduces throw expressions . This addition makes it easier to write more expression-based code. You don't need additional statements for error checking. Default literal expressions \u00b6 Default literal expressions are an enhancement to default value expressions. These expressions initialize a variable to the default value. Where you previously would write: C# Func < string , bool > whereClause = default ( Func < string , bool > ); You can now omit the type on the right-hand side of the initialization: C# Func < string , bool > whereClause = default ; For more information, see the default literal section of the default operator article. Numeric literal syntax improvements \u00b6 Misreading numeric constants can make it harder to understand code when reading it for the first time. Bit masks or other symbolic values are prone to misunderstanding. C# 7.0 includes two new features to write numbers in the most readable fashion for the intended use: binary literals , and digit separators . For those times when you're creating bit masks, or whenever a binary representation of a number makes the most readable code, write that number in binary: [!code-csharp ThousandSeparators ] The 0b at the beginning of the constant indicates that the number is written as a binary number. Binary numbers can get long, so it's often easier to see the bit patterns by introducing the _ as a digit separator, as shown in the binary constant in the preceding example. The digit separator can appear anywhere in the constant. For base 10 numbers, it is common to use it as a thousands separator. Hex and binary numeric literals may begin with an _ : [!code-csharp LargeIntegers ] The digit separator can be used with decimal , float , and double types as well: [!code-csharp OtherConstants ] Taken together, you can declare numeric constants with much more readability. out variables \u00b6 The existing syntax that supports out parameters has been improved in C# 7. You can now declare out variables in the argument list of a method call, rather than writing a separate declaration statement: [!code-csharp OutVariableDeclarations ] You may want to specify the type of the out variable for clarity, as shown in the preceding example. However, the language does support using an implicitly typed local variable: [!code-csharp OutVarVariableDeclarations ] The code is easier to read. You declare the out variable where you use it, not on a preceding line of code. No need to assign an initial value. By declaring the out variable where it's used in a method call, you can't accidentally use it before it is assigned. The syntax added in C# 7.0 to allow out variable declarations has been extended to include field initializers, property initializers, constructor initializers, and query clauses. It enables code such as the following example: C# public class B { public B ( int i , out int j ) { j = i ; } } public class D : B { public D ( int i ) : base ( i , out var j ) { Console . WriteLine ( $\"The value of 'j' is {j}\" ); } } Non-trailing named arguments \u00b6 Method calls may now use named arguments that precede positional arguments when those named arguments are in the correct positions. For more information, see Named and optional arguments . private protected access modifier \u00b6 A new compound access modifier: private protected indicates that a member may be accessed by containing class or derived classes that are declared in the same assembly. While protected internal allows access by derived classes or classes that are in the same assembly, private protected limits access to derived types declared in the same assembly. For more information, see access modifiers in the language reference. Improved overload candidates \u00b6 In every release, the overload resolution rules get updated to address situations where ambiguous method invocations have an \"obvious\" choice. This release adds three new rules to help the compiler pick the obvious choice: When a method group contains both instance and static members, the compiler discards the instance members if the method was invoked without an instance receiver or context. The compiler discards the static members if the method was invoked with an instance receiver. When there is no receiver, the compiler includes only static members in a static context, otherwise both static and instance members. When the receiver is ambiguously an instance or type, the compiler includes both. A static context, where an implicit this instance receiver cannot be used, includes the body of members where no this is defined, such as static members, as well as places where this cannot be used, such as field initializers and constructor-initializers. When a method group contains some generic methods whose type arguments do not satisfy their constraints, these members are removed from the candidate set. For a method group conversion, candidate methods whose return type doesn't match up with the delegate's return type are removed from the set. You'll only notice this change because you'll find fewer compiler errors for ambiguous method overloads when you are sure which method is better. Enabling more efficient safe code \u00b6 You should be able to write C# code safely that performs as well as unsafe code. Safe code avoids classes of errors, such as buffer overruns, stray pointers, and other memory access errors. These new features expand the capabilities of verifiable safe code. Strive to write more of your code using safe constructs. These features make that easier. The following new features support the theme of better performance for safe code: You can access fixed fields without pinning. You can reassign ref local variables. You can use initializers on stackalloc arrays. You can use fixed statements with any type that supports a pattern. You can use additional generic constraints. The in modifier on parameters, to specify that an argument is passed by reference but not modified by the called method. Adding the in modifier to an argument is a source compatible change . The ref readonly modifier on method returns, to indicate that a method returns its value by reference but doesn't allow writes to that object. Adding the ref readonly modifier is a source compatible change , if the return is assigned to a value. Adding the readonly modifier to an existing ref return statement is an incompatible change . It requires callers to update the declaration of ref local variables to include the readonly modifier. The readonly struct declaration, to indicate that a struct is immutable and should be passed as an in parameter to its member methods. Adding the readonly modifier to an existing struct declaration is a binary compatible change . The ref struct declaration, to indicate that a struct type accesses managed memory directly and must always be stack allocated. Adding the ref modifier to an existing struct declaration is an incompatible change . A ref struct cannot be a member of a class or used in other locations where it may be allocated on the heap. You can read more about all these changes in Write safe efficient code . Ref locals and returns \u00b6 This feature enables algorithms that use and return references to variables defined elsewhere. One example is working with large matrices, and finding a single location with certain characteristics. The following method returns a reference to that storage in the matrix: [!code-csharp FindReturningRef ] You can declare the return value as a ref and modify that value in the matrix, as shown in the following code: [!code-csharp AssignRefReturn ] The C# language has several rules that protect you from misusing the ref locals and returns: You must add the ref keyword to the method signature and to all return statements in a method. That makes it clear the method returns by reference throughout the method. A ref return may be assigned to a value variable, or a ref variable. The caller controls whether the return value is copied or not. Omitting the ref modifier when assigning the return value indicates that the caller wants a copy of the value, not a reference to the storage. You can't assign a standard method return value to a ref local variable. That disallows statements like ref int i = sequence.Count(); You can't return a ref to a variable whose lifetime doesn't extend beyond the execution of the method. That means you can't return a reference to a local variable or a variable with a similar scope. ref locals and returns can't be used with async methods. The compiler can't know if the referenced variable has been set to its final value when the async method returns. The addition of ref locals and ref returns enables algorithms that are more efficient by avoiding copying values, or performing dereferencing operations multiple times. Adding ref to the return value is a source compatible change . Existing code compiles, but the ref return value is copied when assigned. Callers must update the storage for the return value to a ref local variable to store the return as a reference. Now, ref locals may be reassigned to refer to different instances after being initialized. The following code now compiles: C# ref VeryLargeStruct refLocal = ref veryLargeStruct ; // initialization refLocal = ref anotherVeryLargeStruct ; // reassigned, refLocal refers to different storage. For more information, see the article on ref returns and ref locals , and the article on foreach . For more information, see the ref keyword article. Conditional ref expressions \u00b6 Finally, the conditional expression may produce a ref result instead of a value result. For example, you would write the following to retrieve a reference to the first element in one of two arrays: C# ref var r = ref ( arr != null ? ref arr [ 0 ] : ref otherArr [ 0 ]); The variable r is a reference to the first value in either arr or otherArr . For more information, see conditional operator (?:) in the language reference. in parameter modifier \u00b6 The in keyword complements the existing ref and out keywords to pass arguments by reference. The in keyword specifies passing the argument by reference, but the called method doesn't modify the value. You may declare overloads that pass by value or by readonly reference, as shown in the following code: C# static void M ( S arg ); static void M ( in S arg ); The by value (first in the preceding example) overload is better than the by readonly reference version. To call the version with the readonly reference argument, you must include the in modifier when calling the method. For more information, see the article on the in parameter modifier . More types support the fixed statement \u00b6 The fixed statement supported a limited set of types. Starting with C# 7.3, any type that contains a GetPinnableReference() method that returns a ref T or ref readonly T may be fixed . Adding this feature means that fixed can be used with and related types. For more information, see the fixed statement article in the language reference. Indexing fixed fields does not require pinning \u00b6 Consider this struct: C# unsafe struct S { public fixed int myFixedField [ 10 ]; } In earlier versions of C#, you needed to pin a variable to access one of the integers that are part of myFixedField . Now, the following code compiles without pinning the variable p inside a separate fixed statement: C# class C { static S s = new S (); unsafe public void M () { int p = s . myFixedField [ 5 ]; } } The variable p accesses one element in myFixedField . You don't need to declare a separate int* variable. You still need an unsafe context. In earlier versions of C#, you need to declare a second fixed pointer: C# class C { static S s = new S (); unsafe public void M () { fixed ( int * ptr = s . myFixedField ) { int p = ptr [ 5 ]; } } } For more information, see the article on the fixed statement . stackalloc arrays support initializers \u00b6 You've been able to specify the values for elements in an array when you initialize it: C# var arr = new int [ 3 ] { 1 , 2 , 3 }; var arr2 = new int [] { 1 , 2 , 3 }; Now, that same syntax can be applied to arrays that are declared with stackalloc : C# int * pArr = stackalloc int [ 3 ] { 1 , 2 , 3 }; int * pArr2 = stackalloc int [] { 1 , 2 , 3 }; Span < int > arr = stackalloc [] { 1 , 2 , 3 }; For more information, see the stackalloc operator article. Enhanced generic constraints \u00b6 You can now specify the type or as base class constraints for a type parameter. You can also use the new unmanaged constraint, to specify that a type parameter must be a non-nullable unmanaged type . For more information, see the articles on where generic constraints and constraints on type parameters . Adding these constraints to existing types is an incompatible change . Closed generic types may no longer meet these new constraints. Generalized async return types \u00b6 Returning a Task object from async methods can introduce performance bottlenecks in certain paths. Task is a reference type, so using it means allocating an object. In cases where a method declared with the async modifier returns a cached result, or completes synchronously, the extra allocations can become a significant time cost in performance critical sections of code. It can become costly if those allocations occur in tight loops. The new language feature means that async method return types aren't limited to Task , Task<T> , and void . The returned type must still satisfy the async pattern, meaning a GetAwaiter method must be accessible. As one concrete example, the ValueTask type has been added to .NET to make use of this new language feature: [!code-csharp UsingValueTask ] [!NOTE] You need to add the NuGet package System.Threading.Tasks.Extensions > in order to use the type. This enhancement is most useful for library authors to avoid allocating a Task in performance critical code. New compiler options \u00b6 New compiler options support new build and DevOps scenarios for C# programs. Reference assembly generation \u00b6 There are two new compiler options that generate reference-only assemblies : ProduceReferenceAssembly and ProduceOnlyReferenceAssembly . The linked articles explain these options and reference assemblies in more detail. Public or Open Source signing \u00b6 The PublicSign compiler option instructs the compiler to sign the assembly using a public key. The assembly is marked as signed, but the signature is taken from the public key. This option enables you to build signed assemblies from open-source projects using a public key. For more information, see the PublicSign compiler option article. pathmap \u00b6 The PathMap compiler option instructs the compiler to replace source paths from the build environment with mapped source paths. The PathMap option controls the source path written by the compiler to PDB files or for the . For more information, see the PathMap compiler option article.","title":"What's new in C# 7.0 through C# 7.3"},{"location":"csharp/csharp-7/#whats-new-in-c-70-through-c-73","text":"C# 7.0 through C# 7.3 brought a number of features and incremental improvements to your development experience with C#. This article provides an overview of the new language features and compiler options. The descriptions describe the behavior for C# 7.3, which is the most recent version supported for .NET Framework-based applications. The language version selection configuration element was added with C# 7.1, which enables you to specify the compiler language version in your project file. C# 7.0-7.3 adds these features and themes to the C# language: What's new in C# 7.0 through C# 7.3 Tuples and discards Pattern matching Async main Local functions More expression-bodied members Throw expressions Default literal expressions Numeric literal syntax improvements out variables Non-trailing named arguments private protected access modifier Improved overload candidates Enabling more efficient safe code Ref locals and returns Conditional ref expressions in parameter modifier More types support the fixed statement Indexing fixed fields does not require pinning stackalloc arrays support initializers Enhanced generic constraints Generalized async return types New compiler options Reference assembly generation Public or Open Source signing pathmap Finally, the compiler has new options: -refout and -refonly that control reference assembly generation . -publicsign to enable Open Source Software (OSS) signing of assemblies. -pathmap to provide a mapping for source directories. The remainder of this article provides an overview of each feature. For each feature, you'll learn the reasoning behind it and the syntax. You can explore these features in your environment using the dotnet try global tool: Install the dotnet-try global tool. Clone the dotnet/try-samples repository. Set the current directory to the csharp7 subdirectory for the try-samples repository. Run dotnet try .","title":"What's new in C# 7.0 through C# 7.3"},{"location":"csharp/csharp-7/#tuples-and-discards","text":"C# provides a rich syntax for classes and structs that is used to explain your design intent. But sometimes that rich syntax requires extra work with minimal benefit. You may often write methods that need a simple structure containing more than one data element. To support these scenarios tuples were added to C#. Tuples are lightweight data structures that contain multiple fields to represent the data members. The fields aren't validated, and you can't define your own methods. C# tuple types support == and != . For more information. [!NOTE] Tuples were available before C# 7.0, but they were inefficient and had no language support. This meant that tuple elements could only be referenced as Item1 , Item2 and so on. C# 7.0 introduces language support for tuples, which enables semantic names for the fields of a tuple using new, more efficient tuple types. You can create a tuple by assigning a value to each member, and optionally providing semantic names to each of the members of the tuple: [!code-csharp NamedTuple ] The namedLetters tuple contains fields referred to as Alpha and Beta . Those names exist only at compile time and aren't preserved, for example when inspecting the tuple using reflection at run time. In a tuple assignment, you can also specify the names of the fields on the right-hand side of the assignment: [!code-csharp ImplicitNamedTuple ] There may be times when you want to unpackage the members of a tuple that were returned from a method. You can do that by declaring separate variables for each of the values in the tuple. This unpackaging is called deconstructing the tuple: [!code-csharp CallingWithDeconstructor ] You can also provide a similar deconstruction for any type in .NET. You write a Deconstruct method as a member of the class. That Deconstruct method provides a set of out arguments for each of the properties you want to extract. Consider this Point class that provides a deconstructor method that extracts the X and Y coordinates: [!code-csharp PointWithDeconstruction ] You can extract the individual fields by assigning a Point to a tuple: [!code-csharp DeconstructPoint ] Many times when you initialize a tuple, the variables used for the right side of the assignment are the same as the names you'd like for the tuple elements: The names of tuple elements can be inferred from the variables used to initialize the tuple: C# int count = 5 ; string label = \"Colors used in the map\" ; var pair = ( count , label ); // element names are \"count\" and \"label\" You can learn more about this feature in the Tuple types article. Often when deconstructing a tuple or calling a method with out parameters, you're forced to define a variable whose value you don't care about and don't intend to use. C# adds support for discards to handle this scenario. A discard is a write-only variable whose name is _ (the underscore character); you can assign all of the values that you intend to discard to the single variable. A discard is like an unassigned variable; apart from the assignment statement, the discard can't be used in code. Discards are supported in the following scenarios: When deconstructing tuples or user-defined types. When calling methods with out parameters. In a pattern matching operation with the is and switch statements. As a standalone identifier when you want to explicitly identify the value of an assignment as a discard. The following example defines a QueryCityDataForYears method that returns a 6-tuple that contains data for a city for two different years. The method call in the example is concerned only with the two population values returned by the method and so treats the remaining values in the tuple as discards when it deconstructs the tuple. [!code-csharp Tuple-discard ] For more information, see Discards .","title":"Tuples and discards"},{"location":"csharp/csharp-7/#pattern-matching","text":"Pattern matching is a set of features that enable new ways to express control flow in your code. You can test variables for their type, values or the values of their properties. These techniques create more readable code flow. Pattern matching supports is expressions and switch expressions. Each enables inspecting an object and its properties to determine if that object satisfies the sought pattern. You use the when keyword to specify additional rules to the pattern. The is pattern expression extends the familiar is operator to query an object about its type and assign the result in one instruction. The following code checks if a variable is an int , and if so, adds it to the current sum: C# if ( input is int count ) sum += count ; The preceding small example demonstrates the enhancements to the is expression. You can test against value types as well as reference types, and you can assign the successful result to a new variable of the correct type. The switch match expression has a familiar syntax, based on the switch statement already part of the C# language. The updated switch statement has several new constructs: The governing type of a switch expression is no longer restricted to integral types, Enum types, string , or a nullable type corresponding to one of those types. Any type may be used. You can test the type of the switch expression in each case label. As with the is expression, you may assign a new variable to that type. You may add a when clause to further test conditions on that variable. The order of case labels is now important. The first branch to match is executed; others are skipped. The following code demonstrates these new features: C# public static int SumPositiveNumbers ( IEnumerable < object > sequence ) { int sum = 0 ; foreach ( var i in sequence ) { switch ( i ) { case 0 : break ; case IEnumerable < int > childSequence : { foreach ( var item in childSequence ) sum += ( item > 0 ) ? item : 0 ; break ; } case int n when n > 0 : sum += n ; break ; case null : throw new NullReferenceException ( \"Null found in sequence\" ); default : throw new InvalidOperationException ( \"Unrecognized type\" ); } } return sum ; } case 0: is the familiar constant pattern. case IEnumerable<int> childSequence: is a type pattern. case int n when n > 0: is a type pattern with an additional when condition. case null: is the null pattern. default: is the familiar default case. Beginning with C# 7.1, the pattern expression for is and the switch type pattern may have the type of a generic type parameter. This can be most useful when checking types that may be either struct or class types, and you want to avoid boxing. You can learn more about pattern matching in Pattern Matching in C# .","title":"Pattern matching"},{"location":"csharp/csharp-7/#async-main","text":"An async main method enables you to use await in your Main method. Previously you would need to write: C# static int Main () { return DoAsyncWork (). GetAwaiter (). GetResult (); } You can now write: C# static async Task < int > Main () { // This could also be replaced with the body // DoAsyncWork, including its await expressions: return await DoAsyncWork (); } If your program doesn't return an exit code, you can declare a Main method that returns a : C# static async Task Main () { await SomeAsyncMethod (); } You can read more about the details in the async main article in the programming guide.","title":"Async main"},{"location":"csharp/csharp-7/#local-functions","text":"Many designs for classes include methods that are called from only one location. These additional private methods keep each method small and focused. Local functions enable you to declare methods inside the context of another method. Local functions make it easier for readers of the class to see that the local method is only called from the context in which it is declared. There are two common use cases for local functions: public iterator methods and public async methods. Both types of methods generate code that reports errors later than programmers might expect. In iterator methods, any exceptions are observed only when calling code that enumerates the returned sequence. In async methods, any exceptions are only observed when the returned Task is awaited. The following example demonstrates separating parameter validation from the iterator implementation using a local function: [!code-csharp 22_IteratorMethodLocal ] The same technique can be employed with async methods to ensure that exceptions arising from argument validation are thrown before the asynchronous work begins: [!code-csharp TaskExample ] This syntax is now supported: C# [field: SomeThingAboutFieldAttribute] public int SomeProperty { get ; set ; } The attribute SomeThingAboutFieldAttribute is applied to the compiler generated backing field for SomeProperty . For more information, see attributes in the C# programming guide. [!NOTE] Some of the designs that are supported by local functions can also be accomplished using lambda expressions . For more information, see Local functions vs. lambda expressions .","title":"Local functions"},{"location":"csharp/csharp-7/#more-expression-bodied-members","text":"C# 6 introduced expression-bodied members for member functions and read-only properties. C# 7.0 expands the allowed members that can be implemented as expressions. In C# 7.0, you can implement constructors , finalizers , and get and set accessors on properties and indexers . The following code shows examples of each: [!code-csharp ExpressionBodiedMembers ] [!NOTE] This example does not need a finalizer, but it is shown to demonstrate the syntax. You should not implement a finalizer in your class unless it is necessary to release unmanaged resources. You should also consider using the class instead of managing unmanaged resources directly. These new locations for expression-bodied members represent an important milestone for the C# language: These features were implemented by community members working on the open-source Roslyn project. Changing a method to an expression bodied member is a binary compatible change .","title":"More expression-bodied members"},{"location":"csharp/csharp-7/#throw-expressions","text":"In C#, throw has always been a statement. Because throw is a statement, not an expression, there were C# constructs where you couldn't use it. These included conditional expressions, null coalescing expressions, and some lambda expressions. The addition of expression-bodied members adds more locations where throw expressions would be useful. So that you can write any of these constructs, C# 7.0 introduces throw expressions . This addition makes it easier to write more expression-based code. You don't need additional statements for error checking.","title":"Throw expressions"},{"location":"csharp/csharp-7/#default-literal-expressions","text":"Default literal expressions are an enhancement to default value expressions. These expressions initialize a variable to the default value. Where you previously would write: C# Func < string , bool > whereClause = default ( Func < string , bool > ); You can now omit the type on the right-hand side of the initialization: C# Func < string , bool > whereClause = default ; For more information, see the default literal section of the default operator article.","title":"Default literal expressions"},{"location":"csharp/csharp-7/#numeric-literal-syntax-improvements","text":"Misreading numeric constants can make it harder to understand code when reading it for the first time. Bit masks or other symbolic values are prone to misunderstanding. C# 7.0 includes two new features to write numbers in the most readable fashion for the intended use: binary literals , and digit separators . For those times when you're creating bit masks, or whenever a binary representation of a number makes the most readable code, write that number in binary: [!code-csharp ThousandSeparators ] The 0b at the beginning of the constant indicates that the number is written as a binary number. Binary numbers can get long, so it's often easier to see the bit patterns by introducing the _ as a digit separator, as shown in the binary constant in the preceding example. The digit separator can appear anywhere in the constant. For base 10 numbers, it is common to use it as a thousands separator. Hex and binary numeric literals may begin with an _ : [!code-csharp LargeIntegers ] The digit separator can be used with decimal , float , and double types as well: [!code-csharp OtherConstants ] Taken together, you can declare numeric constants with much more readability.","title":"Numeric literal syntax improvements"},{"location":"csharp/csharp-7/#out-variables","text":"The existing syntax that supports out parameters has been improved in C# 7. You can now declare out variables in the argument list of a method call, rather than writing a separate declaration statement: [!code-csharp OutVariableDeclarations ] You may want to specify the type of the out variable for clarity, as shown in the preceding example. However, the language does support using an implicitly typed local variable: [!code-csharp OutVarVariableDeclarations ] The code is easier to read. You declare the out variable where you use it, not on a preceding line of code. No need to assign an initial value. By declaring the out variable where it's used in a method call, you can't accidentally use it before it is assigned. The syntax added in C# 7.0 to allow out variable declarations has been extended to include field initializers, property initializers, constructor initializers, and query clauses. It enables code such as the following example: C# public class B { public B ( int i , out int j ) { j = i ; } } public class D : B { public D ( int i ) : base ( i , out var j ) { Console . WriteLine ( $\"The value of 'j' is {j}\" ); } }","title":"out variables"},{"location":"csharp/csharp-7/#non-trailing-named-arguments","text":"Method calls may now use named arguments that precede positional arguments when those named arguments are in the correct positions. For more information, see Named and optional arguments .","title":"Non-trailing named arguments"},{"location":"csharp/csharp-7/#private-protected-access-modifier","text":"A new compound access modifier: private protected indicates that a member may be accessed by containing class or derived classes that are declared in the same assembly. While protected internal allows access by derived classes or classes that are in the same assembly, private protected limits access to derived types declared in the same assembly. For more information, see access modifiers in the language reference.","title":"private protected access modifier"},{"location":"csharp/csharp-7/#improved-overload-candidates","text":"In every release, the overload resolution rules get updated to address situations where ambiguous method invocations have an \"obvious\" choice. This release adds three new rules to help the compiler pick the obvious choice: When a method group contains both instance and static members, the compiler discards the instance members if the method was invoked without an instance receiver or context. The compiler discards the static members if the method was invoked with an instance receiver. When there is no receiver, the compiler includes only static members in a static context, otherwise both static and instance members. When the receiver is ambiguously an instance or type, the compiler includes both. A static context, where an implicit this instance receiver cannot be used, includes the body of members where no this is defined, such as static members, as well as places where this cannot be used, such as field initializers and constructor-initializers. When a method group contains some generic methods whose type arguments do not satisfy their constraints, these members are removed from the candidate set. For a method group conversion, candidate methods whose return type doesn't match up with the delegate's return type are removed from the set. You'll only notice this change because you'll find fewer compiler errors for ambiguous method overloads when you are sure which method is better.","title":"Improved overload candidates"},{"location":"csharp/csharp-7/#enabling-more-efficient-safe-code","text":"You should be able to write C# code safely that performs as well as unsafe code. Safe code avoids classes of errors, such as buffer overruns, stray pointers, and other memory access errors. These new features expand the capabilities of verifiable safe code. Strive to write more of your code using safe constructs. These features make that easier. The following new features support the theme of better performance for safe code: You can access fixed fields without pinning. You can reassign ref local variables. You can use initializers on stackalloc arrays. You can use fixed statements with any type that supports a pattern. You can use additional generic constraints. The in modifier on parameters, to specify that an argument is passed by reference but not modified by the called method. Adding the in modifier to an argument is a source compatible change . The ref readonly modifier on method returns, to indicate that a method returns its value by reference but doesn't allow writes to that object. Adding the ref readonly modifier is a source compatible change , if the return is assigned to a value. Adding the readonly modifier to an existing ref return statement is an incompatible change . It requires callers to update the declaration of ref local variables to include the readonly modifier. The readonly struct declaration, to indicate that a struct is immutable and should be passed as an in parameter to its member methods. Adding the readonly modifier to an existing struct declaration is a binary compatible change . The ref struct declaration, to indicate that a struct type accesses managed memory directly and must always be stack allocated. Adding the ref modifier to an existing struct declaration is an incompatible change . A ref struct cannot be a member of a class or used in other locations where it may be allocated on the heap. You can read more about all these changes in Write safe efficient code .","title":"Enabling more efficient safe code"},{"location":"csharp/csharp-7/#ref-locals-and-returns","text":"This feature enables algorithms that use and return references to variables defined elsewhere. One example is working with large matrices, and finding a single location with certain characteristics. The following method returns a reference to that storage in the matrix: [!code-csharp FindReturningRef ] You can declare the return value as a ref and modify that value in the matrix, as shown in the following code: [!code-csharp AssignRefReturn ] The C# language has several rules that protect you from misusing the ref locals and returns: You must add the ref keyword to the method signature and to all return statements in a method. That makes it clear the method returns by reference throughout the method. A ref return may be assigned to a value variable, or a ref variable. The caller controls whether the return value is copied or not. Omitting the ref modifier when assigning the return value indicates that the caller wants a copy of the value, not a reference to the storage. You can't assign a standard method return value to a ref local variable. That disallows statements like ref int i = sequence.Count(); You can't return a ref to a variable whose lifetime doesn't extend beyond the execution of the method. That means you can't return a reference to a local variable or a variable with a similar scope. ref locals and returns can't be used with async methods. The compiler can't know if the referenced variable has been set to its final value when the async method returns. The addition of ref locals and ref returns enables algorithms that are more efficient by avoiding copying values, or performing dereferencing operations multiple times. Adding ref to the return value is a source compatible change . Existing code compiles, but the ref return value is copied when assigned. Callers must update the storage for the return value to a ref local variable to store the return as a reference. Now, ref locals may be reassigned to refer to different instances after being initialized. The following code now compiles: C# ref VeryLargeStruct refLocal = ref veryLargeStruct ; // initialization refLocal = ref anotherVeryLargeStruct ; // reassigned, refLocal refers to different storage. For more information, see the article on ref returns and ref locals , and the article on foreach . For more information, see the ref keyword article.","title":"Ref locals and returns"},{"location":"csharp/csharp-7/#conditional-ref-expressions","text":"Finally, the conditional expression may produce a ref result instead of a value result. For example, you would write the following to retrieve a reference to the first element in one of two arrays: C# ref var r = ref ( arr != null ? ref arr [ 0 ] : ref otherArr [ 0 ]); The variable r is a reference to the first value in either arr or otherArr . For more information, see conditional operator (?:) in the language reference.","title":"Conditional ref expressions"},{"location":"csharp/csharp-7/#in-parameter-modifier","text":"The in keyword complements the existing ref and out keywords to pass arguments by reference. The in keyword specifies passing the argument by reference, but the called method doesn't modify the value. You may declare overloads that pass by value or by readonly reference, as shown in the following code: C# static void M ( S arg ); static void M ( in S arg ); The by value (first in the preceding example) overload is better than the by readonly reference version. To call the version with the readonly reference argument, you must include the in modifier when calling the method. For more information, see the article on the in parameter modifier .","title":"in parameter modifier"},{"location":"csharp/csharp-7/#more-types-support-the-fixed-statement","text":"The fixed statement supported a limited set of types. Starting with C# 7.3, any type that contains a GetPinnableReference() method that returns a ref T or ref readonly T may be fixed . Adding this feature means that fixed can be used with and related types. For more information, see the fixed statement article in the language reference.","title":"More types support the fixed statement"},{"location":"csharp/csharp-7/#indexing-fixed-fields-does-not-require-pinning","text":"Consider this struct: C# unsafe struct S { public fixed int myFixedField [ 10 ]; } In earlier versions of C#, you needed to pin a variable to access one of the integers that are part of myFixedField . Now, the following code compiles without pinning the variable p inside a separate fixed statement: C# class C { static S s = new S (); unsafe public void M () { int p = s . myFixedField [ 5 ]; } } The variable p accesses one element in myFixedField . You don't need to declare a separate int* variable. You still need an unsafe context. In earlier versions of C#, you need to declare a second fixed pointer: C# class C { static S s = new S (); unsafe public void M () { fixed ( int * ptr = s . myFixedField ) { int p = ptr [ 5 ]; } } } For more information, see the article on the fixed statement .","title":"Indexing fixed fields does not require pinning"},{"location":"csharp/csharp-7/#stackalloc-arrays-support-initializers","text":"You've been able to specify the values for elements in an array when you initialize it: C# var arr = new int [ 3 ] { 1 , 2 , 3 }; var arr2 = new int [] { 1 , 2 , 3 }; Now, that same syntax can be applied to arrays that are declared with stackalloc : C# int * pArr = stackalloc int [ 3 ] { 1 , 2 , 3 }; int * pArr2 = stackalloc int [] { 1 , 2 , 3 }; Span < int > arr = stackalloc [] { 1 , 2 , 3 }; For more information, see the stackalloc operator article.","title":"stackalloc arrays support initializers"},{"location":"csharp/csharp-7/#enhanced-generic-constraints","text":"You can now specify the type or as base class constraints for a type parameter. You can also use the new unmanaged constraint, to specify that a type parameter must be a non-nullable unmanaged type . For more information, see the articles on where generic constraints and constraints on type parameters . Adding these constraints to existing types is an incompatible change . Closed generic types may no longer meet these new constraints.","title":"Enhanced generic constraints"},{"location":"csharp/csharp-7/#generalized-async-return-types","text":"Returning a Task object from async methods can introduce performance bottlenecks in certain paths. Task is a reference type, so using it means allocating an object. In cases where a method declared with the async modifier returns a cached result, or completes synchronously, the extra allocations can become a significant time cost in performance critical sections of code. It can become costly if those allocations occur in tight loops. The new language feature means that async method return types aren't limited to Task , Task<T> , and void . The returned type must still satisfy the async pattern, meaning a GetAwaiter method must be accessible. As one concrete example, the ValueTask type has been added to .NET to make use of this new language feature: [!code-csharp UsingValueTask ] [!NOTE] You need to add the NuGet package System.Threading.Tasks.Extensions > in order to use the type. This enhancement is most useful for library authors to avoid allocating a Task in performance critical code.","title":"Generalized async return types"},{"location":"csharp/csharp-7/#new-compiler-options","text":"New compiler options support new build and DevOps scenarios for C# programs.","title":"New compiler options"},{"location":"csharp/csharp-7/#reference-assembly-generation","text":"There are two new compiler options that generate reference-only assemblies : ProduceReferenceAssembly and ProduceOnlyReferenceAssembly . The linked articles explain these options and reference assemblies in more detail.","title":"Reference assembly generation"},{"location":"csharp/csharp-7/#public-or-open-source-signing","text":"The PublicSign compiler option instructs the compiler to sign the assembly using a public key. The assembly is marked as signed, but the signature is taken from the public key. This option enables you to build signed assemblies from open-source projects using a public key. For more information, see the PublicSign compiler option article.","title":"Public or Open Source signing"},{"location":"csharp/csharp-7/#pathmap","text":"The PathMap compiler option instructs the compiler to replace source paths from the build environment with mapped source paths. The PathMap option controls the source path written by the compiler to PDB files or for the . For more information, see the PathMap compiler option article.","title":"pathmap"},{"location":"csharp/csharp-8/","text":"What's new in C# 8.0 \u00b6 C# 8.0 adds the following features and enhancements to the C# language: What's new in C# 8.0 Readonly members Default interface methods More patterns in more places Switch expressions Property patterns Tuple patterns Positional patterns Using declarations Static local functions Disposable ref structs Nullable reference types Asynchronous streams Asynchronous disposable Indices and ranges Null-coalescing assignment Unmanaged constructed types Stackalloc in nested expressions Enhancement of interpolated verbatim strings C# 8.0 is supported on .NET Core 3.x and .NET Standard 2.1 . For more information, see C# language versioning . The remainder of this article briefly describes these features. Where in-depth articles are available, links to those tutorials and overviews are provided. You can explore these features in your environment using the dotnet try global tool: Install the dotnet-try global tool. Clone the dotnet/try-samples repository. Set the current directory to the csharp8 subdirectory for the try-samples repository. Run dotnet try . Readonly members \u00b6 You can apply the readonly modifier to members of a struct. It indicates that the member doesn't modify state. It's more granular than applying the readonly modifier to a struct declaration. Consider the following mutable struct: C# public struct Point { public double X { get ; set ; } public double Y { get ; set ; } public double Distance => Math . Sqrt ( X * X + Y * Y ); public override string ToString () => $\"({X}, {Y}) is {Distance} from the origin\" ; } Like most structs, the ToString() method doesn't modify state. You could indicate that by adding the readonly modifier to the declaration of ToString() : C# public readonly override string ToString () => $\"({X}, {Y}) is {Distance} from the origin\" ; The preceding change generates a compiler warning, because ToString accesses the Distance property, which isn't marked readonly : Bash Session warning CS8656: Call to non-readonly member 'Point.Distance.get' from a 'readonly' member results in an implicit copy of 'this' The compiler warns you when it needs to create a defensive copy. The Distance property doesn't change state, so you can fix this warning by adding the readonly modifier to the declaration: C# public readonly double Distance => Math . Sqrt ( X * X + Y * Y ); Notice that the readonly modifier is necessary on a read-only property. The compiler doesn't assume get accessors don't modify state; you must declare readonly explicitly. Auto-implemented properties are an exception; the compiler will treat all auto-implemented getters as readonly , so here there's no need to add the readonly modifier to the X and Y properties. The compiler does enforce the rule that readonly members don't modify state. The following method won't compile unless you remove the readonly modifier: C# public readonly void Translate ( int xOffset , int yOffset ) { X += xOffset ; Y += yOffset ; } This feature lets you specify your design intent so the compiler can enforce it, and make optimizations based on that intent. For more information, see the readonly instance members section of the Structure types article. Default interface methods \u00b6 You can now add members to interfaces and provide an implementation for those members. This language feature enables API authors to add methods to an interface in later versions without breaking source or binary compatibility with existing implementations of that interface. Existing implementations inherit the default implementation. This feature also enables C# to interoperate with APIs that target Android or Swift, which support similar features. Default interface methods also enable scenarios similar to a \"traits\" language feature. Default interface methods affect many scenarios and language elements. Our first tutorial covers updating an interface with default implementations . More patterns in more places \u00b6 Pattern matching gives tools to provide shape-dependent functionality across related but different kinds of data. C# 7.0 introduced syntax for type patterns and constant patterns by using the is expression and the switch statement. These features represented the first tentative steps toward supporting programming paradigms where data and functionality live apart. As the industry moves toward more microservices and other cloud-based architectures, other language tools are needed. C# 8.0 expands this vocabulary so you can use more pattern expressions in more places in your code. Consider these features when your data and functionality are separate. Consider pattern matching when your algorithms depend on a fact other than the runtime type of an object. These techniques provide another way to express designs. In addition to new patterns in new places, C# 8.0 adds recursive patterns . The result of any pattern expression is an expression. A recursive pattern is simply a pattern expression applied to the output of another pattern expression. Switch expressions \u00b6 Often, a switch statement produces a value in each of its case blocks. Switch expressions enable you to use more concise expression syntax. There are fewer repetitive case and break keywords, and fewer curly braces. As an example, consider the following enum that lists the colors of the rainbow: C# public enum Rainbow { Red , Orange , Yellow , Green , Blue , Indigo , Violet } If your application defined an RGBColor type that is constructed from the R , G and B components, you could convert a Rainbow value to its RGB values using the following method containing a switch expression: C# public static RGBColor FromRainbow ( Rainbow colorBand ) => colorBand switch { Rainbow . Red => new RGBColor ( 0 xFF , 0 x00 , 0 x00 ), Rainbow . Orange => new RGBColor ( 0 xFF , 0 x7F , 0 x00 ), Rainbow . Yellow => new RGBColor ( 0 xFF , 0 xFF , 0 x00 ), Rainbow . Green => new RGBColor ( 0 x00 , 0 xFF , 0 x00 ), Rainbow . Blue => new RGBColor ( 0 x00 , 0 x00 , 0 xFF ), Rainbow . Indigo => new RGBColor ( 0 x4B , 0 x00 , 0 x82 ), Rainbow . Violet => new RGBColor ( 0 x94 , 0 x00 , 0 xD3 ), _ => throw new ArgumentException ( message : \"invalid enum value\" , paramName : nameof ( colorBand )), }; There are several syntax improvements here: The variable comes before the switch keyword. The different order makes it visually easy to distinguish the switch expression from the switch statement. The case and : elements are replaced with => . It's more concise and intuitive. The default case is replaced with a _ discard. The bodies are expressions, not statements. Contrast that with the equivalent code using the classic switch statement: C# public static RGBColor FromRainbowClassic ( Rainbow colorBand ) { switch ( colorBand ) { case Rainbow . Red : return new RGBColor ( 0 xFF , 0 x00 , 0 x00 ); case Rainbow . Orange : return new RGBColor ( 0 xFF , 0 x7F , 0 x00 ); case Rainbow . Yellow : return new RGBColor ( 0 xFF , 0 xFF , 0 x00 ); case Rainbow . Green : return new RGBColor ( 0 x00 , 0 xFF , 0 x00 ); case Rainbow . Blue : return new RGBColor ( 0 x00 , 0 x00 , 0 xFF ); case Rainbow . Indigo : return new RGBColor ( 0 x4B , 0 x00 , 0 x82 ); case Rainbow . Violet : return new RGBColor ( 0 x94 , 0 x00 , 0 xD3 ); default : throw new ArgumentException ( message : \"invalid enum value\" , paramName : nameof ( colorBand )); }; } Property patterns \u00b6 The property pattern enables you to match on properties of the object examined. Consider an eCommerce site that must compute sales tax based on the buyer's address. That computation isn't a core responsibility of an Address class. It will change over time, likely more often than address format changes. The amount of sales tax depends on the State property of the address. The following method uses the property pattern to compute the sales tax from the address and the price: C# public static decimal ComputeSalesTax ( Address location , decimal salePrice ) => location switch { { State : \"WA\" } => salePrice * 0.06 M , { State : \"MN\" } => salePrice * 0.075 M , { State : \"MI\" } => salePrice * 0.05 M , // other cases removed for brevity... _ => 0 M }; Pattern matching creates a concise syntax for expressing this algorithm. Tuple patterns \u00b6 Some algorithms depend on multiple inputs. Tuple patterns allow you to switch based on multiple values expressed as a tuple . The following code shows a switch expression for the game rock, paper, scissors : C# public static string RockPaperScissors ( string first , string second ) => ( first , second ) switch { ( \"rock\" , \"paper\" ) => \"rock is covered by paper. Paper wins.\" , ( \"rock\" , \"scissors\" ) => \"rock breaks scissors. Rock wins.\" , ( \"paper\" , \"rock\" ) => \"paper covers rock. Paper wins.\" , ( \"paper\" , \"scissors\" ) => \"paper is cut by scissors. Scissors wins.\" , ( \"scissors\" , \"rock\" ) => \"scissors is broken by rock. Rock wins.\" , ( \"scissors\" , \"paper\" ) => \"scissors cuts paper. Scissors wins.\" , ( _ , _ ) => \"tie\" }; The messages indicate the winner. The discard case represents the three combinations for ties, or other text inputs. Positional patterns \u00b6 Some types include a Deconstruct method that deconstructs its properties into discrete variables. When a Deconstruct method is accessible, you can use positional patterns to inspect properties of the object and use those properties for a pattern. Consider the following Point class that includes a Deconstruct method to create discrete variables for X and Y : C# public class Point { public int X { get ; } public int Y { get ; } public Point ( int x , int y ) => ( X , Y ) = ( x , y ); public void Deconstruct ( out int x , out int y ) => ( x , y ) = ( X , Y ); } Additionally, consider the following enum that represents various positions of a quadrant: C# public enum Quadrant { Unknown , Origin , One , Two , Three , Four , OnBorder } The following method uses the positional pattern to extract the values of x and y . Then, it uses a when clause to determine the Quadrant of the point: C# static Quadrant GetQuadrant ( Point point ) => point switch { ( 0 , 0 ) => Quadrant . Origin , var ( x , y ) when x > 0 && y > 0 => Quadrant . One , var ( x , y ) when x < 0 && y > 0 => Quadrant . Two , var ( x , y ) when x < 0 && y < 0 => Quadrant . Three , var ( x , y ) when x > 0 && y < 0 => Quadrant . Four , var ( _ , _ ) => Quadrant . OnBorder , _ => Quadrant . Unknown }; The discard pattern in the preceding switch matches when either x or y is 0, but not both. A switch expression must either produce a value or throw an exception. If none of the cases match, the switch expression throws an exception. The compiler generates a warning for you if you don't cover all possible cases in your switch expression. You can explore pattern matching techniques in this advanced tutorial on pattern matching . Using declarations \u00b6 A using declaration is a variable declaration preceded by the using keyword. It tells the compiler that the variable being declared should be disposed at the end of the enclosing scope. For example, consider the following code that writes a text file: C# static int WriteLinesToFile ( IEnumerable < string > lines ) { using var file = new System . IO . StreamWriter ( \"WriteLines2.txt\" ); int skippedLines = 0 ; foreach ( string line in lines ) { if ( ! line . Contains ( \"Second\" )) { file . WriteLine ( line ); } else { skippedLines ++ ; } } // Notice how skippedLines is in scope here. return skippedLines ; // file is disposed here } In the preceding example, the file is disposed when the closing brace for the method is reached. That's the end of the scope in which file is declared. The preceding code is equivalent to the following code that uses the classic using statement : C# static int WriteLinesToFile ( IEnumerable < string > lines ) { using ( var file = new System . IO . StreamWriter ( \"WriteLines2.txt\" )) { int skippedLines = 0 ; foreach ( string line in lines ) { if ( ! line . Contains ( \"Second\" )) { file . WriteLine ( line ); } else { skippedLines ++ ; } } return skippedLines ; } // file is disposed here } In the preceding example, the file is disposed when the closing brace associated with the using statement is reached. In both cases, the compiler generates the call to Dispose() . The compiler generates an error if the expression in the using statement isn't disposable. Static local functions \u00b6 You can now add the static modifier to local functions to ensure that local function doesn't capture (reference) any variables from the enclosing scope. Doing so generates CS8421 , \"A static local function can't contain a reference to \\ .\" Consider the following code. The local function LocalFunction accesses the variable y , declared in the enclosing scope (the method M ). Therefore, LocalFunction can't be declared with the static modifier: C# int M () { int y ; LocalFunction (); return y ; void LocalFunction () => y = 0 ; } The following code contains a static local function. It can be static because it doesn't access any variables in the enclosing scope: C# int M () { int y = 5 ; int x = 7 ; return Add ( x , y ); static int Add ( int left , int right ) => left + right ; } Disposable ref structs \u00b6 A struct declared with the ref modifier may not implement any interfaces and so can't implement . Therefore, to enable a ref struct to be disposed, it must have an accessible void Dispose() method. This feature also applies to readonly ref struct declarations. Nullable reference types \u00b6 Inside a nullable annotation context, any variable of a reference type is considered to be a nonnullable reference type . If you want to indicate that a variable may be null, you must append the type name with the ? to declare the variable as a nullable reference type . For nonnullable reference types, the compiler uses flow analysis to ensure that local variables are initialized to a non-null value when declared. Fields must be initialized during construction. The compiler generates a warning if the variable isn't set by a call to any of the available constructors or by an initializer. Furthermore, nonnullable reference types can't be assigned a value that could be null. Nullable reference types aren't checked to ensure they aren't assigned or initialized to null. However, the compiler uses flow analysis to ensure that any variable of a nullable reference type is checked against null before it's accessed or assigned to a nonnullable reference type. You can learn more about the feature in the overview of nullable reference types . Try it yourself in a new application in this nullable reference types tutorial . Learn about the steps to migrate an existing codebase to make use of nullable reference types in the migrating an application to use nullable reference types tutorial . Asynchronous streams \u00b6 Starting with C# 8.0, you can create and consume streams asynchronously. A method that returns an asynchronous stream has three properties: It's declared with the async modifier. It returns an . The method contains yield return statements to return successive elements in the asynchronous stream. Consuming an asynchronous stream requires you to add the await keyword before the foreach keyword when you enumerate the elements of the stream. Adding the await keyword requires the method that enumerates the asynchronous stream to be declared with the async modifier and to return a type allowed for an async method. Typically that means returning a or . It can also be a or . A method can both consume and produce an asynchronous stream, which means it would return an . The following code generates a sequence from 0 to 19, waiting 100 ms between generating each number: C# public static async System . Collections . Generic . IAsyncEnumerable < int > GenerateSequence () { for ( int i = 0 ; i < 20 ; i ++ ) { await Task . Delay ( 100 ); yield return i ; } } You would enumerate the sequence using the await foreach statement: C# await foreach ( var number in GenerateSequence ()) { Console . WriteLine ( number ); } You can try asynchronous streams yourself in our tutorial on creating and consuming async streams . By default, stream elements are processed in the captured context. If you want to disable capturing of the context, use the extension method. For more information about synchronization contexts and capturing the current context, see the article on consuming the Task-based asynchronous pattern . Asynchronous disposable \u00b6 Starting with C# 8.0, the language supports asynchronous disposable types that implement the interface. You use the await using statement to work with an asynchronously disposable object. For more information, see the Implement a DisposeAsync method article. Indices and ranges \u00b6 Indices and ranges provide a succinct syntax for accessing single elements or ranges in a sequence. This language support relies on two new types, and two new operators: represents an index into a sequence. The index from end operator ^ , which specifies that an index is relative to the end of the sequence. represents a sub range of a sequence. The range operator .. , which specifies the start and end of a range as its operands. Let's start with the rules for indexes. Consider an array sequence . The 0 index is the same as sequence[0] . The ^0 index is the same as sequence[sequence.Length] . Note that sequence[^0] does throw an exception, just as sequence[sequence.Length] does. For any number n , the index ^n is the same as sequence.Length - n . A range specifies the start and end of a range. The start of the range is inclusive, but the end of the range is exclusive, meaning the start is included in the range but the end isn't included in the range. The range [0..^0] represents the entire range, just as [0..sequence.Length] represents the entire range. Let's look at a few examples. Consider the following array, annotated with its index from the start and from the end: C# var words = new string [] { // index from start index from end \"The\" , // 0 ^9 \"quick\" , // 1 ^8 \"brown\" , // 2 ^7 \"fox\" , // 3 ^6 \"jumped\" , // 4 ^5 \"over\" , // 5 ^4 \"the\" , // 6 ^3 \"lazy\" , // 7 ^2 \"dog\" // 8 ^1 }; // 9 (or words.Length) ^0 You can retrieve the last word with the ^1 index: C# Console . WriteLine ( $\"The last word is {words[^1]}\" ); // writes \"dog\" The following code creates a subrange with the words \"quick\", \"brown\", and \"fox\". It includes words[1] through words[3] . The element words[4] isn't in the range. C# var quickBrownFox = words [ 1. . 4 ]; The following code creates a subrange with \"lazy\" and \"dog\". It includes words[^2] and words[^1] . The end index words[^0] isn't included: C# var lazyDog = words [ ^ 2. . ^ 0 ]; The following examples create ranges that are open ended for the start, end, or both: C# var allWords = words [..]; // contains \"The\" through \"dog\". var firstPhrase = words [.. 4 ]; // contains \"The\" through \"fox\" var lastPhrase = words [ 6. .]; // contains \"the\", \"lazy\" and \"dog\" You can also declare ranges as variables: C# Range phrase = 1. . 4 ; The range can then be used inside the [ and ] characters: C# var text = words [ phrase ]; Not only arrays support indices and ranges. You can also use indices and ranges with string , , or . For more information, see Type support for indices and ranges . You can explore more about indices and ranges in the tutorial on indices and ranges . Null-coalescing assignment \u00b6 C# 8.0 introduces the null-coalescing assignment operator ??= . You can use the ??= operator to assign the value of its right-hand operand to its left-hand operand only if the left-hand operand evaluates to null . C# List < int > numbers = null ; int? i = null ; numbers ??= new List < int > (); numbers . Add ( i ??= 17 ); numbers . Add ( i ??= 20 ); Console . WriteLine ( string . Join ( \" \" , numbers )); // output: 17 17 Console . WriteLine ( i ); // output: 17 For more information, see the ?? and ??= operators article. Unmanaged constructed types \u00b6 In C# 7.3 and earlier, a constructed type (a type that includes at least one type argument) can't be an unmanaged type . Starting with C# 8.0, a constructed value type is unmanaged if it contains fields of unmanaged types only. For example, given the following definition of the generic Coords<T> type: C# public struct Coords < T > { public T X ; public T Y ; } the Coords<int> type is an unmanaged type in C# 8.0 and later. Like for any unmanaged type, you can create a pointer to a variable of this type or allocate a block of memory on the stack for instances of this type: C# Span < Coords < int >> coordinates = stackalloc [] { new Coords < int > { X = 0 , Y = 0 }, new Coords < int > { X = 0 , Y = 3 }, new Coords < int > { X = 4 , Y = 0 } }; For more information, see Unmanaged types . Stackalloc in nested expressions \u00b6 Starting with C# 8.0, if the result of a stackalloc expression is of the or type, you can use the stackalloc expression in other expressions: C# Span < int > numbers = stackalloc [] { 1 , 2 , 3 , 4 , 5 , 6 }; var ind = numbers . IndexOfAny ( stackalloc [] { 2 , 4 , 6 , 8 }); Console . WriteLine ( ind ); // output: 1 Enhancement of interpolated verbatim strings \u00b6 Order of the $ and @ tokens in interpolated verbatim strings can be any: both $@\"...\" and @$\"...\" are valid interpolated verbatim strings. In earlier C# versions, the $ token must appear before the @ token.","title":"What's new in C# 8.0"},{"location":"csharp/csharp-8/#whats-new-in-c-80","text":"C# 8.0 adds the following features and enhancements to the C# language: What's new in C# 8.0 Readonly members Default interface methods More patterns in more places Switch expressions Property patterns Tuple patterns Positional patterns Using declarations Static local functions Disposable ref structs Nullable reference types Asynchronous streams Asynchronous disposable Indices and ranges Null-coalescing assignment Unmanaged constructed types Stackalloc in nested expressions Enhancement of interpolated verbatim strings C# 8.0 is supported on .NET Core 3.x and .NET Standard 2.1 . For more information, see C# language versioning . The remainder of this article briefly describes these features. Where in-depth articles are available, links to those tutorials and overviews are provided. You can explore these features in your environment using the dotnet try global tool: Install the dotnet-try global tool. Clone the dotnet/try-samples repository. Set the current directory to the csharp8 subdirectory for the try-samples repository. Run dotnet try .","title":"What's new in C# 8.0"},{"location":"csharp/csharp-8/#readonly-members","text":"You can apply the readonly modifier to members of a struct. It indicates that the member doesn't modify state. It's more granular than applying the readonly modifier to a struct declaration. Consider the following mutable struct: C# public struct Point { public double X { get ; set ; } public double Y { get ; set ; } public double Distance => Math . Sqrt ( X * X + Y * Y ); public override string ToString () => $\"({X}, {Y}) is {Distance} from the origin\" ; } Like most structs, the ToString() method doesn't modify state. You could indicate that by adding the readonly modifier to the declaration of ToString() : C# public readonly override string ToString () => $\"({X}, {Y}) is {Distance} from the origin\" ; The preceding change generates a compiler warning, because ToString accesses the Distance property, which isn't marked readonly : Bash Session warning CS8656: Call to non-readonly member 'Point.Distance.get' from a 'readonly' member results in an implicit copy of 'this' The compiler warns you when it needs to create a defensive copy. The Distance property doesn't change state, so you can fix this warning by adding the readonly modifier to the declaration: C# public readonly double Distance => Math . Sqrt ( X * X + Y * Y ); Notice that the readonly modifier is necessary on a read-only property. The compiler doesn't assume get accessors don't modify state; you must declare readonly explicitly. Auto-implemented properties are an exception; the compiler will treat all auto-implemented getters as readonly , so here there's no need to add the readonly modifier to the X and Y properties. The compiler does enforce the rule that readonly members don't modify state. The following method won't compile unless you remove the readonly modifier: C# public readonly void Translate ( int xOffset , int yOffset ) { X += xOffset ; Y += yOffset ; } This feature lets you specify your design intent so the compiler can enforce it, and make optimizations based on that intent. For more information, see the readonly instance members section of the Structure types article.","title":"Readonly members"},{"location":"csharp/csharp-8/#default-interface-methods","text":"You can now add members to interfaces and provide an implementation for those members. This language feature enables API authors to add methods to an interface in later versions without breaking source or binary compatibility with existing implementations of that interface. Existing implementations inherit the default implementation. This feature also enables C# to interoperate with APIs that target Android or Swift, which support similar features. Default interface methods also enable scenarios similar to a \"traits\" language feature. Default interface methods affect many scenarios and language elements. Our first tutorial covers updating an interface with default implementations .","title":"Default interface methods"},{"location":"csharp/csharp-8/#more-patterns-in-more-places","text":"Pattern matching gives tools to provide shape-dependent functionality across related but different kinds of data. C# 7.0 introduced syntax for type patterns and constant patterns by using the is expression and the switch statement. These features represented the first tentative steps toward supporting programming paradigms where data and functionality live apart. As the industry moves toward more microservices and other cloud-based architectures, other language tools are needed. C# 8.0 expands this vocabulary so you can use more pattern expressions in more places in your code. Consider these features when your data and functionality are separate. Consider pattern matching when your algorithms depend on a fact other than the runtime type of an object. These techniques provide another way to express designs. In addition to new patterns in new places, C# 8.0 adds recursive patterns . The result of any pattern expression is an expression. A recursive pattern is simply a pattern expression applied to the output of another pattern expression.","title":"More patterns in more places"},{"location":"csharp/csharp-8/#switch-expressions","text":"Often, a switch statement produces a value in each of its case blocks. Switch expressions enable you to use more concise expression syntax. There are fewer repetitive case and break keywords, and fewer curly braces. As an example, consider the following enum that lists the colors of the rainbow: C# public enum Rainbow { Red , Orange , Yellow , Green , Blue , Indigo , Violet } If your application defined an RGBColor type that is constructed from the R , G and B components, you could convert a Rainbow value to its RGB values using the following method containing a switch expression: C# public static RGBColor FromRainbow ( Rainbow colorBand ) => colorBand switch { Rainbow . Red => new RGBColor ( 0 xFF , 0 x00 , 0 x00 ), Rainbow . Orange => new RGBColor ( 0 xFF , 0 x7F , 0 x00 ), Rainbow . Yellow => new RGBColor ( 0 xFF , 0 xFF , 0 x00 ), Rainbow . Green => new RGBColor ( 0 x00 , 0 xFF , 0 x00 ), Rainbow . Blue => new RGBColor ( 0 x00 , 0 x00 , 0 xFF ), Rainbow . Indigo => new RGBColor ( 0 x4B , 0 x00 , 0 x82 ), Rainbow . Violet => new RGBColor ( 0 x94 , 0 x00 , 0 xD3 ), _ => throw new ArgumentException ( message : \"invalid enum value\" , paramName : nameof ( colorBand )), }; There are several syntax improvements here: The variable comes before the switch keyword. The different order makes it visually easy to distinguish the switch expression from the switch statement. The case and : elements are replaced with => . It's more concise and intuitive. The default case is replaced with a _ discard. The bodies are expressions, not statements. Contrast that with the equivalent code using the classic switch statement: C# public static RGBColor FromRainbowClassic ( Rainbow colorBand ) { switch ( colorBand ) { case Rainbow . Red : return new RGBColor ( 0 xFF , 0 x00 , 0 x00 ); case Rainbow . Orange : return new RGBColor ( 0 xFF , 0 x7F , 0 x00 ); case Rainbow . Yellow : return new RGBColor ( 0 xFF , 0 xFF , 0 x00 ); case Rainbow . Green : return new RGBColor ( 0 x00 , 0 xFF , 0 x00 ); case Rainbow . Blue : return new RGBColor ( 0 x00 , 0 x00 , 0 xFF ); case Rainbow . Indigo : return new RGBColor ( 0 x4B , 0 x00 , 0 x82 ); case Rainbow . Violet : return new RGBColor ( 0 x94 , 0 x00 , 0 xD3 ); default : throw new ArgumentException ( message : \"invalid enum value\" , paramName : nameof ( colorBand )); }; }","title":"Switch expressions"},{"location":"csharp/csharp-8/#property-patterns","text":"The property pattern enables you to match on properties of the object examined. Consider an eCommerce site that must compute sales tax based on the buyer's address. That computation isn't a core responsibility of an Address class. It will change over time, likely more often than address format changes. The amount of sales tax depends on the State property of the address. The following method uses the property pattern to compute the sales tax from the address and the price: C# public static decimal ComputeSalesTax ( Address location , decimal salePrice ) => location switch { { State : \"WA\" } => salePrice * 0.06 M , { State : \"MN\" } => salePrice * 0.075 M , { State : \"MI\" } => salePrice * 0.05 M , // other cases removed for brevity... _ => 0 M }; Pattern matching creates a concise syntax for expressing this algorithm.","title":"Property patterns"},{"location":"csharp/csharp-8/#tuple-patterns","text":"Some algorithms depend on multiple inputs. Tuple patterns allow you to switch based on multiple values expressed as a tuple . The following code shows a switch expression for the game rock, paper, scissors : C# public static string RockPaperScissors ( string first , string second ) => ( first , second ) switch { ( \"rock\" , \"paper\" ) => \"rock is covered by paper. Paper wins.\" , ( \"rock\" , \"scissors\" ) => \"rock breaks scissors. Rock wins.\" , ( \"paper\" , \"rock\" ) => \"paper covers rock. Paper wins.\" , ( \"paper\" , \"scissors\" ) => \"paper is cut by scissors. Scissors wins.\" , ( \"scissors\" , \"rock\" ) => \"scissors is broken by rock. Rock wins.\" , ( \"scissors\" , \"paper\" ) => \"scissors cuts paper. Scissors wins.\" , ( _ , _ ) => \"tie\" }; The messages indicate the winner. The discard case represents the three combinations for ties, or other text inputs.","title":"Tuple patterns"},{"location":"csharp/csharp-8/#positional-patterns","text":"Some types include a Deconstruct method that deconstructs its properties into discrete variables. When a Deconstruct method is accessible, you can use positional patterns to inspect properties of the object and use those properties for a pattern. Consider the following Point class that includes a Deconstruct method to create discrete variables for X and Y : C# public class Point { public int X { get ; } public int Y { get ; } public Point ( int x , int y ) => ( X , Y ) = ( x , y ); public void Deconstruct ( out int x , out int y ) => ( x , y ) = ( X , Y ); } Additionally, consider the following enum that represents various positions of a quadrant: C# public enum Quadrant { Unknown , Origin , One , Two , Three , Four , OnBorder } The following method uses the positional pattern to extract the values of x and y . Then, it uses a when clause to determine the Quadrant of the point: C# static Quadrant GetQuadrant ( Point point ) => point switch { ( 0 , 0 ) => Quadrant . Origin , var ( x , y ) when x > 0 && y > 0 => Quadrant . One , var ( x , y ) when x < 0 && y > 0 => Quadrant . Two , var ( x , y ) when x < 0 && y < 0 => Quadrant . Three , var ( x , y ) when x > 0 && y < 0 => Quadrant . Four , var ( _ , _ ) => Quadrant . OnBorder , _ => Quadrant . Unknown }; The discard pattern in the preceding switch matches when either x or y is 0, but not both. A switch expression must either produce a value or throw an exception. If none of the cases match, the switch expression throws an exception. The compiler generates a warning for you if you don't cover all possible cases in your switch expression. You can explore pattern matching techniques in this advanced tutorial on pattern matching .","title":"Positional patterns"},{"location":"csharp/csharp-8/#using-declarations","text":"A using declaration is a variable declaration preceded by the using keyword. It tells the compiler that the variable being declared should be disposed at the end of the enclosing scope. For example, consider the following code that writes a text file: C# static int WriteLinesToFile ( IEnumerable < string > lines ) { using var file = new System . IO . StreamWriter ( \"WriteLines2.txt\" ); int skippedLines = 0 ; foreach ( string line in lines ) { if ( ! line . Contains ( \"Second\" )) { file . WriteLine ( line ); } else { skippedLines ++ ; } } // Notice how skippedLines is in scope here. return skippedLines ; // file is disposed here } In the preceding example, the file is disposed when the closing brace for the method is reached. That's the end of the scope in which file is declared. The preceding code is equivalent to the following code that uses the classic using statement : C# static int WriteLinesToFile ( IEnumerable < string > lines ) { using ( var file = new System . IO . StreamWriter ( \"WriteLines2.txt\" )) { int skippedLines = 0 ; foreach ( string line in lines ) { if ( ! line . Contains ( \"Second\" )) { file . WriteLine ( line ); } else { skippedLines ++ ; } } return skippedLines ; } // file is disposed here } In the preceding example, the file is disposed when the closing brace associated with the using statement is reached. In both cases, the compiler generates the call to Dispose() . The compiler generates an error if the expression in the using statement isn't disposable.","title":"Using declarations"},{"location":"csharp/csharp-8/#static-local-functions","text":"You can now add the static modifier to local functions to ensure that local function doesn't capture (reference) any variables from the enclosing scope. Doing so generates CS8421 , \"A static local function can't contain a reference to \\ .\" Consider the following code. The local function LocalFunction accesses the variable y , declared in the enclosing scope (the method M ). Therefore, LocalFunction can't be declared with the static modifier: C# int M () { int y ; LocalFunction (); return y ; void LocalFunction () => y = 0 ; } The following code contains a static local function. It can be static because it doesn't access any variables in the enclosing scope: C# int M () { int y = 5 ; int x = 7 ; return Add ( x , y ); static int Add ( int left , int right ) => left + right ; }","title":"Static local functions"},{"location":"csharp/csharp-8/#disposable-ref-structs","text":"A struct declared with the ref modifier may not implement any interfaces and so can't implement . Therefore, to enable a ref struct to be disposed, it must have an accessible void Dispose() method. This feature also applies to readonly ref struct declarations.","title":"Disposable ref structs"},{"location":"csharp/csharp-8/#nullable-reference-types","text":"Inside a nullable annotation context, any variable of a reference type is considered to be a nonnullable reference type . If you want to indicate that a variable may be null, you must append the type name with the ? to declare the variable as a nullable reference type . For nonnullable reference types, the compiler uses flow analysis to ensure that local variables are initialized to a non-null value when declared. Fields must be initialized during construction. The compiler generates a warning if the variable isn't set by a call to any of the available constructors or by an initializer. Furthermore, nonnullable reference types can't be assigned a value that could be null. Nullable reference types aren't checked to ensure they aren't assigned or initialized to null. However, the compiler uses flow analysis to ensure that any variable of a nullable reference type is checked against null before it's accessed or assigned to a nonnullable reference type. You can learn more about the feature in the overview of nullable reference types . Try it yourself in a new application in this nullable reference types tutorial . Learn about the steps to migrate an existing codebase to make use of nullable reference types in the migrating an application to use nullable reference types tutorial .","title":"Nullable reference types"},{"location":"csharp/csharp-8/#asynchronous-streams","text":"Starting with C# 8.0, you can create and consume streams asynchronously. A method that returns an asynchronous stream has three properties: It's declared with the async modifier. It returns an . The method contains yield return statements to return successive elements in the asynchronous stream. Consuming an asynchronous stream requires you to add the await keyword before the foreach keyword when you enumerate the elements of the stream. Adding the await keyword requires the method that enumerates the asynchronous stream to be declared with the async modifier and to return a type allowed for an async method. Typically that means returning a or . It can also be a or . A method can both consume and produce an asynchronous stream, which means it would return an . The following code generates a sequence from 0 to 19, waiting 100 ms between generating each number: C# public static async System . Collections . Generic . IAsyncEnumerable < int > GenerateSequence () { for ( int i = 0 ; i < 20 ; i ++ ) { await Task . Delay ( 100 ); yield return i ; } } You would enumerate the sequence using the await foreach statement: C# await foreach ( var number in GenerateSequence ()) { Console . WriteLine ( number ); } You can try asynchronous streams yourself in our tutorial on creating and consuming async streams . By default, stream elements are processed in the captured context. If you want to disable capturing of the context, use the extension method. For more information about synchronization contexts and capturing the current context, see the article on consuming the Task-based asynchronous pattern .","title":"Asynchronous streams"},{"location":"csharp/csharp-8/#asynchronous-disposable","text":"Starting with C# 8.0, the language supports asynchronous disposable types that implement the interface. You use the await using statement to work with an asynchronously disposable object. For more information, see the Implement a DisposeAsync method article.","title":"Asynchronous disposable"},{"location":"csharp/csharp-8/#indices-and-ranges","text":"Indices and ranges provide a succinct syntax for accessing single elements or ranges in a sequence. This language support relies on two new types, and two new operators: represents an index into a sequence. The index from end operator ^ , which specifies that an index is relative to the end of the sequence. represents a sub range of a sequence. The range operator .. , which specifies the start and end of a range as its operands. Let's start with the rules for indexes. Consider an array sequence . The 0 index is the same as sequence[0] . The ^0 index is the same as sequence[sequence.Length] . Note that sequence[^0] does throw an exception, just as sequence[sequence.Length] does. For any number n , the index ^n is the same as sequence.Length - n . A range specifies the start and end of a range. The start of the range is inclusive, but the end of the range is exclusive, meaning the start is included in the range but the end isn't included in the range. The range [0..^0] represents the entire range, just as [0..sequence.Length] represents the entire range. Let's look at a few examples. Consider the following array, annotated with its index from the start and from the end: C# var words = new string [] { // index from start index from end \"The\" , // 0 ^9 \"quick\" , // 1 ^8 \"brown\" , // 2 ^7 \"fox\" , // 3 ^6 \"jumped\" , // 4 ^5 \"over\" , // 5 ^4 \"the\" , // 6 ^3 \"lazy\" , // 7 ^2 \"dog\" // 8 ^1 }; // 9 (or words.Length) ^0 You can retrieve the last word with the ^1 index: C# Console . WriteLine ( $\"The last word is {words[^1]}\" ); // writes \"dog\" The following code creates a subrange with the words \"quick\", \"brown\", and \"fox\". It includes words[1] through words[3] . The element words[4] isn't in the range. C# var quickBrownFox = words [ 1. . 4 ]; The following code creates a subrange with \"lazy\" and \"dog\". It includes words[^2] and words[^1] . The end index words[^0] isn't included: C# var lazyDog = words [ ^ 2. . ^ 0 ]; The following examples create ranges that are open ended for the start, end, or both: C# var allWords = words [..]; // contains \"The\" through \"dog\". var firstPhrase = words [.. 4 ]; // contains \"The\" through \"fox\" var lastPhrase = words [ 6. .]; // contains \"the\", \"lazy\" and \"dog\" You can also declare ranges as variables: C# Range phrase = 1. . 4 ; The range can then be used inside the [ and ] characters: C# var text = words [ phrase ]; Not only arrays support indices and ranges. You can also use indices and ranges with string , , or . For more information, see Type support for indices and ranges . You can explore more about indices and ranges in the tutorial on indices and ranges .","title":"Indices and ranges"},{"location":"csharp/csharp-8/#null-coalescing-assignment","text":"C# 8.0 introduces the null-coalescing assignment operator ??= . You can use the ??= operator to assign the value of its right-hand operand to its left-hand operand only if the left-hand operand evaluates to null . C# List < int > numbers = null ; int? i = null ; numbers ??= new List < int > (); numbers . Add ( i ??= 17 ); numbers . Add ( i ??= 20 ); Console . WriteLine ( string . Join ( \" \" , numbers )); // output: 17 17 Console . WriteLine ( i ); // output: 17 For more information, see the ?? and ??= operators article.","title":"Null-coalescing assignment"},{"location":"csharp/csharp-8/#unmanaged-constructed-types","text":"In C# 7.3 and earlier, a constructed type (a type that includes at least one type argument) can't be an unmanaged type . Starting with C# 8.0, a constructed value type is unmanaged if it contains fields of unmanaged types only. For example, given the following definition of the generic Coords<T> type: C# public struct Coords < T > { public T X ; public T Y ; } the Coords<int> type is an unmanaged type in C# 8.0 and later. Like for any unmanaged type, you can create a pointer to a variable of this type or allocate a block of memory on the stack for instances of this type: C# Span < Coords < int >> coordinates = stackalloc [] { new Coords < int > { X = 0 , Y = 0 }, new Coords < int > { X = 0 , Y = 3 }, new Coords < int > { X = 4 , Y = 0 } }; For more information, see Unmanaged types .","title":"Unmanaged constructed types"},{"location":"csharp/csharp-8/#stackalloc-in-nested-expressions","text":"Starting with C# 8.0, if the result of a stackalloc expression is of the or type, you can use the stackalloc expression in other expressions: C# Span < int > numbers = stackalloc [] { 1 , 2 , 3 , 4 , 5 , 6 }; var ind = numbers . IndexOfAny ( stackalloc [] { 2 , 4 , 6 , 8 }); Console . WriteLine ( ind ); // output: 1","title":"Stackalloc in nested expressions"},{"location":"csharp/csharp-8/#enhancement-of-interpolated-verbatim-strings","text":"Order of the $ and @ tokens in interpolated verbatim strings can be any: both $@\"...\" and @$\"...\" are valid interpolated verbatim strings. In earlier C# versions, the $ token must appear before the @ token.","title":"Enhancement of interpolated verbatim strings"},{"location":"csharp/csharp-9/","text":"What's new in C# 9.0 \u00b6 C# 9.0 adds the following features and enhancements to the C# language: Records Init only setters Top-level statements Pattern matching enhancements Performance and interop Native sized integers Function pointers Suppress emitting localsinit flag Fit and finish features Target-typed new expressions static anonymous functions Target-typed conditional expressions Covariant return types Extension GetEnumerator support for foreach loops Lambda discard parameters Attributes on local functions Support for code generators Module initializers New features for partial methods Warning wave 5 C# 9.0 is supported on .NET 5 . For more information, see C# language versioning . You can download the latest .NET SDK from the .NET downloads page . Record types \u00b6 C# 9.0 introduces record types . You use the record keyword to define a reference type that provides built-in functionality for encapsulating data. You can create record types with immutable properties by using positional parameters or standard property syntax: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"PositionalRecord\"::: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"ImmutableRecord\"::: You can also create record types with mutable properties and fields: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"MutableRecord\"::: While records can be mutable, they are primarily intended for supporting immutable data models. The record type offers the following features: What's new in C# 9.0 Record types Positional syntax for property definition Immutability Value equality Nondestructive mutation Built-in formatting for display Inheritance Init only setters Top-level statements Pattern matching enhancements Performance and interop Fit and finish features Support for code generators You can use structure types to design data-centric types that provide value equality and little or no behavior. But for relatively large data models, structure types have some disadvantages: They don't support inheritance. They're less efficient at determining value equality. For value types, the method uses reflection to find all fields. For records, the compiler generates the Equals method. In practice, the implementation of value equality in records is measurably faster. They use more memory in some scenarios, since every instance has a complete copy of all of the data. Record types are reference types , so a record instance contains only a reference to the data. Positional syntax for property definition \u00b6 You can use positional parameters to declare properties of a record and to initialize the property values when you create an instance: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"InstantiatePositional\"::: When you use the positional syntax for property definition, the compiler creates: A public init-only auto-implemented property for each positional parameter provided in the record declaration. An init-only property can only be set in the constructor or by using a property initializer. A primary constructor whose parameters match the positional parameters on the record declaration. A Deconstruct method with an out parameter for each positional parameter provided in the record declaration. For more information, see Positional syntax in the C# language reference article about records. Immutability \u00b6 A record type is not necessarily immutable. You can declare properties with set accessors and fields that aren't readonly . But while records can be mutable, they make it easier to create immutable data models. Properties that you create by using positional syntax are immutable. Immutability can be useful when you want a data-centric type to be thread-safe or a hash code to remain the same in a hash table. It can prevent bugs that happen when you pass an argument by reference to a method, and the method unexpectedly changes the argument value. The features unique to record types are implemented by compiler-synthesized methods, and none of these methods compromises immutability by modifying object state. Value equality \u00b6 Value equality means that two variables of a record type are equal if the types match and all property and field values match. For other reference types, equality means identity. That is, two variables of a reference type are equal if they refer to the same object. The following example illustrates value equality of record types: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"Equality\"::: In class types, you could manually override equality methods and operators to achieve value equality, but developing and testing that code would be time-consuming and error-prone. Having this functionality built-in prevents bugs that would result from forgetting to update custom override code when properties or fields are added or changed. For more information, see Value equality in the C# language reference article about records. Nondestructive mutation \u00b6 If you need to mutate immutable properties of a record instance, you can use a with expression to achieve nondestructive mutation . A with expression makes a new record instance that is a copy of an existing record instance, with specified properties and fields modified. You use object initializer syntax to specify the values to be changed, as shown in the following example: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"WithExpressions\"::: For more information, see Nondestructive mutation in the C# language reference article about records. Built-in formatting for display \u00b6 Record types have a compiler-generated method that displays the names and values of public properties and fields. The ToString method returns a string of the following format: \\ { \\ = \\ , \\ = \\ , ...} For reference types, the type name of the object that the property refers to is displayed instead of the property value. In the following example, the array is a reference type, so System.String[] is displayed instead of the actual array element values: Text Only Person { FirstName = Nancy, LastName = Davolio, ChildNames = System.String[] } For more information, see Built-in formatting in the C# language reference article about records. Inheritance \u00b6 A record can inherit from another record. However, a record can't inherit from a class, and a class can't inherit from a record. The following example illustrates inheritance with positional property syntax: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"PositionalInheritance\"::: For two record variables to be equal, the run-time type must be equal. The types of the containing variables might be different. This is illustrated in the following code example: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"InheritanceEquality\"::: In the example, all instances have the same properties and the same property values. But student == teacher returns False although both are Person -type variables. And student == student2 returns True although one is a Person variable and one is a Student variable. All public properties and fields of both derived and base types are included in the ToString output, as shown in the following example: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"ToStringInheritance\"::: For more information, see Inheritance in the C# language reference article about records. Init only setters \u00b6 Init only setters provide consistent syntax to initialize members of an object. Property initializers make it clear which value is setting which property. The downside is that those properties must be settable. Starting with C# 9.0, you can create init accessors instead of set accessors for properties and indexers. Callers can use property initializer syntax to set these values in creation expressions, but those properties are readonly once construction has completed. Init only setters provide a window to change state. That window closes when the construction phase ends. The construction phase effectively ends after all initialization, including property initializers and with-expressions have completed. You can declare init only setters in any type you write. For example, the following struct defines a weather observation structure: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/WeatherObservation.cs\" ID=\"DeclareWeatherObservation\"::: Callers can use property initializer syntax to set the values, while still preserving the immutability: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/WeatherObservation.cs\" ID=\"UseWeatherObservation\"::: An attempt to change an observation after initialization results in a compiler error: C# // Error! CS8852. now . TemperatureInCelsius = 18 ; Init only setters can be useful to set base class properties from derived classes. They can also set derived properties through helpers in a base class. Positional records declare properties using init only setters. Those setters are used in with-expressions. You can declare init only setters for any class , struct , or record you define. For more information, see init (C# Reference) . Top-level statements \u00b6 Top-level statements remove unnecessary ceremony from many applications. Consider the canonical \"Hello World!\" program: C# using System ; namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); } } } There's only one line of code that does anything. With top-level statements, you can replace all that boilerplate with the using directive and the single line that does the work: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/Program.cs\" ID=\"TopLevelStatements\"::: If you wanted a one-line program, you could remove the using directive and use the fully qualified type name: C# System . Console . WriteLine ( \"Hello World!\" ); Only one file in your application may use top-level statements. If the compiler finds top-level statements in multiple source files, it's an error. It's also an error if you combine top-level statements with a declared program entry point method, typically a Main method. In a sense, you can think that one file contains the statements that would normally be in the Main method of a Program class. One of the most common uses for this feature is creating teaching materials. Beginner C# developers can write the canonical \"Hello World!\" in one or two lines of code. None of the extra ceremony is needed. However, seasoned developers will find many uses for this feature as well. Top-level statements enable a script-like experience for experimentation similar to what Jupyter notebooks provide. Top-level statements are great for small console programs and utilities. Azure Functions is an ideal use case for top-level statements. Most importantly, top-level statements don't limit your application's scope or complexity. Those statements can access or use any .NET class. They also don't limit your use of command-line arguments or return values. Top-level statements can access an array of strings named args . If the top-level statements return an integer value, that value becomes the integer return code from a synthesized Main method. The top-level statements may contain async expressions. In that case, the synthesized entry point returns a Task , or Task<int> . For more information, see Top-level statements in the C# Programming Guide. Pattern matching enhancements \u00b6 C# 9 includes new pattern matching improvements: Type patterns match an object matches a particular type Parenthesized patterns enforce or emphasize the precedence of pattern combinations Conjunctive and patterns require both patterns to match Disjunctive or patterns require either pattern to match Negated not patterns require that a pattern doesn't match Relational patterns require the input be less than, greater than, less than or equal, or greater than or equal to a given constant. These patterns enrich the syntax for patterns. Consider these examples: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/PatternUtilities.cs\" ID=\"IsLetterPattern\"::: With optional parentheses to make it clear that and has higher precedence than or : :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/PatternUtilities.cs\" ID=\"IsLetterOrSeparatorPattern\"::: One of the most common uses is a new syntax for a null check: C# if ( e is not null ) { // ... } Any of these patterns can be used in any context where patterns are allowed: is pattern expressions, switch expressions, nested patterns, and the pattern of a switch statement's case label. For more information, see Patterns (C# reference) . For more information, see the Relational patterns and Logical patterns sections of the Patterns article. Performance and interop \u00b6 Three new features improve support for native interop and low-level libraries that require high performance: native sized integers, function pointers, and omitting the localsinit flag. Native sized integers, nint and nuint , are integer types. They're expressed by the underlying types and . The compiler surfaces additional conversions and operations for these types as native ints. Native sized integers define properties for MaxValue or MinValue . These values can't be expressed as compile-time constants because they depend on the native size of an integer on the target machine. Those values are readonly at run time. You can use constant values for nint in the range [ int.MinValue .. int.MaxValue ]. You can use constant values for nuint in the range [ uint.MinValue .. uint.MaxValue ]. The compiler performs constant folding for all unary and binary operators using the and types. If the result doesn't fit in 32 bits, the operation is executed at run time and isn't considered a constant. Native sized integers can increase performance in scenarios where integer math is used extensively and needs to have the fastest performance possible. For more information, see nint and nuint types . Function pointers provide an easy syntax to access the IL opcodes ldftn and calli . You can declare function pointers using new delegate* syntax. A delegate* type is a pointer type. Invoking the delegate* type uses calli , in contrast to a delegate that uses callvirt on the Invoke() method. Syntactically, the invocations are identical. Function pointer invocation uses the managed calling convention. You add the unmanaged keyword after the delegate* syntax to declare that you want the unmanaged calling convention. Other calling conventions can be specified using attributes on the delegate* declaration. For more information, see Unsafe code and pointer types . Finally, you can add the to instruct the compiler not to emit the localsinit flag. This flag instructs the CLR to zero-initialize all local variables. The localsinit flag has been the default behavior for C# since 1.0. However, the extra zero-initialization may have measurable performance impact in some scenarios. In particular, when you use stackalloc . In those cases, you can add the . You may add it to a single method or property, or to a class , struct , interface , or even a module. This attribute doesn't affect abstract methods; it affects the code generated for the implementation. For more information, see SkipLocalsInit attribute . These features can improve performance in some scenarios. They should be used only after careful benchmarking both before and after adoption. Code involving native sized integers must be tested on multiple target platforms with different integer sizes. The other features require unsafe code. Fit and finish features \u00b6 Many of the other features help you write code more efficiently. In C# 9.0, you can omit the type in a new expression when the created object's type is already known. The most common use is in field declarations: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"WeatherStationField\"::: Target-typed new can also be used when you need to create a new object to pass as an argument to a method. Consider a ForecastFor() method with the following signature: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"ForecastSignature\"::: You could call it as follows: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"TargetTypeNewArgument\"::: Another nice use for this feature is to combine it with init only properties to initialize a new object: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"InitWeatherStation\"::: You can return an instance created by the default constructor using a return new(); statement. A similar feature improves the target type resolution of conditional expressions . With this change, the two expressions need not have an implicit conversion from one to the other, but may both have implicit conversions to a target type. You likely won't notice this change. What you will notice is that some conditional expressions that previously required casts or wouldn't compile now just work. Starting in C# 9.0, you can add the static modifier to lambda expressions or anonymous methods . Static lambda expressions are analogous to the static local functions: a static lambda or anonymous method can't capture local variables or instance state. The static modifier prevents accidentally capturing other variables. Covariant return types provide flexibility for the return types of override methods. An override method can return a type derived from the return type of the overridden base method. This can be useful for records and for other types that support virtual clone or factory methods. In addition, the foreach loop will recognize and use an extension method GetEnumerator that otherwise satisfies the foreach pattern. This change means foreach is consistent with other pattern-based constructions such as the async pattern, and pattern-based deconstruction. In practice, this change means you can add foreach support to any type. You should limit its use to when enumerating an object makes sense in your design. Next, you can use discards as parameters to lambda expressions. This convenience enables you to avoid naming the argument, and the compiler may avoid using it. You use the _ for any argument. For more information, see the Input parameters of a lambda expression section of the Lambda expressions article. Finally, you can now apply attributes to local functions . For example, you can apply nullable attribute annotations to local functions. Support for code generators \u00b6 Two final features support C# code generators. C# code generators are a component you can write that is similar to a roslyn analyzer or code fix. The difference is that code generators analyze code and write new source code files as part of the compilation process. A typical code generator searches code for attributes or other conventions. A code generator reads attributes or other code elements using the Roslyn analysis APIs. From that information, it adds new code to the compilation. Source generators can only add code; they aren't allowed to modify any existing code in the compilation. The two features added for code generators are extensions to partial method syntax , and module initializers . First, the changes to partial methods. Before C# 9.0, partial methods are private but can't specify an access modifier, have a void return, and can't have out parameters. These restrictions meant that if no method implementation is provided, the compiler removes all calls to the partial method. C# 9.0 removes these restrictions, but requires that partial method declarations have an implementation. Code generators can provide that implementation. To avoid introducing a breaking change, the compiler considers any partial method without an access modifier to follow the old rules. If the partial method includes the private access modifier, the new rules govern that partial method. For more information, see partial method (C# Reference) . The second new feature for code generators is module initializers . Module initializers are methods that have the attribute attached to them. These methods will be called by the runtime before any other field access or method invocation within the entire module. A module initializer method: Must be static Must be parameterless Must return void Must not be a generic method Must not be contained in a generic class Must be accessible from the containing module That last bullet point effectively means the method and its containing class must be internal or public. The method can't be a local function. For more information, see ModuleInitializer attribute .","title":"What's new in C# 9.0 - C# Guide"},{"location":"csharp/csharp-9/#whats-new-in-c-90","text":"C# 9.0 adds the following features and enhancements to the C# language: Records Init only setters Top-level statements Pattern matching enhancements Performance and interop Native sized integers Function pointers Suppress emitting localsinit flag Fit and finish features Target-typed new expressions static anonymous functions Target-typed conditional expressions Covariant return types Extension GetEnumerator support for foreach loops Lambda discard parameters Attributes on local functions Support for code generators Module initializers New features for partial methods Warning wave 5 C# 9.0 is supported on .NET 5 . For more information, see C# language versioning . You can download the latest .NET SDK from the .NET downloads page .","title":"What's new in C# 9.0"},{"location":"csharp/csharp-9/#record-types","text":"C# 9.0 introduces record types . You use the record keyword to define a reference type that provides built-in functionality for encapsulating data. You can create record types with immutable properties by using positional parameters or standard property syntax: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"PositionalRecord\"::: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"ImmutableRecord\"::: You can also create record types with mutable properties and fields: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"MutableRecord\"::: While records can be mutable, they are primarily intended for supporting immutable data models. The record type offers the following features: What's new in C# 9.0 Record types Positional syntax for property definition Immutability Value equality Nondestructive mutation Built-in formatting for display Inheritance Init only setters Top-level statements Pattern matching enhancements Performance and interop Fit and finish features Support for code generators You can use structure types to design data-centric types that provide value equality and little or no behavior. But for relatively large data models, structure types have some disadvantages: They don't support inheritance. They're less efficient at determining value equality. For value types, the method uses reflection to find all fields. For records, the compiler generates the Equals method. In practice, the implementation of value equality in records is measurably faster. They use more memory in some scenarios, since every instance has a complete copy of all of the data. Record types are reference types , so a record instance contains only a reference to the data.","title":"Record types"},{"location":"csharp/csharp-9/#positional-syntax-for-property-definition","text":"You can use positional parameters to declare properties of a record and to initialize the property values when you create an instance: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"InstantiatePositional\"::: When you use the positional syntax for property definition, the compiler creates: A public init-only auto-implemented property for each positional parameter provided in the record declaration. An init-only property can only be set in the constructor or by using a property initializer. A primary constructor whose parameters match the positional parameters on the record declaration. A Deconstruct method with an out parameter for each positional parameter provided in the record declaration. For more information, see Positional syntax in the C# language reference article about records.","title":"Positional syntax for property definition"},{"location":"csharp/csharp-9/#immutability","text":"A record type is not necessarily immutable. You can declare properties with set accessors and fields that aren't readonly . But while records can be mutable, they make it easier to create immutable data models. Properties that you create by using positional syntax are immutable. Immutability can be useful when you want a data-centric type to be thread-safe or a hash code to remain the same in a hash table. It can prevent bugs that happen when you pass an argument by reference to a method, and the method unexpectedly changes the argument value. The features unique to record types are implemented by compiler-synthesized methods, and none of these methods compromises immutability by modifying object state.","title":"Immutability"},{"location":"csharp/csharp-9/#value-equality","text":"Value equality means that two variables of a record type are equal if the types match and all property and field values match. For other reference types, equality means identity. That is, two variables of a reference type are equal if they refer to the same object. The following example illustrates value equality of record types: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"Equality\"::: In class types, you could manually override equality methods and operators to achieve value equality, but developing and testing that code would be time-consuming and error-prone. Having this functionality built-in prevents bugs that would result from forgetting to update custom override code when properties or fields are added or changed. For more information, see Value equality in the C# language reference article about records.","title":"Value equality"},{"location":"csharp/csharp-9/#nondestructive-mutation","text":"If you need to mutate immutable properties of a record instance, you can use a with expression to achieve nondestructive mutation . A with expression makes a new record instance that is a copy of an existing record instance, with specified properties and fields modified. You use object initializer syntax to specify the values to be changed, as shown in the following example: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"WithExpressions\"::: For more information, see Nondestructive mutation in the C# language reference article about records.","title":"Nondestructive mutation"},{"location":"csharp/csharp-9/#built-in-formatting-for-display","text":"Record types have a compiler-generated method that displays the names and values of public properties and fields. The ToString method returns a string of the following format: \\ { \\ = \\ , \\ = \\ , ...} For reference types, the type name of the object that the property refers to is displayed instead of the property value. In the following example, the array is a reference type, so System.String[] is displayed instead of the actual array element values: Text Only Person { FirstName = Nancy, LastName = Davolio, ChildNames = System.String[] } For more information, see Built-in formatting in the C# language reference article about records.","title":"Built-in formatting for display"},{"location":"csharp/csharp-9/#inheritance","text":"A record can inherit from another record. However, a record can't inherit from a class, and a class can't inherit from a record. The following example illustrates inheritance with positional property syntax: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"PositionalInheritance\"::: For two record variables to be equal, the run-time type must be equal. The types of the containing variables might be different. This is illustrated in the following code example: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"InheritanceEquality\"::: In the example, all instances have the same properties and the same property values. But student == teacher returns False although both are Person -type variables. And student == student2 returns True although one is a Person variable and one is a Student variable. All public properties and fields of both derived and base types are included in the ToString output, as shown in the following example: :::code language=\"csharp\" source=\"../language-reference/builtin-types/snippets/shared/RecordType.cs\" id=\"ToStringInheritance\"::: For more information, see Inheritance in the C# language reference article about records.","title":"Inheritance"},{"location":"csharp/csharp-9/#init-only-setters","text":"Init only setters provide consistent syntax to initialize members of an object. Property initializers make it clear which value is setting which property. The downside is that those properties must be settable. Starting with C# 9.0, you can create init accessors instead of set accessors for properties and indexers. Callers can use property initializer syntax to set these values in creation expressions, but those properties are readonly once construction has completed. Init only setters provide a window to change state. That window closes when the construction phase ends. The construction phase effectively ends after all initialization, including property initializers and with-expressions have completed. You can declare init only setters in any type you write. For example, the following struct defines a weather observation structure: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/WeatherObservation.cs\" ID=\"DeclareWeatherObservation\"::: Callers can use property initializer syntax to set the values, while still preserving the immutability: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/WeatherObservation.cs\" ID=\"UseWeatherObservation\"::: An attempt to change an observation after initialization results in a compiler error: C# // Error! CS8852. now . TemperatureInCelsius = 18 ; Init only setters can be useful to set base class properties from derived classes. They can also set derived properties through helpers in a base class. Positional records declare properties using init only setters. Those setters are used in with-expressions. You can declare init only setters for any class , struct , or record you define. For more information, see init (C# Reference) .","title":"Init only setters"},{"location":"csharp/csharp-9/#top-level-statements","text":"Top-level statements remove unnecessary ceremony from many applications. Consider the canonical \"Hello World!\" program: C# using System ; namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); } } } There's only one line of code that does anything. With top-level statements, you can replace all that boilerplate with the using directive and the single line that does the work: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/Program.cs\" ID=\"TopLevelStatements\"::: If you wanted a one-line program, you could remove the using directive and use the fully qualified type name: C# System . Console . WriteLine ( \"Hello World!\" ); Only one file in your application may use top-level statements. If the compiler finds top-level statements in multiple source files, it's an error. It's also an error if you combine top-level statements with a declared program entry point method, typically a Main method. In a sense, you can think that one file contains the statements that would normally be in the Main method of a Program class. One of the most common uses for this feature is creating teaching materials. Beginner C# developers can write the canonical \"Hello World!\" in one or two lines of code. None of the extra ceremony is needed. However, seasoned developers will find many uses for this feature as well. Top-level statements enable a script-like experience for experimentation similar to what Jupyter notebooks provide. Top-level statements are great for small console programs and utilities. Azure Functions is an ideal use case for top-level statements. Most importantly, top-level statements don't limit your application's scope or complexity. Those statements can access or use any .NET class. They also don't limit your use of command-line arguments or return values. Top-level statements can access an array of strings named args . If the top-level statements return an integer value, that value becomes the integer return code from a synthesized Main method. The top-level statements may contain async expressions. In that case, the synthesized entry point returns a Task , or Task<int> . For more information, see Top-level statements in the C# Programming Guide.","title":"Top-level statements"},{"location":"csharp/csharp-9/#pattern-matching-enhancements","text":"C# 9 includes new pattern matching improvements: Type patterns match an object matches a particular type Parenthesized patterns enforce or emphasize the precedence of pattern combinations Conjunctive and patterns require both patterns to match Disjunctive or patterns require either pattern to match Negated not patterns require that a pattern doesn't match Relational patterns require the input be less than, greater than, less than or equal, or greater than or equal to a given constant. These patterns enrich the syntax for patterns. Consider these examples: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/PatternUtilities.cs\" ID=\"IsLetterPattern\"::: With optional parentheses to make it clear that and has higher precedence than or : :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/PatternUtilities.cs\" ID=\"IsLetterOrSeparatorPattern\"::: One of the most common uses is a new syntax for a null check: C# if ( e is not null ) { // ... } Any of these patterns can be used in any context where patterns are allowed: is pattern expressions, switch expressions, nested patterns, and the pattern of a switch statement's case label. For more information, see Patterns (C# reference) . For more information, see the Relational patterns and Logical patterns sections of the Patterns article.","title":"Pattern matching enhancements"},{"location":"csharp/csharp-9/#performance-and-interop","text":"Three new features improve support for native interop and low-level libraries that require high performance: native sized integers, function pointers, and omitting the localsinit flag. Native sized integers, nint and nuint , are integer types. They're expressed by the underlying types and . The compiler surfaces additional conversions and operations for these types as native ints. Native sized integers define properties for MaxValue or MinValue . These values can't be expressed as compile-time constants because they depend on the native size of an integer on the target machine. Those values are readonly at run time. You can use constant values for nint in the range [ int.MinValue .. int.MaxValue ]. You can use constant values for nuint in the range [ uint.MinValue .. uint.MaxValue ]. The compiler performs constant folding for all unary and binary operators using the and types. If the result doesn't fit in 32 bits, the operation is executed at run time and isn't considered a constant. Native sized integers can increase performance in scenarios where integer math is used extensively and needs to have the fastest performance possible. For more information, see nint and nuint types . Function pointers provide an easy syntax to access the IL opcodes ldftn and calli . You can declare function pointers using new delegate* syntax. A delegate* type is a pointer type. Invoking the delegate* type uses calli , in contrast to a delegate that uses callvirt on the Invoke() method. Syntactically, the invocations are identical. Function pointer invocation uses the managed calling convention. You add the unmanaged keyword after the delegate* syntax to declare that you want the unmanaged calling convention. Other calling conventions can be specified using attributes on the delegate* declaration. For more information, see Unsafe code and pointer types . Finally, you can add the to instruct the compiler not to emit the localsinit flag. This flag instructs the CLR to zero-initialize all local variables. The localsinit flag has been the default behavior for C# since 1.0. However, the extra zero-initialization may have measurable performance impact in some scenarios. In particular, when you use stackalloc . In those cases, you can add the . You may add it to a single method or property, or to a class , struct , interface , or even a module. This attribute doesn't affect abstract methods; it affects the code generated for the implementation. For more information, see SkipLocalsInit attribute . These features can improve performance in some scenarios. They should be used only after careful benchmarking both before and after adoption. Code involving native sized integers must be tested on multiple target platforms with different integer sizes. The other features require unsafe code.","title":"Performance and interop"},{"location":"csharp/csharp-9/#fit-and-finish-features","text":"Many of the other features help you write code more efficiently. In C# 9.0, you can omit the type in a new expression when the created object's type is already known. The most common use is in field declarations: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"WeatherStationField\"::: Target-typed new can also be used when you need to create a new object to pass as an argument to a method. Consider a ForecastFor() method with the following signature: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"ForecastSignature\"::: You could call it as follows: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"TargetTypeNewArgument\"::: Another nice use for this feature is to combine it with init only properties to initialize a new object: :::code language=\"csharp\" source=\"snippets/whats-new-csharp9/FitAndFinish.cs\" ID=\"InitWeatherStation\"::: You can return an instance created by the default constructor using a return new(); statement. A similar feature improves the target type resolution of conditional expressions . With this change, the two expressions need not have an implicit conversion from one to the other, but may both have implicit conversions to a target type. You likely won't notice this change. What you will notice is that some conditional expressions that previously required casts or wouldn't compile now just work. Starting in C# 9.0, you can add the static modifier to lambda expressions or anonymous methods . Static lambda expressions are analogous to the static local functions: a static lambda or anonymous method can't capture local variables or instance state. The static modifier prevents accidentally capturing other variables. Covariant return types provide flexibility for the return types of override methods. An override method can return a type derived from the return type of the overridden base method. This can be useful for records and for other types that support virtual clone or factory methods. In addition, the foreach loop will recognize and use an extension method GetEnumerator that otherwise satisfies the foreach pattern. This change means foreach is consistent with other pattern-based constructions such as the async pattern, and pattern-based deconstruction. In practice, this change means you can add foreach support to any type. You should limit its use to when enumerating an object makes sense in your design. Next, you can use discards as parameters to lambda expressions. This convenience enables you to avoid naming the argument, and the compiler may avoid using it. You use the _ for any argument. For more information, see the Input parameters of a lambda expression section of the Lambda expressions article. Finally, you can now apply attributes to local functions . For example, you can apply nullable attribute annotations to local functions.","title":"Fit and finish features"},{"location":"csharp/csharp-9/#support-for-code-generators","text":"Two final features support C# code generators. C# code generators are a component you can write that is similar to a roslyn analyzer or code fix. The difference is that code generators analyze code and write new source code files as part of the compilation process. A typical code generator searches code for attributes or other conventions. A code generator reads attributes or other code elements using the Roslyn analysis APIs. From that information, it adds new code to the compilation. Source generators can only add code; they aren't allowed to modify any existing code in the compilation. The two features added for code generators are extensions to partial method syntax , and module initializers . First, the changes to partial methods. Before C# 9.0, partial methods are private but can't specify an access modifier, have a void return, and can't have out parameters. These restrictions meant that if no method implementation is provided, the compiler removes all calls to the partial method. C# 9.0 removes these restrictions, but requires that partial method declarations have an implementation. Code generators can provide that implementation. To avoid introducing a breaking change, the compiler considers any partial method without an access modifier to follow the old rules. If the partial method includes the private access modifier, the new rules govern that partial method. For more information, see partial method (C# Reference) . The second new feature for code generators is module initializers . Module initializers are methods that have the attribute attached to them. These methods will be called by the runtime before any other field access or method invocation within the entire module. A module initializer method: Must be static Must be parameterless Must return void Must not be a generic method Must not be contained in a generic class Must be accessible from the containing module That last bullet point effectively means the method and its containing class must be internal or public. The method can't be a local function. For more information, see ModuleInitializer attribute .","title":"Support for code generators"},{"location":"csharp/csharp-features/","text":"C# features \u00b6 Extension Method \u00b6 While using extension method we need to answer the following questions * Is the primary parameter an instance? * Does the method logically operate on that instance? * Is it desirable for the method to appear in IntelliSense for that type? Extension method Text Only // Documents to provide intellisense public static string InsertSpaces(this string source){ } Usage Text Only str.InsertSpaces();","title":"C# features"},{"location":"csharp/csharp-features/#c-features","text":"","title":"C# features"},{"location":"csharp/csharp-features/#extension-method","text":"While using extension method we need to answer the following questions * Is the primary parameter an instance? * Does the method logically operate on that instance? * Is it desirable for the method to appear in IntelliSense for that type? Extension method Text Only // Documents to provide intellisense public static string InsertSpaces(this string source){ } Usage Text Only str.InsertSpaces();","title":"Extension Method"},{"location":"csharp/csharp-fundamentals/","text":"OOP Fundamentals \u00b6 Introduction \u00b6 Identifying classes from Requirements \u00b6 Building Entity Classes \u00b6 Building Entity Classes - Methods \u00b6 Seperation of Responsibilities/Concerns \u00b6 Establishing Relationships \u00b6 Leveraging Reuse through Inheritance \u00b6 Building Reusable Components \u00b6 Understanding Interfaces \u00b6","title":"OOP Fundamentals"},{"location":"csharp/csharp-fundamentals/#oop-fundamentals","text":"","title":"OOP Fundamentals"},{"location":"csharp/csharp-fundamentals/#introduction","text":"","title":"Introduction"},{"location":"csharp/csharp-fundamentals/#identifying-classes-from-requirements","text":"","title":"Identifying classes from Requirements"},{"location":"csharp/csharp-fundamentals/#building-entity-classes","text":"","title":"Building Entity Classes"},{"location":"csharp/csharp-fundamentals/#building-entity-classes-methods","text":"","title":"Building Entity Classes - Methods"},{"location":"csharp/csharp-fundamentals/#seperation-of-responsibilitiesconcerns","text":"","title":"Seperation of Responsibilities/Concerns"},{"location":"csharp/csharp-fundamentals/#establishing-relationships","text":"","title":"Establishing Relationships"},{"location":"csharp/csharp-fundamentals/#leveraging-reuse-through-inheritance","text":"","title":"Leveraging Reuse through Inheritance"},{"location":"csharp/csharp-fundamentals/#building-reusable-components","text":"","title":"Building Reusable Components"},{"location":"csharp/csharp-fundamentals/#understanding-interfaces","text":"","title":"Understanding Interfaces"},{"location":"explore/data-dcitionary-pipeline/","text":"\ud83e\uddfa Data dictionary for metadata driven Synapse pipeline \u00b6 Diagram \u00b6 graph TD; Process[Process] --> ProcessStep[ProcessStep] ProcessStep --> ProcessStepElementMapping[ProcessStepElementMapping] ProcessStepElementMapping --> DataSet[DataSet] DataSet --> DataSetElement[DataSetElement] ProcessStepElementMapping --> Action[Action] Action --> ActionType[ActionType] Action --> ActionParameter[ActionParameter] Derive the entity relation from given list of entities with their properties: \u00b6 Process: ProcessID, ProcessCode, ProcessName, ProcessDescription, TriggerPattern, KeyParameterTypeID, ImplementationInfo, MaxConcurrency, IsParallelProcess, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ProcessStep: ProcessStepID, ProcessID, ProcessStepName, ProcessStepDescription, ProcessStepTypeID, SubProcessID, ProcessStepOrder, SourceDatasetID, SourceDatasetValidationTypeID, DestinationDatasetID, ActionID, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; DataSet: DatasetID, DatasetName, DatasetDescription, DatasetTypeID, NamePattern, LocationPattern, DatasetKey, DatasetSource, ConnectionDescription, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; DataSetElement: DatasetElementID, DatasetID, DatasetElementName, DatasetElementDescription, DatasetElementOrder, DatasetElementType, IsRequired, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ProcessStepElementMapping: ProcessStepElementMappingID, ProcessStepID, DestinationElementID, ElementSourceTypeID, Source, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ActionType: ActionTypeID, ActionTypeDescription, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy; Action: ActionID, ActionName, ActionDescription, ActionTypeID, NamePattern, LocationPattern, ParameterExpression, ConnectionDescription, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ActionParameter: ActionParameterID, ActionID, ParameterName, ParameterValueTypeID, ParameterValue, ParameterOrder, IsRequired, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy; The entities and their relationships are as follows: - Process has a one-to-many relationship with ProcessStep, as a Process can have multiple ProcessSteps, but a ProcessStep can only belong to one Process. The ProcessID field in the ProcessStep entity is used to establish this relationship. - ProcessStep has a one-to-one relationship with SubProcess, as a ProcessStep can only belong to one SubProcess, but a SubProcess can only have one ProcessStep. The SubProcessID field in the ProcessStep entity is used to establish this relationship. - ProcessStep has a many-to-one relationship with DataSet, as a ProcessStep can have a source or destination DataSet, but a DataSet can be used by multiple ProcessSteps. The SourceDatasetID and DestinationDatasetID fields in the ProcessStep entity are used to establish this relationship. - ProcessStep has a many-to-one relationship with Action, as a ProcessStep can have an action, but an Action can be used by multiple ProcessSteps. The ActionID field in the ProcessStep entity is used to establish this relationship. - DataSet has a one-to-many relationship with DataSetElement, as a DataSet can have multiple DataSetElements, but a DataSetElement can only belong to one DataSet. The DatasetID field in the DataSetElement entity is used to establish this relationship. - ProcessStep has a many-to-many relationship with DataSetElement, as a ProcessStep can have multiple DataSetElements, and a DataSetElement can be used by multiple ProcessSteps. The ProcessStepElementMapping entity is used to establish this relationship, with ProcessStepID and DestinationElementID fields. - Action has a one-to-many relationship with ActionParameter, as an Action can have multiple ActionParameters, but an ActionParameter can only belong to one Action. The ActionID field in the ActionParameter entity is used to establish this relationship. - Action has a one-to-one relationship with ActionType, as an Action can only have one ActionType, but an ActionType can be used by multiple Actions. The ActionTypeID field in the Action entity is used to establish this relationship. schema for them in pyspark \u00b6 Python from pyspark.sql.types import * # Define the schema for the Process table process_schema = StructType ([ StructField ( \"ProcessID\" , IntegerType ()), StructField ( \"ProcessCode\" , StringType ()), StructField ( \"ProcessName\" , StringType ()), StructField ( \"ProcessDescription\" , StringType ()), StructField ( \"TriggerPattern\" , StringType ()), StructField ( \"KeyParameterTypeID\" , IntegerType ()), StructField ( \"ImplementationInfo\" , StringType ()), StructField ( \"MaxConcurrency\" , IntegerType ()), StructField ( \"IsParallelProcess\" , BooleanType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the Process DataFrame process_df = spark . createDataFrame ( data , process_schema ) # Define the schema for the ProcessStep table process_step_schema = StructType ([ StructField ( \"ProcessStepID\" , IntegerType ()), StructField ( \"ProcessID\" , IntegerType ()), StructField ( \"ProcessStepName\" , StringType ()), StructField ( \"ProcessStepDescription\" , StringType ()), StructField ( \"ProcessStepTypeID\" , IntegerType ()), StructField ( \"SubProcessID\" , IntegerType ()), StructField ( \"ProcessStepOrder\" , IntegerType ()), StructField ( \"SourceDatasetID\" , IntegerType ()), StructField ( \"SourceDatasetValidationTypeID\" , IntegerType ()), StructField ( \"DestinationDatasetID\" , IntegerType ()), StructField ( \"ActionID\" , IntegerType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the ProcessStep DataFrame process_step_df = spark . createDataFrame ( data , process_step_schema ) # Define the schema for the DataSet table dataset_schema = StructType ([ StructField ( \"DatasetID\" , IntegerType ()), StructField ( \"DatasetName\" , StringType ()), StructField ( \"DatasetDescription\" , StringType ()), StructField ( \"DatasetTypeID\" , IntegerType ()), StructField ( \"NamePattern\" , StringType ()), StructField ( \"LocationPattern\" , StringType ()), StructField ( \"DatasetKey\" , StringType ()), StructField ( \"DatasetSource\" , StringType ()), StructField ( \"ConnectionDescription\" , StringType ()), StructField ( \"EffectiveDate StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the DataSet DataFrame dataset_df = spark . createDataFrame ( data , dataset_schema ) # Define the schema for the DataSetElement table dataset_element_schema = StructType ([ StructField ( \"DatasetElementID\" , IntegerType ()), StructField ( \"DatasetID\" , IntegerType ()), StructField ( \"DatasetElementName\" , StringType ()), StructField ( \"DatasetElementDescription\" , StringType ()), StructField ( \"DatasetElementOrder\" , IntegerType ()), StructField ( \"DatasetElementType\" , StringType ()), StructField ( \"IsRequired\" , BooleanType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the DataSetElement DataFrame dataset_element_df = spark . createDataFrame ( data , dataset_element_schema ) # Define the schema for the ProcessStepElementMapping table process_step_element_mapping_schema = StructType ([ StructField ( \"ProcessStepElementMappingID\" , IntegerType ()), StructField ( \"ProcessStepID\" , IntegerType ()), StructField ( \"DestinationElementID\" , IntegerType ()), StructField ( \"ElementSourceTypeID\" , IntegerType ()), StructField ( \"Source\" , StringType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the ProcessStepElementMapping DataFrame process_step_element_mapping_df = spark . createDataFrame ( data , process_step_element_mapping_schema ) # Define the schema for the ActionType table action_type_schema = StructType ([ StructField ( \"ActionTypeID\" , IntegerType ()), StructField ( \"ActionTypeDescription\" , StringType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()) ]) # Create the ActionType DataFrame action_type_df = spark . createDataFrame ( data , action_type_schema ) # Define the schema for the Action table action_schema = StructType ([ StructField ( \"ActionID\" , IntegerType ()), StructField ( \"ActionName\" , StringType ()), StructField ( \"ActionDescription\" , StringType ()), StructField ( \"ActionTypeID\" , IntegerType ()), StructField ( \"NamePattern\" , StringType ()), StructField ( \"LocationPattern\" , StringType ()), StructField ( \"ParameterExpression\" , StringType ()), StructField ( \"ConnectionDescription\" , StringType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the Action DataFrame action_df = spark . createDataFrame ( data , action_schema ) # Define the schema for the ActionParameter table action_parameter_schema = StructType ([ StructField ( \"ActionParameterID\" , IntegerType ()), StructField ( \"ActionID\" , IntegerType ()), StructField ( \"ParameterName\" , StringType ()), StructField ( \"ParameterValueTypeID\" , IntegerType ()), StructField ( \"ParameterValue\" , StringType ()), StructField ( \"ParameterOrder\" , IntegerType ()), StructField ( \"IsRequired\" , BooleanType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()) ]) # Create the ActionParameter DataFrame action_parameter_df = spark . createDataFrame ( data , action_parameter_schema ) It's worth noting that the data variable used in the example is a list of rows that should be loaded into the DataFrames. Also, this is just an example, you may need to adjust the datatype of the fields or add/remove fields as per your use case. Here is how the relationships between the entities can be defined in PySpark: Python # Define the relationships between the tables process_df . createOrReplaceTempView ( \"process\" ) process_step_df . createOrReplaceTempView ( \"process_step\" ) dataset_df . createOrReplaceTempView ( \"dataset\" ) dataset_element_df . createOrReplaceTempView ( \"dataset_element\" ) process_step_element_mapping_df . createOrReplaceTempView ( \"process_step_element_mapping\" ) action_type_df . createOrReplaceTempView ( \"action_type\" ) action_df . createOrReplaceTempView ( \"action\" ) action_parameter_df . createOrReplaceTempView ( \"action_parameter\" ) # One-to-many relationship from Process to ProcessStep spark . sql ( \"SELECT process.*, process_step.* FROM process JOIN process_step ON process.ProcessID = process_step.ProcessID\" ) # One-to-one relationship from ProcessStep to SubProcess spark . sql ( \"SELECT process_step.*, sub_process.* FROM process_step JOIN sub_process ON process_step.SubProcessID = sub_process.SubProcessID\" ) # Many-to-one relationship from ProcessStep to DataSet spark . sql ( \"SELECT process_step.*, dataset.* FROM process_step JOIN dataset ON process_step.SourceDatasetID = dataset.DatasetID OR process_step.DestinationDatasetID = dataset.DatasetID\" ) # Many-to-one relationship from ProcessStep to Action spark . sql ( \"SELECT process_step.*, action.* FROM process_step JOIN action ON process_step.ActionID = action.ActionID\" ) # One-to-many relationship from DataSet to DataSetElement spark . sql ( \"SELECT dataset.*, dataset_element.* FROM dataset JOIN dataset_element ON dataset.DatasetID = dataset_element.DatasetID\" ) # Many-to-many relationship from ProcessStep to DataSetElement spark . sql ( \"SELECT process_step.*, dataset_element.* FROM process_step JOIN process_step_element_mapping ON process_step.ProcessStepID = process_step_element_mapping.ProcessStepID JOIN dataset_element ON process_step_element_mapping.DestinationElementID = dataset_element.DatasetElementID\" ) # One-to-many relationship from Action to ActionParameter spark . sql ( \"SELECT action.*, action_parameter.* FROM action JOIN action_parameter ON action.ActionID = action_parameter.ActionID\" ) # One-to-one relationship from Action to ActionType spark . sql ( \"SELECT action.*, action_type.* FROM action JOIN action_type ON action.ActionTypeID = action_type.ActionTypeID\" ) It is worth noting that this is just an example and you may need to adjust the query as per your requirement. Also, the above mentioned SQL Queries are used to join the dataframe and load the data in the required format. Pyspark dataframe from individual dataframe which are grouped by process, processStep and so on \u00b6 Use PySpark to group the individual dataframes by their respective entities and create a new dataframe. Here is an example of how you can group the dataframes by \"Process\" and create a new dataframe: Python from pyspark.sql.functions import * # Group the dataframes by Process grouped_process_df = process_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" ) . agg ( sum ( \"MaxConcurrency\" ) . alias ( \"TotalMaxConcurrency\" ), count ( \"*\" ) . alias ( \"NumOfProcessSteps\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ) ) # Create a new dataframe with the grouped data process_summary_df = grouped_process_df . select ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"TotalMaxConcurrency\" , \"NumOfProcessSteps\" , \"ImplementationInfo\" , \"IsParallelProcess\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" ) You can also group other dataframes by their respective entities in a similar way. Python grouped_processStep_df = processStep_df . groupBy ( \"ProcessStepID\" , \"ProcessID\" , \"ProcessStepName\" ) . agg ( first ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescription\" ), first ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeID\" ), first ( \"SubProcessID\" ) . alias ( \"SubProcessID\" ), first ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrder\" ), first ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetID\" ), first ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeID\" ), first ( \"DestinationDatasetID\" ) . alias ( \"DestinationDatasetID\" ), first ( \"ActionID\" ) . alias ( \"ActionID\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ) ) processStep_summary_df = grouped_processStep_df . select ( \"ProcessStepID\" , \"ProcessID\" , \"ProcessStepName\" , \"ProcessStepDescription\" , \"ProcessStepTypeID\" , \"SubProcessID\" , \"ProcessStepOrder\" , \"SourceDatasetID\" , \"SourceDatasetValidationTypeID\" , \"DestinationDatasetID\" , \"ActionID\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" ) In this example, I am using the groupBy method to group the dataframe by specific columns and the agg method to aggregate the data. You can use different aggregation functions like sum, count, first, last etc. to get the desired values for the new dataframe. It's worth noting that this is just an example, you may need to adjust the code as per your use case. Now let's create nested group starting from process \u00b6 Create nested groups starting from the \"Process\" dataframe in PySpark. Here is an example of how you can group the dataframes by \"Process\" and then group the resulting dataframe by \"ProcessStep\": Python from pyspark.sql.functions import * # Group the dataframes by Process grouped_process_df = process_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" ) . agg ( sum ( \"MaxConcurrency\" ) . alias ( \"TotalMaxConcurrency\" ), count ( \"*\" ) . alias ( \"NumOfProcessSteps\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepID\" ) . alias ( \"ProcessStepIDList\" ) ) # Group the resulting dataframe by ProcessStep grouped_processStep_df = grouped_process_df . selectExpr ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"TotalMaxConcurrency\" , \"NumOfProcessSteps\" , \"ImplementationInfo\" , \"IsParallelProcess\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" , \"explode(ProcessStepIDList) as ProcessStepID\" ) grouped_processStep_df = grouped_processStep_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"ProcessStepID\" ) . agg ( first ( \"TotalMaxConcurrency\" ) . alias ( \"TotalMaxConcurrency\" ), first ( \"NumOfProcessSteps\" ) . alias ( \"NumOfProcessSteps\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescriptionList\" ), collect_list ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeIDList\" ), collect_list ( \"SubProcessID\" ) . alias ( \"SubProcessIDList\" ), collect_list ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrderList\" ), collect_list ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetIDList\" ), collect_list ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeIDList\" ), collect_list ( \"DestinationDatasetIDList\" ) . alias ( \"DestinationDatasetIDList\" ), collect_list ( \"ActionID\" ) . alias ( \"ActionIDList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ) ) In this example, I am using the collect_list function to collect all the values for each unique ProcessStepID and then groupBy method to group the resulting dataframe by \"ProcessID\", \"ProcessCode\", \"ProcessName\" and \"ProcessStepID\". Once you have the nested group it's possible to explode the lists and have separate rows for each processStep by using selectExpr and explode method It's worth noting that this is just an example, you may need to adjust the code as per your use case. Example of how you can use the selectExpr and explode method to explode the lists and have separate rows for each processStep: Python # Explode the lists and create separate rows for each ProcessStep exploded_df = grouped_processStep_df . selectExpr ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"TotalMaxConcurrency\" , \"NumOfProcessSteps\" , \"ImplementationInfo\" , \"IsParallelProcess\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" , \"ProcessStepID\" , \"explode(ProcessStepDescriptionList) as ProcessStepDescription\" , \"explode(ProcessStepTypeIDList) as ProcessStepTypeID\" , \"explode(SubProcessIDList) as SubProcessID\" , \"explode(ProcessStepOrderList) as ProcessStepOrder\" , \"explode(SourceDatasetIDList) as SourceDatasetID\" , \"explode(SourceDatasetValidationTypeIDList) as SourceDatasetValidationTypeID\" , \"explode(DestinationDatasetIDList) as DestinationDatasetID\" , \"explode(ActionIDList) as ActionID\" , \"explode(EffectiveDateList) as EffectiveDate\" , \"explode(ExpireDateList) as ExpireDate\" , \"explode(CreatedDatetimeList) as CreatedDatetime\" , \"explode(CreatedByList) as CreatedBy\" , \"explode(UpdatedDatetimeList) as UpdatedDatetime\" , \"explode(UpdatedByList) as UpdatedBy\" , \"explode(ValidFromList) as ValidFrom\" , \"explode(ValidToList) as ValidTo\" ) This will create a new dataframe, exploded_df, where each row represents a unique ProcessStep. Each row will contain the corresponding ProcessStep information as well as the information from the parent Process. You can also use the selectExpr Python exploded_df . selectExpr ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"ProcessStepID\" , \"ProcessStepDescription\" , \"ProcessStepTypeID\" , \"SubProcessID\" , \"ProcessStepOrder\" , \"SourceDatasetID\" , \"SourceDatasetValidationTypeID\" , \"DestinationDatasetID\" , \"ActionID\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" ) This will select only the relevant columns for the exploded dataframe. It's worth noting that the above method will repeat the parent process information for each processStep, if you don't want this, you can use a join method to bring the parent process information. It's also worth noting that this is just an example, you may need to adjust the code as per your use case. All related information from dataframes group by entities for a given process \u00b6 Use Spark SQL to query the dataframes and find all related information for a given process. Here is an example of how you can query the dataframes and group them by entities for a given process: Python from pyspark.sql.functions import * # Create a temporary view for each dataframe process_df . createOrReplaceTempView ( \"process\" ) processStep_df . createOrReplaceTempView ( \"processStep\" ) dataset_df . createOrReplaceTempView ( \"dataset\" ) datasetElement_df . createOrReplaceTempView ( \"datasetElement\" ) processStepElementMapping_df . createOrReplaceTempView ( \"processStepElementMapping\" ) actionType_df . createOrReplaceTempView ( \"actionType\" ) action_df . createOrReplaceTempView ( \"action\" ) actionParameter_df . createOrReplaceTempView ( \"actionParameter\" ) # Define the process ID for which you want to find the related information process_id = \"123\" # Query to find all related information for the given process ID query = \"\"\" SELECT process.*, processStep.*, dataset.*, datasetElement.*, processStepElementMapping.*, actionType.*, action.*, actionParameter.* FROM process LEFT JOIN processStep ON process.ProcessID = processStep.ProcessID LEFT JOIN dataset ON processStep.SourceDatasetID = dataset.DatasetID OR processStep.DestinationDatasetID = dataset.DatasetID LEFT JOIN datasetElement ON dataset.DatasetID = datasetElement.DatasetID LEFT JOIN processStepElementMapping ON processStep.ProcessStepID = processStepElementMapping.ProcessStepID LEFT JOIN actionType ON processStep.ProcessStepTypeID = actionType.ActionTypeID LEFT JOIN action ON processStep.ActionID = action.ActionID LEFT JOIN actionParameter ON action.ActionID = actionParameter.ActionID WHERE process.ProcessID = ' {} ' \"\"\" . format ( process_id ) # Execute the query and group the results by entities result_df = spark . sql ( query ) In this example, I'm using the LEFT JOIN operator to join the dataframes based on the relationships between the entities. I'm also using the WHERE clause to filter the results based on the given process ID. It's worth noting that this is just an example, you may need to adjust the code as per your use case. Use the groupBy method to group the results by entities after executing the query. Here is an example of how you can group the results: Python # Group the results by Process grouped_by_process_df = result_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" ) . agg ( first ( \"ProcessDescription\" ) . alias ( \"ProcessDescription\" ), first ( \"TriggerPattern\" ) . alias ( \"TriggerPattern\" ), first ( \"KeyParameterTypeID\" ) . alias ( \"KeyParameterTypeID\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"MaxConcurrency\" ) . alias ( \"MaxConcurrency\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepID\" ) . alias ( \"ProcessStepIDList\" ), collect_list ( \"ProcessStepName\" ) . alias ( \"ProcessStepNameList\" ), collect_list ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescriptionList\" ), collect_list ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeIDList\" ), collect_list ( \"SubProcessID\" ) . alias ( \"SubProcessIDList\" ), collect_list ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrderList\" ), collect_list ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetIDList\" ), collect_list ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeIDList\" ), collect_list # seems incomplete grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"DestinationDatasetID\" ) . alias ( \"DestinationDatasetIDList\" ), collect_list ( \"ActionID\" ) . alias ( \"ActionIDList\" ), collect_list ( \"DatasetID\" ) . alias ( \"DatasetIDList\" ), collect_list ( \"DatasetName\" ) . alias ( \"DatasetNameList\" ), collect_list ( \"DatasetDescription\" ) . alias ( \"DatasetDescriptionList\" ), collect_list ( \"DatasetTypeID\" ) . alias ( \"DatasetTypeIDList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"DatasetKey\" ) . alias ( \"DatasetKeyList\" ), collect_list ( \"DatasetSource\" ) . alias ( \"DatasetSourceList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"DatasetElementID\" ) . alias ( \"DatasetElementIDList\" ), collect_list ( \"DatasetElementName\" ) . alias ( \"DatasetElementNameList\" ), collect_list ( \"DatasetElementDescription\" ) . alias ( \"DatasetElementDescriptionList\" ), collect_list ( \"DatasetElementOrder\" ) . alias ( \"DatasetElementOrderList\" ), collect_list ( \"DatasetElementType\" ) . alias ( \"DatasetElementTypeList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ), collect_list ( \"ProcessStepElementMappingID\" ) . alias ( \"ProcessStepElementMappingIDList\" ), collect_list ( \"DestinationElementID\" ) . alias ( \"DestinationElementIDList\" ), collect_list ( \"ElementSourceTypeID\" ) . alias ( \"ElementSourceTypeIDList\" ), collect_list ( \"Source\" ) . alias ( \"SourceList\" ), collect_list # same here grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"ActionTypeID\" ) . alias ( \"ActionTypeIDList\" ), collect_list ( \"ActionTypeDescription\" ) . alias ( \"ActionTypeDescriptionList\" ), collect_list ( \"ActionName\" ) . alias ( \"ActionNameList\" ), collect_list ( \"ActionDescription\" ) . alias ( \"ActionDescriptionList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"ParameterExpression\" ) . alias ( \"ParameterExpressionList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"ActionParameterID\" ) . alias ( \"ActionParameterIDList\" ), collect_list ( \"ParameterName\" ) . alias ( \"ParameterNameList\" ), collect_list ( \"ParameterValueTypeID\" ) . alias ( \"ParameterValueTypeIDList\" ), collect_list ( \"ParameterValue\" ) . alias ( \"ParameterValueList\" ), collect_list ( \"ParameterOrder\" ) . alias ( \"ParameterOrderList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ), ) In this example, I'm using the groupBy method to group the results by Process ID, Process Code, and Process Name, and using the agg method and the collect_list function to collect all the values for each unique Process ID and then group the resulting dataframe by process. It's worth noting that this is just an example, you may need to adjust the code as per your use case. Find all related information from dataframes group by entities for a given ProcessCode \u00b6 use Spark SQL to query the dataframes and find all related information for a given ProcessCode. Here is an example of how you can query the dataframes and group them by entities for a given ProcessCode: Python from pyspark.sql.functions import * # Create a temporary view for each dataframe process_df . createOrReplaceTempView ( \"process\" ) processStep_df . createOrReplaceTempView ( \"processStep\" ) dataset_df . createOrReplaceTempView ( \"dataset\" ) datasetElement_df . createOrReplaceTempView ( \"datasetElement\" ) processStepElementMapping_df . createOrReplaceTempView ( \"processStepElementMapping\" ) actionType_df . createOrReplaceTempView ( \"actionType\" ) action_df . createOrReplaceTempView ( \"action\" ) actionParameter_df . createOrReplaceTempView ( \"actionParameter\" ) # Define the process code for which you want to find the related information process_code = \"code123\" # Query to find all related information for the given process code query = \"\"\" SELECT process.*, processStep.*, dataset.*, datasetElement.*, processStepElementMapping.*, actionType.*, action.*, actionParameter.* FROM process LEFT JOIN processStep ON process.ProcessID = processStep.ProcessID LEFT JOIN dataset ON processStep.SourceDatasetID = dataset.DatasetID OR processStep.DestinationDatasetID = dataset.DatasetID LEFT JOIN datasetElement ON dataset.DatasetID = datasetElement.DatasetID LEFT JOIN processStepElementMapping ON processStep.ProcessStepID = processStepElementMapping.ProcessStepID LEFT JOIN actionType ON processStep.ProcessStepTypeID = actionType.ActionTypeID LEFT JOIN action ON processStep.ActionID = action.ActionID LEFT JOIN actionParameter ON action.ActionID = actionParameter.ActionID WHERE process.ProcessCode = ' {} ' \"\"\" . format ( process_code ) # Execute the query and group the results by entities result_df = spark . sql ( query ) In this example, I'm using the LEFT JOIN operator to join the dataframes based on the relationships between the entities. I'm also using the WHERE clause to filter the results based on the given ProcessCode. It's worth noting that this is just an example, you may need to adjust the code as per your use case. Once you have the result of the query, you can use the groupBy and agg method as previously explained to group the result by entities. Query to group them \u00b6 Python # Group the results by Process grouped_by_process_df = result_df . groupBy ( \"ProcessCode\" , \"ProcessName\" ) . agg ( first ( \"ProcessID\" ) . alias ( \"ProcessID\" ), first ( \"ProcessDescription\" ) . alias ( \"ProcessDescription\" ), first ( \"TriggerPattern\" ) . alias ( \"TriggerPattern\" ), first ( \"KeyParameterTypeID\" ) . alias ( \"KeyParameterTypeID\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"MaxConcurrency\" ) . alias ( \"MaxConcurrency\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepID\" ) . alias ( \"ProcessStepIDList\" ), collect_list ( \"ProcessStepName\" ) . alias ( \"ProcessStepNameList\" ), collect_list ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescriptionList\" ), collect_list ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeIDList\" ), collect_list ( \"SubProcessID\" ) . alias ( \"SubProcessIDList\" ), collect_list ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrderList\" ), collect_list ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetIDList\" ), collect_list ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeIDList\" ), collect_list ( \"DestinationDatasetID\" ) . alias ( \"DestinationDatasetIDList\" ), collect_list ( \"ActionID\" ) . alias ( \"ActionIDList\" ), collect_list ( \"DatasetID\" ) . alias ( \"DatasetIDList\" ), collect_ grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"DatasetName\" ) . alias ( \"DatasetNameList\" ), collect_list ( \"DatasetDescription\" ) . alias ( \"DatasetDescriptionList\" ), collect_list ( \"DatasetTypeID\" ) . alias ( \"DatasetTypeIDList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"DatasetKey\" ) . alias ( \"DatasetKeyList\" ), collect_list ( \"DatasetSource\" ) . alias ( \"DatasetSourceList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"DatasetElementID\" ) . alias ( \"DatasetElementIDList\" ), collect_list ( \"DatasetElementName\" ) . alias ( \"DatasetElementNameList\" ), collect_list ( \"DatasetElementDescription\" ) . alias ( \"DatasetElementDescriptionList\" ), collect_list ( \"DatasetElementOrder\" ) . alias ( \"DatasetElementOrderList\" ), collect_list ( \"DatasetElementType\" ) . alias ( \"DatasetElementTypeList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ), collect_list ( \"ProcessStepElementMappingID\" ) . alias ( \"ProcessStepElementMappingIDList\" ), collect_list ( \"DestinationElementID\" ) . alias ( \"DestinationElementIDList\" ), collect_list ( \"ElementSourceTypeID\" ) . alias ( \"ElementSourceTypeIDList\" ), collect_list ( \"Source\" ) . alias ( \"SourceList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \" grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"ActionTypeID\" ) . alias ( \"ActionTypeIDList\" ), collect_list ( \"ActionTypeDescription\" ) . alias ( \"ActionTypeDescriptionList\" ), collect_list ( \"ActionName\" ) . alias ( \"ActionNameList\" ), collect_list ( \"ActionDescription\" ) . alias ( \"ActionDescriptionList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"ParameterExpression\" ) . alias ( \"ParameterExpressionList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"ActionParameterID\" ) . alias ( \"ActionParameterIDList\" ), collect_list ( \"ParameterName\" ) . alias ( \"ParameterNameList\" ), collect_list ( \"ParameterValueTypeID\" ) . alias ( \"ParameterValueTypeIDList\" ), collect_list ( \"ParameterValue\" ) . alias ( \"ParameterValueList\" ), collect_list ( \"ParameterOrder\" ) . alias ( \"ParameterOrderList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ) ) In this example , I 'm using the groupBy method to group the results by Process Code and Process Name, and using the agg method and the collect_list function to collect all the values for each unique Process Code and then group the resulting dataframe by process. It 's worth noting that this is just an example, you may need to adjust the code as per your use case. Flow Diagram \u00b6 flowchart TD subgraph Process ProcessID(ProcessID) --> ProcessCode(ProcessCode) ProcessCode --> ProcessName(ProcessName) ProcessName --> ProcessDescription(ProcessDescription) ProcessDescription --> TriggerPattern(TriggerPattern) TriggerPattern --> KeyParameterTypeID(KeyParameterTypeID) KeyParameterTypeID --> ImplementationInfo(ImplementationInfo) ImplementationInfo --> MaxConcurrency(MaxConcurrency) MaxConcurrency --> IsParallelProcess(IsParallelProcess) IsParallelProcess --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ProcessStep ProcessStepID(ProcessStepID) --> ProcessID(ProcessID) ProcessID --> ProcessStepName(ProcessStepName) ProcessStepName --> ProcessStepDescription(ProcessStepDescription) ProcessStepDescription --> ProcessStepTypeID(ProcessStepTypeID) ProcessStepTypeID --> SubProcessID(SubProcessID) SubProcessID --> ProcessStepOrder(ProcessStepOrder) ProcessStepOrder --> SourceDatasetID(SourceDatasetID) SourceDatasetID --> SourceDatasetValidationTypeID(SourceDatasetValidationTypeID) SourceDatasetValidationTypeID --> DestinationDatasetID(DestinationDatasetID) DestinationDatasetID --> ActionID(ActionID) ActionID --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph DataSet DatasetName --> DatasetDescription(DatasetDescription) DatasetDescription --> DatasetTypeID(DatasetTypeID) DatasetTypeID --> NamePattern(NamePattern) NamePattern --> LocationPattern(LocationPattern) LocationPattern --> DatasetKey(DatasetKey) DatasetKey --> DatasetSource(DatasetSource) DatasetSource --> ConnectionDescription(ConnectionDescription) ConnectionDescription --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph DataSetElement DatasetElementID(DatasetElementID) --> DatasetID(DatasetID) DatasetID --> DatasetElementName(DatasetElementName) DatasetElementName --> DatasetElementDescription(DatasetElementDescription) DatasetElementDescription --> DatasetElementOrder(DatasetElementOrder) DatasetElementOrder --> DatasetElementType(DatasetElementType) DatasetElementType --> IsRequired(IsRequired) IsRequired --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ProcessStepElementMapping ProcessStepElementMappingID(ProcessStepElementMappingID) --> ProcessStepID(ProcessStepID) ProcessStepID --> DestinationElementID(DestinationElementID) DestinationElementID --> ElementSourceTypeID(ElementSourceTypeID) ElementSourceTypeID --> Source(Source) Source --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ActionType ActionTypeID(ActionTypeID) --> ActionTypeDescription(ActionTypeDescription) ActionTypeDescription --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) end subgraph Action NamePattern --> LocationPattern(LocationPattern) LocationPattern --> ParameterExpression(ParameterExpression) ParameterExpression --> ConnectionDescription(ConnectionDescription) ConnectionDescription --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ActionParameter ActionParameterID(ActionParameterID) --> ActionID(ActionID) ActionID --> ParameterName(ParameterName) ParameterName --> ParameterValueTypeID(ParameterValueTypeID) ParameterValueTypeID --> ParameterValue(ParameterValue) ParameterValue --> ParameterOrder(ParameterOrder) ParameterOrder --> IsRequired(IsRequired) IsRequired --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) end Text Only Process --> ProcessStep ProcessStep --> SubProcess ProcessStep --> DataSet ProcessStep --> Action DataSet --> DataSetElement ProcessStep --> ProcessStepElementMapping ProcessStepElementMapping --> DataSetElement Action --> ActionParameter Action --> ActionType","title":"\ud83e\uddfa Data dictionary for metadata driven Synapse pipeline"},{"location":"explore/data-dcitionary-pipeline/#data-dictionary-for-metadata-driven-synapse-pipeline","text":"","title":"\ud83e\uddfa Data dictionary for metadata driven Synapse pipeline"},{"location":"explore/data-dcitionary-pipeline/#diagram","text":"graph TD; Process[Process] --> ProcessStep[ProcessStep] ProcessStep --> ProcessStepElementMapping[ProcessStepElementMapping] ProcessStepElementMapping --> DataSet[DataSet] DataSet --> DataSetElement[DataSetElement] ProcessStepElementMapping --> Action[Action] Action --> ActionType[ActionType] Action --> ActionParameter[ActionParameter]","title":"Diagram"},{"location":"explore/data-dcitionary-pipeline/#derive-the-entity-relation-from-given-list-of-entities-with-their-properties","text":"Process: ProcessID, ProcessCode, ProcessName, ProcessDescription, TriggerPattern, KeyParameterTypeID, ImplementationInfo, MaxConcurrency, IsParallelProcess, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ProcessStep: ProcessStepID, ProcessID, ProcessStepName, ProcessStepDescription, ProcessStepTypeID, SubProcessID, ProcessStepOrder, SourceDatasetID, SourceDatasetValidationTypeID, DestinationDatasetID, ActionID, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; DataSet: DatasetID, DatasetName, DatasetDescription, DatasetTypeID, NamePattern, LocationPattern, DatasetKey, DatasetSource, ConnectionDescription, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; DataSetElement: DatasetElementID, DatasetID, DatasetElementName, DatasetElementDescription, DatasetElementOrder, DatasetElementType, IsRequired, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ProcessStepElementMapping: ProcessStepElementMappingID, ProcessStepID, DestinationElementID, ElementSourceTypeID, Source, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ActionType: ActionTypeID, ActionTypeDescription, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy; Action: ActionID, ActionName, ActionDescription, ActionTypeID, NamePattern, LocationPattern, ParameterExpression, ConnectionDescription, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy, ValidFrom, ValidTo; ActionParameter: ActionParameterID, ActionID, ParameterName, ParameterValueTypeID, ParameterValue, ParameterOrder, IsRequired, EffectiveDate, ExpireDate, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy; The entities and their relationships are as follows: - Process has a one-to-many relationship with ProcessStep, as a Process can have multiple ProcessSteps, but a ProcessStep can only belong to one Process. The ProcessID field in the ProcessStep entity is used to establish this relationship. - ProcessStep has a one-to-one relationship with SubProcess, as a ProcessStep can only belong to one SubProcess, but a SubProcess can only have one ProcessStep. The SubProcessID field in the ProcessStep entity is used to establish this relationship. - ProcessStep has a many-to-one relationship with DataSet, as a ProcessStep can have a source or destination DataSet, but a DataSet can be used by multiple ProcessSteps. The SourceDatasetID and DestinationDatasetID fields in the ProcessStep entity are used to establish this relationship. - ProcessStep has a many-to-one relationship with Action, as a ProcessStep can have an action, but an Action can be used by multiple ProcessSteps. The ActionID field in the ProcessStep entity is used to establish this relationship. - DataSet has a one-to-many relationship with DataSetElement, as a DataSet can have multiple DataSetElements, but a DataSetElement can only belong to one DataSet. The DatasetID field in the DataSetElement entity is used to establish this relationship. - ProcessStep has a many-to-many relationship with DataSetElement, as a ProcessStep can have multiple DataSetElements, and a DataSetElement can be used by multiple ProcessSteps. The ProcessStepElementMapping entity is used to establish this relationship, with ProcessStepID and DestinationElementID fields. - Action has a one-to-many relationship with ActionParameter, as an Action can have multiple ActionParameters, but an ActionParameter can only belong to one Action. The ActionID field in the ActionParameter entity is used to establish this relationship. - Action has a one-to-one relationship with ActionType, as an Action can only have one ActionType, but an ActionType can be used by multiple Actions. The ActionTypeID field in the Action entity is used to establish this relationship.","title":"Derive the entity relation from given list of entities with their properties:"},{"location":"explore/data-dcitionary-pipeline/#schema-for-them-in-pyspark","text":"Python from pyspark.sql.types import * # Define the schema for the Process table process_schema = StructType ([ StructField ( \"ProcessID\" , IntegerType ()), StructField ( \"ProcessCode\" , StringType ()), StructField ( \"ProcessName\" , StringType ()), StructField ( \"ProcessDescription\" , StringType ()), StructField ( \"TriggerPattern\" , StringType ()), StructField ( \"KeyParameterTypeID\" , IntegerType ()), StructField ( \"ImplementationInfo\" , StringType ()), StructField ( \"MaxConcurrency\" , IntegerType ()), StructField ( \"IsParallelProcess\" , BooleanType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the Process DataFrame process_df = spark . createDataFrame ( data , process_schema ) # Define the schema for the ProcessStep table process_step_schema = StructType ([ StructField ( \"ProcessStepID\" , IntegerType ()), StructField ( \"ProcessID\" , IntegerType ()), StructField ( \"ProcessStepName\" , StringType ()), StructField ( \"ProcessStepDescription\" , StringType ()), StructField ( \"ProcessStepTypeID\" , IntegerType ()), StructField ( \"SubProcessID\" , IntegerType ()), StructField ( \"ProcessStepOrder\" , IntegerType ()), StructField ( \"SourceDatasetID\" , IntegerType ()), StructField ( \"SourceDatasetValidationTypeID\" , IntegerType ()), StructField ( \"DestinationDatasetID\" , IntegerType ()), StructField ( \"ActionID\" , IntegerType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the ProcessStep DataFrame process_step_df = spark . createDataFrame ( data , process_step_schema ) # Define the schema for the DataSet table dataset_schema = StructType ([ StructField ( \"DatasetID\" , IntegerType ()), StructField ( \"DatasetName\" , StringType ()), StructField ( \"DatasetDescription\" , StringType ()), StructField ( \"DatasetTypeID\" , IntegerType ()), StructField ( \"NamePattern\" , StringType ()), StructField ( \"LocationPattern\" , StringType ()), StructField ( \"DatasetKey\" , StringType ()), StructField ( \"DatasetSource\" , StringType ()), StructField ( \"ConnectionDescription\" , StringType ()), StructField ( \"EffectiveDate StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the DataSet DataFrame dataset_df = spark . createDataFrame ( data , dataset_schema ) # Define the schema for the DataSetElement table dataset_element_schema = StructType ([ StructField ( \"DatasetElementID\" , IntegerType ()), StructField ( \"DatasetID\" , IntegerType ()), StructField ( \"DatasetElementName\" , StringType ()), StructField ( \"DatasetElementDescription\" , StringType ()), StructField ( \"DatasetElementOrder\" , IntegerType ()), StructField ( \"DatasetElementType\" , StringType ()), StructField ( \"IsRequired\" , BooleanType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the DataSetElement DataFrame dataset_element_df = spark . createDataFrame ( data , dataset_element_schema ) # Define the schema for the ProcessStepElementMapping table process_step_element_mapping_schema = StructType ([ StructField ( \"ProcessStepElementMappingID\" , IntegerType ()), StructField ( \"ProcessStepID\" , IntegerType ()), StructField ( \"DestinationElementID\" , IntegerType ()), StructField ( \"ElementSourceTypeID\" , IntegerType ()), StructField ( \"Source\" , StringType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the ProcessStepElementMapping DataFrame process_step_element_mapping_df = spark . createDataFrame ( data , process_step_element_mapping_schema ) # Define the schema for the ActionType table action_type_schema = StructType ([ StructField ( \"ActionTypeID\" , IntegerType ()), StructField ( \"ActionTypeDescription\" , StringType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()) ]) # Create the ActionType DataFrame action_type_df = spark . createDataFrame ( data , action_type_schema ) # Define the schema for the Action table action_schema = StructType ([ StructField ( \"ActionID\" , IntegerType ()), StructField ( \"ActionName\" , StringType ()), StructField ( \"ActionDescription\" , StringType ()), StructField ( \"ActionTypeID\" , IntegerType ()), StructField ( \"NamePattern\" , StringType ()), StructField ( \"LocationPattern\" , StringType ()), StructField ( \"ParameterExpression\" , StringType ()), StructField ( \"ConnectionDescription\" , StringType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()), StructField ( \"ValidFrom\" , DateType ()), StructField ( \"ValidTo\" , DateType ()) ]) # Create the Action DataFrame action_df = spark . createDataFrame ( data , action_schema ) # Define the schema for the ActionParameter table action_parameter_schema = StructType ([ StructField ( \"ActionParameterID\" , IntegerType ()), StructField ( \"ActionID\" , IntegerType ()), StructField ( \"ParameterName\" , StringType ()), StructField ( \"ParameterValueTypeID\" , IntegerType ()), StructField ( \"ParameterValue\" , StringType ()), StructField ( \"ParameterOrder\" , IntegerType ()), StructField ( \"IsRequired\" , BooleanType ()), StructField ( \"EffectiveDate\" , DateType ()), StructField ( \"ExpireDate\" , DateType ()), StructField ( \"CreatedDatetime\" , TimestampType ()), StructField ( \"CreatedBy\" , StringType ()), StructField ( \"UpdatedDatetime\" , TimestampType ()), StructField ( \"UpdatedBy\" , StringType ()) ]) # Create the ActionParameter DataFrame action_parameter_df = spark . createDataFrame ( data , action_parameter_schema ) It's worth noting that the data variable used in the example is a list of rows that should be loaded into the DataFrames. Also, this is just an example, you may need to adjust the datatype of the fields or add/remove fields as per your use case. Here is how the relationships between the entities can be defined in PySpark: Python # Define the relationships between the tables process_df . createOrReplaceTempView ( \"process\" ) process_step_df . createOrReplaceTempView ( \"process_step\" ) dataset_df . createOrReplaceTempView ( \"dataset\" ) dataset_element_df . createOrReplaceTempView ( \"dataset_element\" ) process_step_element_mapping_df . createOrReplaceTempView ( \"process_step_element_mapping\" ) action_type_df . createOrReplaceTempView ( \"action_type\" ) action_df . createOrReplaceTempView ( \"action\" ) action_parameter_df . createOrReplaceTempView ( \"action_parameter\" ) # One-to-many relationship from Process to ProcessStep spark . sql ( \"SELECT process.*, process_step.* FROM process JOIN process_step ON process.ProcessID = process_step.ProcessID\" ) # One-to-one relationship from ProcessStep to SubProcess spark . sql ( \"SELECT process_step.*, sub_process.* FROM process_step JOIN sub_process ON process_step.SubProcessID = sub_process.SubProcessID\" ) # Many-to-one relationship from ProcessStep to DataSet spark . sql ( \"SELECT process_step.*, dataset.* FROM process_step JOIN dataset ON process_step.SourceDatasetID = dataset.DatasetID OR process_step.DestinationDatasetID = dataset.DatasetID\" ) # Many-to-one relationship from ProcessStep to Action spark . sql ( \"SELECT process_step.*, action.* FROM process_step JOIN action ON process_step.ActionID = action.ActionID\" ) # One-to-many relationship from DataSet to DataSetElement spark . sql ( \"SELECT dataset.*, dataset_element.* FROM dataset JOIN dataset_element ON dataset.DatasetID = dataset_element.DatasetID\" ) # Many-to-many relationship from ProcessStep to DataSetElement spark . sql ( \"SELECT process_step.*, dataset_element.* FROM process_step JOIN process_step_element_mapping ON process_step.ProcessStepID = process_step_element_mapping.ProcessStepID JOIN dataset_element ON process_step_element_mapping.DestinationElementID = dataset_element.DatasetElementID\" ) # One-to-many relationship from Action to ActionParameter spark . sql ( \"SELECT action.*, action_parameter.* FROM action JOIN action_parameter ON action.ActionID = action_parameter.ActionID\" ) # One-to-one relationship from Action to ActionType spark . sql ( \"SELECT action.*, action_type.* FROM action JOIN action_type ON action.ActionTypeID = action_type.ActionTypeID\" ) It is worth noting that this is just an example and you may need to adjust the query as per your requirement. Also, the above mentioned SQL Queries are used to join the dataframe and load the data in the required format.","title":"schema for them in pyspark"},{"location":"explore/data-dcitionary-pipeline/#pyspark-dataframe-from-individual-dataframe-which-are-grouped-by-process-processstep-and-so-on","text":"Use PySpark to group the individual dataframes by their respective entities and create a new dataframe. Here is an example of how you can group the dataframes by \"Process\" and create a new dataframe: Python from pyspark.sql.functions import * # Group the dataframes by Process grouped_process_df = process_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" ) . agg ( sum ( \"MaxConcurrency\" ) . alias ( \"TotalMaxConcurrency\" ), count ( \"*\" ) . alias ( \"NumOfProcessSteps\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ) ) # Create a new dataframe with the grouped data process_summary_df = grouped_process_df . select ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"TotalMaxConcurrency\" , \"NumOfProcessSteps\" , \"ImplementationInfo\" , \"IsParallelProcess\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" ) You can also group other dataframes by their respective entities in a similar way. Python grouped_processStep_df = processStep_df . groupBy ( \"ProcessStepID\" , \"ProcessID\" , \"ProcessStepName\" ) . agg ( first ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescription\" ), first ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeID\" ), first ( \"SubProcessID\" ) . alias ( \"SubProcessID\" ), first ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrder\" ), first ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetID\" ), first ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeID\" ), first ( \"DestinationDatasetID\" ) . alias ( \"DestinationDatasetID\" ), first ( \"ActionID\" ) . alias ( \"ActionID\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ) ) processStep_summary_df = grouped_processStep_df . select ( \"ProcessStepID\" , \"ProcessID\" , \"ProcessStepName\" , \"ProcessStepDescription\" , \"ProcessStepTypeID\" , \"SubProcessID\" , \"ProcessStepOrder\" , \"SourceDatasetID\" , \"SourceDatasetValidationTypeID\" , \"DestinationDatasetID\" , \"ActionID\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" ) In this example, I am using the groupBy method to group the dataframe by specific columns and the agg method to aggregate the data. You can use different aggregation functions like sum, count, first, last etc. to get the desired values for the new dataframe. It's worth noting that this is just an example, you may need to adjust the code as per your use case.","title":"Pyspark dataframe from individual dataframe which are grouped by process, processStep and so on"},{"location":"explore/data-dcitionary-pipeline/#now-lets-create-nested-group-starting-from-process","text":"Create nested groups starting from the \"Process\" dataframe in PySpark. Here is an example of how you can group the dataframes by \"Process\" and then group the resulting dataframe by \"ProcessStep\": Python from pyspark.sql.functions import * # Group the dataframes by Process grouped_process_df = process_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" ) . agg ( sum ( \"MaxConcurrency\" ) . alias ( \"TotalMaxConcurrency\" ), count ( \"*\" ) . alias ( \"NumOfProcessSteps\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepID\" ) . alias ( \"ProcessStepIDList\" ) ) # Group the resulting dataframe by ProcessStep grouped_processStep_df = grouped_process_df . selectExpr ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"TotalMaxConcurrency\" , \"NumOfProcessSteps\" , \"ImplementationInfo\" , \"IsParallelProcess\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" , \"explode(ProcessStepIDList) as ProcessStepID\" ) grouped_processStep_df = grouped_processStep_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"ProcessStepID\" ) . agg ( first ( \"TotalMaxConcurrency\" ) . alias ( \"TotalMaxConcurrency\" ), first ( \"NumOfProcessSteps\" ) . alias ( \"NumOfProcessSteps\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescriptionList\" ), collect_list ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeIDList\" ), collect_list ( \"SubProcessID\" ) . alias ( \"SubProcessIDList\" ), collect_list ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrderList\" ), collect_list ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetIDList\" ), collect_list ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeIDList\" ), collect_list ( \"DestinationDatasetIDList\" ) . alias ( \"DestinationDatasetIDList\" ), collect_list ( \"ActionID\" ) . alias ( \"ActionIDList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ) ) In this example, I am using the collect_list function to collect all the values for each unique ProcessStepID and then groupBy method to group the resulting dataframe by \"ProcessID\", \"ProcessCode\", \"ProcessName\" and \"ProcessStepID\". Once you have the nested group it's possible to explode the lists and have separate rows for each processStep by using selectExpr and explode method It's worth noting that this is just an example, you may need to adjust the code as per your use case. Example of how you can use the selectExpr and explode method to explode the lists and have separate rows for each processStep: Python # Explode the lists and create separate rows for each ProcessStep exploded_df = grouped_processStep_df . selectExpr ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"TotalMaxConcurrency\" , \"NumOfProcessSteps\" , \"ImplementationInfo\" , \"IsParallelProcess\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" , \"ProcessStepID\" , \"explode(ProcessStepDescriptionList) as ProcessStepDescription\" , \"explode(ProcessStepTypeIDList) as ProcessStepTypeID\" , \"explode(SubProcessIDList) as SubProcessID\" , \"explode(ProcessStepOrderList) as ProcessStepOrder\" , \"explode(SourceDatasetIDList) as SourceDatasetID\" , \"explode(SourceDatasetValidationTypeIDList) as SourceDatasetValidationTypeID\" , \"explode(DestinationDatasetIDList) as DestinationDatasetID\" , \"explode(ActionIDList) as ActionID\" , \"explode(EffectiveDateList) as EffectiveDate\" , \"explode(ExpireDateList) as ExpireDate\" , \"explode(CreatedDatetimeList) as CreatedDatetime\" , \"explode(CreatedByList) as CreatedBy\" , \"explode(UpdatedDatetimeList) as UpdatedDatetime\" , \"explode(UpdatedByList) as UpdatedBy\" , \"explode(ValidFromList) as ValidFrom\" , \"explode(ValidToList) as ValidTo\" ) This will create a new dataframe, exploded_df, where each row represents a unique ProcessStep. Each row will contain the corresponding ProcessStep information as well as the information from the parent Process. You can also use the selectExpr Python exploded_df . selectExpr ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" , \"ProcessStepID\" , \"ProcessStepDescription\" , \"ProcessStepTypeID\" , \"SubProcessID\" , \"ProcessStepOrder\" , \"SourceDatasetID\" , \"SourceDatasetValidationTypeID\" , \"DestinationDatasetID\" , \"ActionID\" , \"EffectiveDate\" , \"ExpireDate\" , \"CreatedDatetime\" , \"CreatedBy\" , \"UpdatedDatetime\" , \"UpdatedBy\" , \"ValidFrom\" , \"ValidTo\" ) This will select only the relevant columns for the exploded dataframe. It's worth noting that the above method will repeat the parent process information for each processStep, if you don't want this, you can use a join method to bring the parent process information. It's also worth noting that this is just an example, you may need to adjust the code as per your use case.","title":"Now let's create nested group starting from process"},{"location":"explore/data-dcitionary-pipeline/#all-related-information-from-dataframes-group-by-entities-for-a-given-process","text":"Use Spark SQL to query the dataframes and find all related information for a given process. Here is an example of how you can query the dataframes and group them by entities for a given process: Python from pyspark.sql.functions import * # Create a temporary view for each dataframe process_df . createOrReplaceTempView ( \"process\" ) processStep_df . createOrReplaceTempView ( \"processStep\" ) dataset_df . createOrReplaceTempView ( \"dataset\" ) datasetElement_df . createOrReplaceTempView ( \"datasetElement\" ) processStepElementMapping_df . createOrReplaceTempView ( \"processStepElementMapping\" ) actionType_df . createOrReplaceTempView ( \"actionType\" ) action_df . createOrReplaceTempView ( \"action\" ) actionParameter_df . createOrReplaceTempView ( \"actionParameter\" ) # Define the process ID for which you want to find the related information process_id = \"123\" # Query to find all related information for the given process ID query = \"\"\" SELECT process.*, processStep.*, dataset.*, datasetElement.*, processStepElementMapping.*, actionType.*, action.*, actionParameter.* FROM process LEFT JOIN processStep ON process.ProcessID = processStep.ProcessID LEFT JOIN dataset ON processStep.SourceDatasetID = dataset.DatasetID OR processStep.DestinationDatasetID = dataset.DatasetID LEFT JOIN datasetElement ON dataset.DatasetID = datasetElement.DatasetID LEFT JOIN processStepElementMapping ON processStep.ProcessStepID = processStepElementMapping.ProcessStepID LEFT JOIN actionType ON processStep.ProcessStepTypeID = actionType.ActionTypeID LEFT JOIN action ON processStep.ActionID = action.ActionID LEFT JOIN actionParameter ON action.ActionID = actionParameter.ActionID WHERE process.ProcessID = ' {} ' \"\"\" . format ( process_id ) # Execute the query and group the results by entities result_df = spark . sql ( query ) In this example, I'm using the LEFT JOIN operator to join the dataframes based on the relationships between the entities. I'm also using the WHERE clause to filter the results based on the given process ID. It's worth noting that this is just an example, you may need to adjust the code as per your use case. Use the groupBy method to group the results by entities after executing the query. Here is an example of how you can group the results: Python # Group the results by Process grouped_by_process_df = result_df . groupBy ( \"ProcessID\" , \"ProcessCode\" , \"ProcessName\" ) . agg ( first ( \"ProcessDescription\" ) . alias ( \"ProcessDescription\" ), first ( \"TriggerPattern\" ) . alias ( \"TriggerPattern\" ), first ( \"KeyParameterTypeID\" ) . alias ( \"KeyParameterTypeID\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"MaxConcurrency\" ) . alias ( \"MaxConcurrency\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepID\" ) . alias ( \"ProcessStepIDList\" ), collect_list ( \"ProcessStepName\" ) . alias ( \"ProcessStepNameList\" ), collect_list ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescriptionList\" ), collect_list ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeIDList\" ), collect_list ( \"SubProcessID\" ) . alias ( \"SubProcessIDList\" ), collect_list ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrderList\" ), collect_list ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetIDList\" ), collect_list ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeIDList\" ), collect_list # seems incomplete grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"DestinationDatasetID\" ) . alias ( \"DestinationDatasetIDList\" ), collect_list ( \"ActionID\" ) . alias ( \"ActionIDList\" ), collect_list ( \"DatasetID\" ) . alias ( \"DatasetIDList\" ), collect_list ( \"DatasetName\" ) . alias ( \"DatasetNameList\" ), collect_list ( \"DatasetDescription\" ) . alias ( \"DatasetDescriptionList\" ), collect_list ( \"DatasetTypeID\" ) . alias ( \"DatasetTypeIDList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"DatasetKey\" ) . alias ( \"DatasetKeyList\" ), collect_list ( \"DatasetSource\" ) . alias ( \"DatasetSourceList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"DatasetElementID\" ) . alias ( \"DatasetElementIDList\" ), collect_list ( \"DatasetElementName\" ) . alias ( \"DatasetElementNameList\" ), collect_list ( \"DatasetElementDescription\" ) . alias ( \"DatasetElementDescriptionList\" ), collect_list ( \"DatasetElementOrder\" ) . alias ( \"DatasetElementOrderList\" ), collect_list ( \"DatasetElementType\" ) . alias ( \"DatasetElementTypeList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ), collect_list ( \"ProcessStepElementMappingID\" ) . alias ( \"ProcessStepElementMappingIDList\" ), collect_list ( \"DestinationElementID\" ) . alias ( \"DestinationElementIDList\" ), collect_list ( \"ElementSourceTypeID\" ) . alias ( \"ElementSourceTypeIDList\" ), collect_list ( \"Source\" ) . alias ( \"SourceList\" ), collect_list # same here grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"ActionTypeID\" ) . alias ( \"ActionTypeIDList\" ), collect_list ( \"ActionTypeDescription\" ) . alias ( \"ActionTypeDescriptionList\" ), collect_list ( \"ActionName\" ) . alias ( \"ActionNameList\" ), collect_list ( \"ActionDescription\" ) . alias ( \"ActionDescriptionList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"ParameterExpression\" ) . alias ( \"ParameterExpressionList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"ActionParameterID\" ) . alias ( \"ActionParameterIDList\" ), collect_list ( \"ParameterName\" ) . alias ( \"ParameterNameList\" ), collect_list ( \"ParameterValueTypeID\" ) . alias ( \"ParameterValueTypeIDList\" ), collect_list ( \"ParameterValue\" ) . alias ( \"ParameterValueList\" ), collect_list ( \"ParameterOrder\" ) . alias ( \"ParameterOrderList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ), ) In this example, I'm using the groupBy method to group the results by Process ID, Process Code, and Process Name, and using the agg method and the collect_list function to collect all the values for each unique Process ID and then group the resulting dataframe by process. It's worth noting that this is just an example, you may need to adjust the code as per your use case.","title":"All related information from dataframes group by entities for a given process"},{"location":"explore/data-dcitionary-pipeline/#find-all-related-information-from-dataframes-group-by-entities-for-a-given-processcode","text":"use Spark SQL to query the dataframes and find all related information for a given ProcessCode. Here is an example of how you can query the dataframes and group them by entities for a given ProcessCode: Python from pyspark.sql.functions import * # Create a temporary view for each dataframe process_df . createOrReplaceTempView ( \"process\" ) processStep_df . createOrReplaceTempView ( \"processStep\" ) dataset_df . createOrReplaceTempView ( \"dataset\" ) datasetElement_df . createOrReplaceTempView ( \"datasetElement\" ) processStepElementMapping_df . createOrReplaceTempView ( \"processStepElementMapping\" ) actionType_df . createOrReplaceTempView ( \"actionType\" ) action_df . createOrReplaceTempView ( \"action\" ) actionParameter_df . createOrReplaceTempView ( \"actionParameter\" ) # Define the process code for which you want to find the related information process_code = \"code123\" # Query to find all related information for the given process code query = \"\"\" SELECT process.*, processStep.*, dataset.*, datasetElement.*, processStepElementMapping.*, actionType.*, action.*, actionParameter.* FROM process LEFT JOIN processStep ON process.ProcessID = processStep.ProcessID LEFT JOIN dataset ON processStep.SourceDatasetID = dataset.DatasetID OR processStep.DestinationDatasetID = dataset.DatasetID LEFT JOIN datasetElement ON dataset.DatasetID = datasetElement.DatasetID LEFT JOIN processStepElementMapping ON processStep.ProcessStepID = processStepElementMapping.ProcessStepID LEFT JOIN actionType ON processStep.ProcessStepTypeID = actionType.ActionTypeID LEFT JOIN action ON processStep.ActionID = action.ActionID LEFT JOIN actionParameter ON action.ActionID = actionParameter.ActionID WHERE process.ProcessCode = ' {} ' \"\"\" . format ( process_code ) # Execute the query and group the results by entities result_df = spark . sql ( query ) In this example, I'm using the LEFT JOIN operator to join the dataframes based on the relationships between the entities. I'm also using the WHERE clause to filter the results based on the given ProcessCode. It's worth noting that this is just an example, you may need to adjust the code as per your use case. Once you have the result of the query, you can use the groupBy and agg method as previously explained to group the result by entities.","title":"Find all related information from dataframes group by entities for a given ProcessCode"},{"location":"explore/data-dcitionary-pipeline/#query-to-group-them","text":"Python # Group the results by Process grouped_by_process_df = result_df . groupBy ( \"ProcessCode\" , \"ProcessName\" ) . agg ( first ( \"ProcessID\" ) . alias ( \"ProcessID\" ), first ( \"ProcessDescription\" ) . alias ( \"ProcessDescription\" ), first ( \"TriggerPattern\" ) . alias ( \"TriggerPattern\" ), first ( \"KeyParameterTypeID\" ) . alias ( \"KeyParameterTypeID\" ), first ( \"ImplementationInfo\" ) . alias ( \"ImplementationInfo\" ), first ( \"MaxConcurrency\" ) . alias ( \"MaxConcurrency\" ), first ( \"IsParallelProcess\" ) . alias ( \"IsParallelProcess\" ), first ( \"EffectiveDate\" ) . alias ( \"EffectiveDate\" ), first ( \"ExpireDate\" ) . alias ( \"ExpireDate\" ), first ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetime\" ), first ( \"CreatedBy\" ) . alias ( \"CreatedBy\" ), first ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetime\" ), first ( \"UpdatedBy\" ) . alias ( \"UpdatedBy\" ), first ( \"ValidFrom\" ) . alias ( \"ValidFrom\" ), first ( \"ValidTo\" ) . alias ( \"ValidTo\" ), collect_list ( \"ProcessStepID\" ) . alias ( \"ProcessStepIDList\" ), collect_list ( \"ProcessStepName\" ) . alias ( \"ProcessStepNameList\" ), collect_list ( \"ProcessStepDescription\" ) . alias ( \"ProcessStepDescriptionList\" ), collect_list ( \"ProcessStepTypeID\" ) . alias ( \"ProcessStepTypeIDList\" ), collect_list ( \"SubProcessID\" ) . alias ( \"SubProcessIDList\" ), collect_list ( \"ProcessStepOrder\" ) . alias ( \"ProcessStepOrderList\" ), collect_list ( \"SourceDatasetID\" ) . alias ( \"SourceDatasetIDList\" ), collect_list ( \"SourceDatasetValidationTypeID\" ) . alias ( \"SourceDatasetValidationTypeIDList\" ), collect_list ( \"DestinationDatasetID\" ) . alias ( \"DestinationDatasetIDList\" ), collect_list ( \"ActionID\" ) . alias ( \"ActionIDList\" ), collect_list ( \"DatasetID\" ) . alias ( \"DatasetIDList\" ), collect_ grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"DatasetName\" ) . alias ( \"DatasetNameList\" ), collect_list ( \"DatasetDescription\" ) . alias ( \"DatasetDescriptionList\" ), collect_list ( \"DatasetTypeID\" ) . alias ( \"DatasetTypeIDList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"DatasetKey\" ) . alias ( \"DatasetKeyList\" ), collect_list ( \"DatasetSource\" ) . alias ( \"DatasetSourceList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \"UpdatedDatetimeList\" ), collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"DatasetElementID\" ) . alias ( \"DatasetElementIDList\" ), collect_list ( \"DatasetElementName\" ) . alias ( \"DatasetElementNameList\" ), collect_list ( \"DatasetElementDescription\" ) . alias ( \"DatasetElementDescriptionList\" ), collect_list ( \"DatasetElementOrder\" ) . alias ( \"DatasetElementOrderList\" ), collect_list ( \"DatasetElementType\" ) . alias ( \"DatasetElementTypeList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ), collect_list ( \"ProcessStepElementMappingID\" ) . alias ( \"ProcessStepElementMappingIDList\" ), collect_list ( \"DestinationElementID\" ) . alias ( \"DestinationElementIDList\" ), collect_list ( \"ElementSourceTypeID\" ) . alias ( \"ElementSourceTypeIDList\" ), collect_list ( \"Source\" ) . alias ( \"SourceList\" ), collect_list ( \"EffectiveDate\" ) . alias ( \"EffectiveDateList\" ), collect_list ( \"ExpireDate\" ) . alias ( \"ExpireDateList\" ), collect_list ( \"CreatedDatetime\" ) . alias ( \"CreatedDatetimeList\" ), collect_list ( \"CreatedBy\" ) . alias ( \"CreatedByList\" ), collect_list ( \"UpdatedDatetime\" ) . alias ( \" grouped_by_process_df = grouped_by_process_df . agg ( collect_list ( \"UpdatedBy\" ) . alias ( \"UpdatedByList\" ), collect_list ( \"ValidFrom\" ) . alias ( \"ValidFromList\" ), collect_list ( \"ValidTo\" ) . alias ( \"ValidToList\" ), collect_list ( \"ActionTypeID\" ) . alias ( \"ActionTypeIDList\" ), collect_list ( \"ActionTypeDescription\" ) . alias ( \"ActionTypeDescriptionList\" ), collect_list ( \"ActionName\" ) . alias ( \"ActionNameList\" ), collect_list ( \"ActionDescription\" ) . alias ( \"ActionDescriptionList\" ), collect_list ( \"NamePattern\" ) . alias ( \"NamePatternList\" ), collect_list ( \"LocationPattern\" ) . alias ( \"LocationPatternList\" ), collect_list ( \"ParameterExpression\" ) . alias ( \"ParameterExpressionList\" ), collect_list ( \"ConnectionDescription\" ) . alias ( \"ConnectionDescriptionList\" ), collect_list ( \"ActionParameterID\" ) . alias ( \"ActionParameterIDList\" ), collect_list ( \"ParameterName\" ) . alias ( \"ParameterNameList\" ), collect_list ( \"ParameterValueTypeID\" ) . alias ( \"ParameterValueTypeIDList\" ), collect_list ( \"ParameterValue\" ) . alias ( \"ParameterValueList\" ), collect_list ( \"ParameterOrder\" ) . alias ( \"ParameterOrderList\" ), collect_list ( \"IsRequired\" ) . alias ( \"IsRequiredList\" ) ) In this example , I 'm using the groupBy method to group the results by Process Code and Process Name, and using the agg method and the collect_list function to collect all the values for each unique Process Code and then group the resulting dataframe by process. It 's worth noting that this is just an example, you may need to adjust the code as per your use case.","title":"Query to group them"},{"location":"explore/data-dcitionary-pipeline/#flow-diagram","text":"flowchart TD subgraph Process ProcessID(ProcessID) --> ProcessCode(ProcessCode) ProcessCode --> ProcessName(ProcessName) ProcessName --> ProcessDescription(ProcessDescription) ProcessDescription --> TriggerPattern(TriggerPattern) TriggerPattern --> KeyParameterTypeID(KeyParameterTypeID) KeyParameterTypeID --> ImplementationInfo(ImplementationInfo) ImplementationInfo --> MaxConcurrency(MaxConcurrency) MaxConcurrency --> IsParallelProcess(IsParallelProcess) IsParallelProcess --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ProcessStep ProcessStepID(ProcessStepID) --> ProcessID(ProcessID) ProcessID --> ProcessStepName(ProcessStepName) ProcessStepName --> ProcessStepDescription(ProcessStepDescription) ProcessStepDescription --> ProcessStepTypeID(ProcessStepTypeID) ProcessStepTypeID --> SubProcessID(SubProcessID) SubProcessID --> ProcessStepOrder(ProcessStepOrder) ProcessStepOrder --> SourceDatasetID(SourceDatasetID) SourceDatasetID --> SourceDatasetValidationTypeID(SourceDatasetValidationTypeID) SourceDatasetValidationTypeID --> DestinationDatasetID(DestinationDatasetID) DestinationDatasetID --> ActionID(ActionID) ActionID --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph DataSet DatasetName --> DatasetDescription(DatasetDescription) DatasetDescription --> DatasetTypeID(DatasetTypeID) DatasetTypeID --> NamePattern(NamePattern) NamePattern --> LocationPattern(LocationPattern) LocationPattern --> DatasetKey(DatasetKey) DatasetKey --> DatasetSource(DatasetSource) DatasetSource --> ConnectionDescription(ConnectionDescription) ConnectionDescription --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph DataSetElement DatasetElementID(DatasetElementID) --> DatasetID(DatasetID) DatasetID --> DatasetElementName(DatasetElementName) DatasetElementName --> DatasetElementDescription(DatasetElementDescription) DatasetElementDescription --> DatasetElementOrder(DatasetElementOrder) DatasetElementOrder --> DatasetElementType(DatasetElementType) DatasetElementType --> IsRequired(IsRequired) IsRequired --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ProcessStepElementMapping ProcessStepElementMappingID(ProcessStepElementMappingID) --> ProcessStepID(ProcessStepID) ProcessStepID --> DestinationElementID(DestinationElementID) DestinationElementID --> ElementSourceTypeID(ElementSourceTypeID) ElementSourceTypeID --> Source(Source) Source --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ActionType ActionTypeID(ActionTypeID) --> ActionTypeDescription(ActionTypeDescription) ActionTypeDescription --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) end subgraph Action NamePattern --> LocationPattern(LocationPattern) LocationPattern --> ParameterExpression(ParameterExpression) ParameterExpression --> ConnectionDescription(ConnectionDescription) ConnectionDescription --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) UpdatedBy --> ValidFrom(ValidFrom) ValidFrom --> ValidTo(ValidTo) end subgraph ActionParameter ActionParameterID(ActionParameterID) --> ActionID(ActionID) ActionID --> ParameterName(ParameterName) ParameterName --> ParameterValueTypeID(ParameterValueTypeID) ParameterValueTypeID --> ParameterValue(ParameterValue) ParameterValue --> ParameterOrder(ParameterOrder) ParameterOrder --> IsRequired(IsRequired) IsRequired --> EffectiveDate(EffectiveDate) EffectiveDate --> ExpireDate(ExpireDate) ExpireDate --> CreatedDatetime(CreatedDatetime) CreatedDatetime --> CreatedBy(CreatedBy) CreatedBy --> UpdatedDatetime(UpdatedDatetime) UpdatedDatetime --> UpdatedBy(UpdatedBy) end Text Only Process --> ProcessStep ProcessStep --> SubProcess ProcessStep --> DataSet ProcessStep --> Action DataSet --> DataSetElement ProcessStep --> ProcessStepElementMapping ProcessStepElementMapping --> DataSetElement Action --> ActionParameter Action --> ActionType","title":"Flow Diagram"},{"location":"explore/fastapi-uvicorn/","text":"Configure Fast API with Uvicorn in Dockerfile \u00b6 Create required app module \u00b6 To create the required app module for the uvicorn command in the Dockerfile, you will need to create a new file in your project directory. The file should be named after the app module specified in the Dockerfile command. For example, if the command in the Dockerfile is uvicorn my_app:app , the app module should be called my_app.py . In the app module file, you will need to create an instance of the FastAPI class, which is a web framework for building APIs with Python. Here is an example of what the app module file might look like: Python from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : str = None ): return { \"item_id\" : item_id , \"q\" : q } In this example, the FastAPI class is imported and an instance of the class is created. Then two routes are defined, / and /items/{item_id} with GET request handling function. You can add any other routes and functionality as per your requirement. Once you have created the app module file, you can then proceed to build the Docker image using the Dockerfile and commands I mentioned in my previous answer. Module in different folder/subfolder \u00b6 In the uvicorn command in the Dockerfile, you will need to provide the path to the app module file that you created. The path should be relative to the root directory of your project. For example, if your project directory is structured like this: Python my_project / app_module . py Dockerfile The path to the app module in the uvicorn command should be app_module:app . If your app module is in the subdirectory, you will need to provide the path to the subdirectory as well. For example, if your project directory is structured like this: Python my_project / my_app / __init__ . py app . py Dockerfile The path to the app module in the uvicorn command should be my_app.app:app . If your app module is in the subdirectory and subfolder, you will need to provide the path to the subdirectory and subfolder as well. For example, if your project directory is structured like this: Python my_project / my_app / api / __init__ . py app . py Dockerfile The path to the app module in the uvicorn command should be my_app.api.app:app . You can also use environment variable to provide the path of the app module, which is helpful when you're running your application in different environments. You can use ENV command in the Dockerfile to set environment variable and use it in uvicorn command. Python ENV APP_MODULE = app_module . app : app CMD uvicorn $ APP_MODULE -- host = 0.0.0.0 -- port = 8000 You can also use .env file to set environment variable and use it in uvicorn command. Python CMD sh - c \"source .env && uvicorn $APP_MODULE --host=0.0.0.0 --port=8000\" This way you can change the path of the app module easily without changing the Dockerfile.","title":"Configure Fast API with Uvicorn in Dockerfile"},{"location":"explore/fastapi-uvicorn/#configure-fast-api-with-uvicorn-in-dockerfile","text":"","title":"Configure Fast API with Uvicorn in Dockerfile"},{"location":"explore/fastapi-uvicorn/#create-required-app-module","text":"To create the required app module for the uvicorn command in the Dockerfile, you will need to create a new file in your project directory. The file should be named after the app module specified in the Dockerfile command. For example, if the command in the Dockerfile is uvicorn my_app:app , the app module should be called my_app.py . In the app module file, you will need to create an instance of the FastAPI class, which is a web framework for building APIs with Python. Here is an example of what the app module file might look like: Python from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : str = None ): return { \"item_id\" : item_id , \"q\" : q } In this example, the FastAPI class is imported and an instance of the class is created. Then two routes are defined, / and /items/{item_id} with GET request handling function. You can add any other routes and functionality as per your requirement. Once you have created the app module file, you can then proceed to build the Docker image using the Dockerfile and commands I mentioned in my previous answer.","title":"Create required app module"},{"location":"explore/fastapi-uvicorn/#module-in-different-foldersubfolder","text":"In the uvicorn command in the Dockerfile, you will need to provide the path to the app module file that you created. The path should be relative to the root directory of your project. For example, if your project directory is structured like this: Python my_project / app_module . py Dockerfile The path to the app module in the uvicorn command should be app_module:app . If your app module is in the subdirectory, you will need to provide the path to the subdirectory as well. For example, if your project directory is structured like this: Python my_project / my_app / __init__ . py app . py Dockerfile The path to the app module in the uvicorn command should be my_app.app:app . If your app module is in the subdirectory and subfolder, you will need to provide the path to the subdirectory and subfolder as well. For example, if your project directory is structured like this: Python my_project / my_app / api / __init__ . py app . py Dockerfile The path to the app module in the uvicorn command should be my_app.api.app:app . You can also use environment variable to provide the path of the app module, which is helpful when you're running your application in different environments. You can use ENV command in the Dockerfile to set environment variable and use it in uvicorn command. Python ENV APP_MODULE = app_module . app : app CMD uvicorn $ APP_MODULE -- host = 0.0.0.0 -- port = 8000 You can also use .env file to set environment variable and use it in uvicorn command. Python CMD sh - c \"source .env && uvicorn $APP_MODULE --host=0.0.0.0 --port=8000\" This way you can change the path of the app module easily without changing the Dockerfile.","title":"Module in different folder/subfolder"},{"location":"explore/generate-stock-schema-delta-api/","text":"Design stock and option schema and provide fast api to access using spark delta \u00b6 Create delta lake schema to record stock buy and sell \u00b6 Python { \"type\" : \"record\" , \"name\" : \"stock_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } The above schema defines a record for a stock transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, number of shares, and price per share. This schema can be used with Delta Lake to record stock buy and sell transactions. Also define schema to store buy sell of options \u00b6 Python { \"type\" : \"record\" , \"name\" : \"options_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"option_type\" , \"type\" : \"string\" }, { \"name\" : \"strike_price\" , \"type\" : \"double\" }, { \"name\" : \"expiration_date\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } This schema defines a record for an options transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, option type (call or put), strike price, expiration date, number of shares, and price per share. This schema can be used with Delta Lake to record buy and sell transactions for options. Create PySpark schema from previous schemas and provide code to read write and delete records \u00b6 Here is the PySpark code to create the schema for the stock_transaction record: Python from pyspark.sql.types import * stock_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) And here is the PySpark code to create the schema for the options_transaction record: Python options_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"option_type\" , StringType ()), StructField ( \"strike_price\" , DoubleType ()), StructField ( \"expiration_date\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) To read a Delta Lake table into a PySpark DataFrame, you can use the read method: df = spark.read.format(\"delta\").load(\"path/to/delta/table\") To write a DataFrame to a Delta Lake table, you can use the write method: df.write.format(\"delta\").save(\"path/to/delta/table\") To delete a Delta Lake table, you can use the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table \") It is important to note that the above code will delete the entire table. If you would like to delete specific records, you can use a filter in the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table WHERE stock_symbol='AAPL'\") Generate fast-api apis to interact with add, update delete process from PySpark \u00b6 Here is an example of how you could use the FastAPI library to create a REST API that interacts with a Delta Lake table using PySpark: Python from fastapi import FastAPI , HTTPException from pyspark.sql import SparkSession app = FastAPI () # Create a SparkSession spark = SparkSession . builder . appName ( \"DeltaLakeAPI\" ) . getOrCreate () @app . post ( \"/stock\" ) async def add_stock_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], stock_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/stock/ {transaction_id} \" ) async def update_stock_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/stock/ {transaction_id} \" ) async def delete_stock_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } This is an example to handle the stock transactions. You can use the same approach to handle option transaction as well. In the above example, the add_stock_transaction endpoint accepts a JSON object containing the transaction data, creates a DataFrame from that data, and writes it to the Delta Lake table. The update_stock_transaction endpoint accepts a transaction ID and a JSON object containing the updated transaction data, reads the Delta Lake table into a DataFrame, filters the DataFrame for the specified transaction, updates the values in the DataFrame with the incoming data, and overwrites the Delta Lake table with the updated DataFrame. The delete_stock_transaction endpoint accepts a transaction ID and deletes the specified transaction from the Delta Lake table. It is important to note that the above code is an example and may require additional error handling and validation. Also, you should update the path of delta table in the code to use your own delta table. Generate similar api for options schema \u00b6 Here's an example of how you could use the same approach to create REST APIs for the options_transaction schema: Python @app . post ( \"/options\" ) async def add_options_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], options_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/options/ {transaction_id} \" ) async def update_options_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/options/ {transaction_id} \" ) async def delete_options_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/options_delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } The above code is similar to the previous code for handling stock transactions, but it uses the options_transaction schema and different endpoint paths and file paths. As with the previous example, you should update the path of delta table in the code to use your own delta table. Provide api for profit and loss for both stocks and options \u00b6 Here's an example of how you could create a REST API endpoint for calculating profit and loss for both stock and options transactions using PySpark: Python @app . get ( \"/profit_loss/ {stock_symbol} \" ) async def get_profit_loss ( stock_symbol : str ): # Read the stock transaction table into a DataFrame stock_df = spark . read . format ( \"delta\" ) . load ( \"path/to/stock_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Read the options transaction table into a DataFrame options_df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Calculate the profit and loss for the stock transactions stock_profit_loss = stock_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) stock_profit_loss = stock_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] # Calculate the profit and loss for the options transactions options_profit_loss = options_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) options_profit_loss = options_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] return { \"stock_profit_loss\" : stock_profit_loss , \"options_profit_loss\" : options_profit_loss } In the above code, the get_profit_loss endpoint accepts a stock symbol, reads the stock transaction table and options transaction table into DataFrames, filters the DataFrames for transactions with the specified stock symbol, calculates the profit and loss for the stock and options transactions using the formula (price - cost) * shares, and returns the calculated values in a JSON response. In the above code, cost is not present in the schema, you need to add that accordingly. Also, you should update the path of delta table in the code to use your own delta table. It is important to note that the above code is an example and may require additional error handling and validation.","title":"Design stock and option schema and provide fast api to access using spark delta"},{"location":"explore/generate-stock-schema-delta-api/#design-stock-and-option-schema-and-provide-fast-api-to-access-using-spark-delta","text":"","title":"Design stock and option schema and provide fast api to access using spark delta"},{"location":"explore/generate-stock-schema-delta-api/#create-delta-lake-schema-to-record-stock-buy-and-sell","text":"Python { \"type\" : \"record\" , \"name\" : \"stock_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } The above schema defines a record for a stock transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, number of shares, and price per share. This schema can be used with Delta Lake to record stock buy and sell transactions.","title":"Create delta lake schema to record stock buy and sell"},{"location":"explore/generate-stock-schema-delta-api/#also-define-schema-to-store-buy-sell-of-options","text":"Python { \"type\" : \"record\" , \"name\" : \"options_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"option_type\" , \"type\" : \"string\" }, { \"name\" : \"strike_price\" , \"type\" : \"double\" }, { \"name\" : \"expiration_date\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } This schema defines a record for an options transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, option type (call or put), strike price, expiration date, number of shares, and price per share. This schema can be used with Delta Lake to record buy and sell transactions for options.","title":"Also define schema to store buy sell of options"},{"location":"explore/generate-stock-schema-delta-api/#create-pyspark-schema-from-previous-schemas-and-provide-code-to-read-write-and-delete-records","text":"Here is the PySpark code to create the schema for the stock_transaction record: Python from pyspark.sql.types import * stock_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) And here is the PySpark code to create the schema for the options_transaction record: Python options_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"option_type\" , StringType ()), StructField ( \"strike_price\" , DoubleType ()), StructField ( \"expiration_date\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) To read a Delta Lake table into a PySpark DataFrame, you can use the read method: df = spark.read.format(\"delta\").load(\"path/to/delta/table\") To write a DataFrame to a Delta Lake table, you can use the write method: df.write.format(\"delta\").save(\"path/to/delta/table\") To delete a Delta Lake table, you can use the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table \") It is important to note that the above code will delete the entire table. If you would like to delete specific records, you can use a filter in the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table WHERE stock_symbol='AAPL'\")","title":"Create PySpark schema from previous schemas and provide code to read write and delete records"},{"location":"explore/generate-stock-schema-delta-api/#generate-fast-api-apis-to-interact-with-add-update-delete-process-from-pyspark","text":"Here is an example of how you could use the FastAPI library to create a REST API that interacts with a Delta Lake table using PySpark: Python from fastapi import FastAPI , HTTPException from pyspark.sql import SparkSession app = FastAPI () # Create a SparkSession spark = SparkSession . builder . appName ( \"DeltaLakeAPI\" ) . getOrCreate () @app . post ( \"/stock\" ) async def add_stock_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], stock_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/stock/ {transaction_id} \" ) async def update_stock_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/stock/ {transaction_id} \" ) async def delete_stock_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } This is an example to handle the stock transactions. You can use the same approach to handle option transaction as well. In the above example, the add_stock_transaction endpoint accepts a JSON object containing the transaction data, creates a DataFrame from that data, and writes it to the Delta Lake table. The update_stock_transaction endpoint accepts a transaction ID and a JSON object containing the updated transaction data, reads the Delta Lake table into a DataFrame, filters the DataFrame for the specified transaction, updates the values in the DataFrame with the incoming data, and overwrites the Delta Lake table with the updated DataFrame. The delete_stock_transaction endpoint accepts a transaction ID and deletes the specified transaction from the Delta Lake table. It is important to note that the above code is an example and may require additional error handling and validation. Also, you should update the path of delta table in the code to use your own delta table.","title":"Generate fast-api apis to interact with add, update delete process from PySpark"},{"location":"explore/generate-stock-schema-delta-api/#generate-similar-api-for-options-schema","text":"Here's an example of how you could use the same approach to create REST APIs for the options_transaction schema: Python @app . post ( \"/options\" ) async def add_options_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], options_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/options/ {transaction_id} \" ) async def update_options_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/options/ {transaction_id} \" ) async def delete_options_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/options_delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } The above code is similar to the previous code for handling stock transactions, but it uses the options_transaction schema and different endpoint paths and file paths. As with the previous example, you should update the path of delta table in the code to use your own delta table.","title":"Generate similar api for options schema"},{"location":"explore/generate-stock-schema-delta-api/#provide-api-for-profit-and-loss-for-both-stocks-and-options","text":"Here's an example of how you could create a REST API endpoint for calculating profit and loss for both stock and options transactions using PySpark: Python @app . get ( \"/profit_loss/ {stock_symbol} \" ) async def get_profit_loss ( stock_symbol : str ): # Read the stock transaction table into a DataFrame stock_df = spark . read . format ( \"delta\" ) . load ( \"path/to/stock_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Read the options transaction table into a DataFrame options_df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Calculate the profit and loss for the stock transactions stock_profit_loss = stock_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) stock_profit_loss = stock_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] # Calculate the profit and loss for the options transactions options_profit_loss = options_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) options_profit_loss = options_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] return { \"stock_profit_loss\" : stock_profit_loss , \"options_profit_loss\" : options_profit_loss } In the above code, the get_profit_loss endpoint accepts a stock symbol, reads the stock transaction table and options transaction table into DataFrames, filters the DataFrames for transactions with the specified stock symbol, calculates the profit and loss for the stock and options transactions using the formula (price - cost) * shares, and returns the calculated values in a JSON response. In the above code, cost is not present in the schema, you need to add that accordingly. Also, you should update the path of delta table in the code to use your own delta table. It is important to note that the above code is an example and may require additional error handling and validation.","title":"Provide api for profit and loss for both stocks and options"},{"location":"explore/generate-stock-schema-vbt/","text":"Stock and Option Schema and strategies with VBT \u00b6 Generate schema for stocks and options using pyspark with delta lake including charges \u00b6 Here is an example schema for a stocks and options table using PySpark and Delta Lake: Python from pyspark.sql.types import StructType , StructField , StringType , DoubleType , IntegerType schema = StructType ([ StructField ( \"symbol\" , StringType (), True ), StructField ( \"timestamp\" , StringType (), True ), StructField ( \"price\" , DoubleType (), True ), StructField ( \"volume\" , IntegerType (), True ), StructField ( \"strike_price\" , DoubleType (), True ), StructField ( \"expiration_date\" , StringType (), True ), StructField ( \"option_type\" , StringType (), True ), StructField ( \"charge\" , DoubleType (), True ) ]) This schema includes the following fields: \"symbol\": a string representing the stock or option symbol \"timestamp\": a string representing the timestamp of the stock or option data \"price\": a double representing the price of the stock or option \"volume\": an integer representing the volume of the stock or option \"strike_price\": a double representing the strike price of the option \"expiration_date\": a string representing the expiration date of the option \"option_type\": a string representing the option type (e.g. \"call\" or \"put\") \"charge\": a double representing any charges associated with the stock or option You can use this schema to create a Delta Lake table using the create method, and then read and write data to the table using PySpark DataFrames. Python from delta.tables import * # create the delta table ( spark . read . format ( \"delta\" ) . load ( \"path/to/table\" ) . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/table\" )) Please note that this is a basic example, you should adapt this schema to your specific use case, and that the path/to/table is a example and should be replaced by the actual path to your table. Create ichimoku strategy using vectorbt \u00b6 Here is an example of how to create an Ichimoku strategy using the vectorbt library: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the strategy strategy = vbt . Strategy ( indicator_df , entry_rule = lambda x : x . tenkan_sen_cross_kijun_sen , exit_rule = lambda x : x . chikou_span_cross_price ) # Run the strategy results_df = strategy . run ( df ) In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. The strategy is created using the vbt.Strategy() class and passed the indicator_df variable. The entry_rule is set to the tenkan_sen crossing the kijun_sen and exit_rule is set to chikou_span crossing the price. The run() method is then called on the strategy and passed the df variable, which returns a DataFrame containing the results of the strategy, such as the entry and exit signals, PnL, and performance metrics. Please note that this is a basic example and that you should adapt it to your specific use case, and that the column names in your dataframe may vary than the one in this example. Generate signal to buy when close price above chiku span and above ichimoku cloudu and sell signal when close price is below chiku span and below ichimoku cloud \u00b6 Here is an example of how to generate buy and sell signals using the vectorbt library based on the close price being above the Chikou Span and above the Ichimoku Cloud, and below the Chikou Span and below the Ichimoku Cloud respectively: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the buy signal buy_signal = ( df [ 'close' ] > indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] > indicator_df [ 'senkou_span_a' ]) # Create the sell signal sell_signal = ( df [ 'close' ] < indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] < indicator_df [ 'senkou_span_a' ]) # Add the signals to the DataFrame df [ 'buy_signal' ] = buy_signal df [ 'sell_signal' ] = sell_signal In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. The buy signal is generated by checking whether the close price is greater than the Chikou Span and greater than the Senkou Span A (which forms the top of the Ichimoku cloud). The sell signal is generated by checking whether the close price is less than the Chikou Span and less than the Senkou Span A. The buy and sell signals are added as new columns to the DataFrame and can be used to make trading decisions. Please note that this is a basic example and that you should adapt it to your specific use case, and that the column names in your dataframe may vary than the one in this example. Generate same strategy using vectorbtpro \u00b6 Here is an example of how to generate the same buy and sell signals using the vectorbtpro library: Python import vectorbt as vbt import vectorbt.signals as signals # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the buy signal buy_signal = signals . CrossSignal . from_boolean ( ( df [ 'close' ] > indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] > indicator_df [ 'senkou_span_a' ]), win_type = 'expanding' , center = True ) # Create the sell signal sell_signal = signals . CrossSignal . from_boolean ( ( df [ 'close' ] < indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] < indicator_df [ 'senkou_span_a' ]), win_type = 'expanding' , center = True ) # Run the strategy strategy = vbt . Strategy ( df , entry_signal = buy_signal , exit_signal = sell_signal ) results_df = strategy . run () In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. The buy and sell signals are generated by creating instances of the CrossSignal class from the boolean conditions that check if close price is above chiku span and above ichimoku cloud or below chiku span and below ichimoku cloud. The win_type is set to 'expanding' and center=True so that the signal will be generated when the condition is met for the first time and will be held until the condition is no longer met. The signals are passed to the vbt.Strategy class as the entry_signal and exit_signal, respectively. The run() method is then called on the strategy, which returns a DataFrame containing the results of the strategy, such as the entry and exit signals, PnL, and performance metrics. Please note that this is a basic example and that you should adapt it to your specific use case, and that the column names in your dataframe may vary than the one in this example. Strategy to generate signal for unusual volume for the assets \u00b6 Here is an example of how to generate a signal for unusual volume using the vectorbt library: Python import vectorbt as vbt # Compute the average volume over a rolling window mean_volume = df [ 'volume' ] . rolling ( window = 10 ) . mean () # Compute the standard deviation of volume over a rolling window std_volume = df [ 'volume' ] . rolling ( window = 10 ) . std () # Create the signal unusual_volume_signal = df [ 'volume' ] > mean_volume + 2 * std_volume # Add the signal to the DataFrame df [ 'unusual_volume_signal' ] = unusual_volume_signal In this example, the df variable is a DataFrame containing the historical volume and other relevant data for the assets you are trading. The average volume is computed over a rolling window of 10 days using the rolling() method and the mean() function, and assigned to the mean_volume variable. Similarly, the standard deviation of volume is computed over a rolling window of 10 days using the rolling() method and the std() function, and assigned to the std_volume variable. The signal for unusual volume is generated by checking whether the volume is greater than the average volume plus 2 times the standard deviation of volume. The signal is added as a new column to the DataFrame and can be used to make trading decisions. Please note that this is a basic example and that you should adapt it to your specific use case, and that the window size and the threshold for unusual volume might vary depending on the assets and the market conditions. Also, the column names in your dataframe may vary than the one in this example. Backtest both strategies using vectorbt \u00b6 Here is an example of how to backtest both the Ichimoku strategy and the unusual volume strategy using the vectorbt library: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy ichimoku_strategy = vbt . Strategy ( indicator_df , entry_rule = lambda x : x . tenkan_sen_cross_kijun_sen , exit_rule = lambda x : x . chikou_span_cross_price ) # Backtest the Ichimoku strategy ichimoku_results_df = ichimoku_strategy . run ( df ) # Compute the average volume over a rolling window mean_volume = df [ 'volume' ] . rolling ( window = 10 ) . mean () # Compute the standard deviation of volume over a rolling window std_volume = df [ 'volume' ] . rolling ( window = 10 ) . std () # Create the unusual volume signal unusual_volume_signal = df [ 'volume' ] > mean_volume + 2 * std_volume # Create the unusual volume strategy unusual_volume_strategy = vbt . Strategy ( df , entry_signal = unusual_volume_signal ) # Backtest the unusual volume strategy unusual_volume_results_df = unusual_volume_strategy . run () In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. A vbt.Strategy object is then created using the indicator_df and the entry and exit rules defined as lambda functions. Then the run() method is called on the strategy which returns the DataFrame of the results. The signal for unusual volume is generated by checking whether the volume is greater than the average volume plus 2 times the standard deviation of volume. A vbt.Strategy object is then created using the DataFrame and the entry signal for unusual volume. Then the run() method is called on the strategy which returns the DataFrame of the results. You can then use the results DataFrames to analyze the performance of the strategies, such as the PnL, the number of trades, the win rate, the maximum drawdown, etc. Please note that this is a basic example and that you should adapt it to your specific use case, and that the window size and the threshold for unusual volume might vary depending on the assets and the market conditions. Also, the column names in your dataframe may vary than the one in this example. Generate alerts for signals \u00b6 Once you have generated the signals, you can use the vectorbt library to create alerts based on them. Here is an example of how to create alerts for the Ichimoku strategy and the unusual volume strategy: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy ichimoku_strategy = vbt . Strategy ( indicator_df , entry_rule = lambda x : x . tenkan_sen_cross_kijun_sen , exit_rule = lambda x : x . chikou_span_cross_price ) # Backtest the Ichimoku strategy ichimoku_results_df = ichimoku_strategy . run ( df ) # Create an alert for the Ichimoku strategy ichimoku_alert = ichimoku_strategy . alerts . for_signals ( 'entry' , send_alert = lambda x : print ( f 'Ichimoku strategy: Buy signal at { x . name } ' ) ) # Compute the average volume over a rolling window mean_volume = df [ 'volume' ] . rolling ( window = 10 ) . mean () # Compute the standard deviation of volume over a rolling window std_volume = df [ 'volume' ] . rolling ( window = 10 ) . std () # Create the unusual volume signal unusual_volume_signal = df [ 'volume' ] > mean_volume + 2 * std_volume # Create the unusual volume strategy unusual_volume_strategy = vbt . Strategy ( df , entry_signal = unusual_volume_signal ) # Backtest the unusual volume strategy unusual_volume_results_df = unusual_volume_strategy . run () # Create an alert for the unusual volume strategy unusual_volume_alert = unusual_volume_strategy . alerts . for_signals ( 'entry' , send_alert = lambda x : print ( f 'Unusual volume strategy: Buy signal at { x . name } ' ) ) In this example, the vectorbt library is used to create alerts for each of the two strategies. The alerts.for_signals() method is called on the strategy object, which accepts the signal type ('entry' in this case) and a function send_alert that will be called when the signal is triggered. The function send_alert is passed a lambda function that prints a message to the console when the signal is triggered. This can be replaced with any other method of alerting, such as sending an email or a text message, or updating a database. Please note that this is a basic example and that you should adapt it to your specific use case and that the column names in your dataframe may vary than the one in this example. Optimize performance of strategies using numba \u00b6 You can use the numba library to optimize the performance of your strategies built using vectorbt. numba is a just-in-time (JIT) compiler that can be used to speed up the execution of Python code. Here is an example of how to optimize the performance of the Ichimoku strategy using numba: Python import numba as nb import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy @nb . njit def ichimoku_strategy ( indicator_df ): entry_rule = indicator_df [ 'tenkan_sen_cross_kijun_sen' ] exit_rule = indicator_df [ 'chikou_span_cross_price' ] return entry_rule , exit_rule ichimoku_strategy = vbt . Strategy ( indicator_df , entry_rule = ichimoku_strategy ) # Backtest the Ichimoku strategy ichimoku_results_df = ichimoku_strategy . run ( df ) In this example, the numba.njit decorator is used to optimize the performance of the ichimoku_strategy function. This function contains the entry and exit rules for the Ichimoku strategy. The @nb .njit decorator tells numba to compile the function, which results in faster execution times. You can also use numba.jit decorator that will try to optimize the code, but it will not throw an error if it fails. You can use this approach to optimize any other part of your code that is computationally intensive. Please note that this is a basic example and that you should adapt it to your specific use case. Also, that numba optimization may not always be the best option, as it can have some overhead and the performance gains may be minimal for small data sets. Create strategies and alerts in pyspark \u00b6 Here is an example of how to create the Ichimoku strategy and the unusual volume strategy, as well as the associated alerts, using PySpark: Python from pyspark.sql import SparkSession from pyspark.sql.functions import col , when , lead # Create a SparkSession spark = SparkSession . builder . appName ( \"Ichimoku_Unusual_Volume\" ) . getOrCreate () # Read the data into a DataFrame df = spark . read . csv ( \"path_to_data.csv\" , header = True , inferSchema = True ) # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy ichimoku_strategy = indicator_df . withColumn ( \"buy_signal\" , when (( col ( \"tenkan_sen\" ) > col ( \"kijun_sen\" )) & ( col ( \"close\" ) > col ( \"senkou_span_a\" )), 1 ) . otherwise ( 0 )) # Create the alert for the Ichimoku strategy ichimoku_alert = ichimoku_strategy . filter ( col ( \"buy_signal\" ) == 1 ) . select ( \"timestamp\" , \"stock\" ) . show () # Compute the average volume over a rolling window mean_volume = df . select ( avg ( \"volume\" ) . over ( Window . orderBy ( \"timestamp\" ) . rowsBetween ( - 10 , 0 ))) . alias ( \"mean_volume\" ) df = df . join ( mean_volume , \"timestamp\" ) # Compute the standard deviation of volume over a rolling window std_volume = df . select ( stddev ( \"volume\" ) . over ( Window . orderBy ( \"timestamp\" ) . rowsBetween ( - 10 , 0 ))) . alias ( \"std_volume\" ) df = df . join ( std_volume , \"timestamp\" ) # Create the unusual volume strategy unusual_volume_strategy = df . withColumn ( \"buy_signal\" , when (( col ( \"volume\" ) > col ( \"mean_volume\" ) + 2 * col ( \"std_volume\" )), 1 ) . otherwise ( 0 )) # Create the alert for the unusual volume strategy unusual_volume_alert = unusual_volume_strategy . filter ( col ( \"buy_signal\" ) == 1 ) . select ( \"timestamp\" , \"stock\" ) . show () In this example, the data is read into a DataFrame using the spark.read.csv() method. The Ichimoku indicators are calculated using the `vbt.indicators","title":"Stock and Option Schema and strategies with VBT"},{"location":"explore/generate-stock-schema-vbt/#stock-and-option-schema-and-strategies-with-vbt","text":"","title":"Stock and Option Schema and strategies with VBT"},{"location":"explore/generate-stock-schema-vbt/#generate-schema-for-stocks-and-options-using-pyspark-with-delta-lake-including-charges","text":"Here is an example schema for a stocks and options table using PySpark and Delta Lake: Python from pyspark.sql.types import StructType , StructField , StringType , DoubleType , IntegerType schema = StructType ([ StructField ( \"symbol\" , StringType (), True ), StructField ( \"timestamp\" , StringType (), True ), StructField ( \"price\" , DoubleType (), True ), StructField ( \"volume\" , IntegerType (), True ), StructField ( \"strike_price\" , DoubleType (), True ), StructField ( \"expiration_date\" , StringType (), True ), StructField ( \"option_type\" , StringType (), True ), StructField ( \"charge\" , DoubleType (), True ) ]) This schema includes the following fields: \"symbol\": a string representing the stock or option symbol \"timestamp\": a string representing the timestamp of the stock or option data \"price\": a double representing the price of the stock or option \"volume\": an integer representing the volume of the stock or option \"strike_price\": a double representing the strike price of the option \"expiration_date\": a string representing the expiration date of the option \"option_type\": a string representing the option type (e.g. \"call\" or \"put\") \"charge\": a double representing any charges associated with the stock or option You can use this schema to create a Delta Lake table using the create method, and then read and write data to the table using PySpark DataFrames. Python from delta.tables import * # create the delta table ( spark . read . format ( \"delta\" ) . load ( \"path/to/table\" ) . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/table\" )) Please note that this is a basic example, you should adapt this schema to your specific use case, and that the path/to/table is a example and should be replaced by the actual path to your table.","title":"Generate schema for stocks and options using pyspark with delta lake including charges"},{"location":"explore/generate-stock-schema-vbt/#create-ichimoku-strategy-using-vectorbt","text":"Here is an example of how to create an Ichimoku strategy using the vectorbt library: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the strategy strategy = vbt . Strategy ( indicator_df , entry_rule = lambda x : x . tenkan_sen_cross_kijun_sen , exit_rule = lambda x : x . chikou_span_cross_price ) # Run the strategy results_df = strategy . run ( df ) In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. The strategy is created using the vbt.Strategy() class and passed the indicator_df variable. The entry_rule is set to the tenkan_sen crossing the kijun_sen and exit_rule is set to chikou_span crossing the price. The run() method is then called on the strategy and passed the df variable, which returns a DataFrame containing the results of the strategy, such as the entry and exit signals, PnL, and performance metrics. Please note that this is a basic example and that you should adapt it to your specific use case, and that the column names in your dataframe may vary than the one in this example.","title":"Create ichimoku strategy using vectorbt"},{"location":"explore/generate-stock-schema-vbt/#generate-signal-to-buy-when-close-price-above-chiku-span-and-above-ichimoku-cloudu-and-sell-signal-when-close-price-is-below-chiku-span-and-below-ichimoku-cloud","text":"Here is an example of how to generate buy and sell signals using the vectorbt library based on the close price being above the Chikou Span and above the Ichimoku Cloud, and below the Chikou Span and below the Ichimoku Cloud respectively: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the buy signal buy_signal = ( df [ 'close' ] > indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] > indicator_df [ 'senkou_span_a' ]) # Create the sell signal sell_signal = ( df [ 'close' ] < indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] < indicator_df [ 'senkou_span_a' ]) # Add the signals to the DataFrame df [ 'buy_signal' ] = buy_signal df [ 'sell_signal' ] = sell_signal In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. The buy signal is generated by checking whether the close price is greater than the Chikou Span and greater than the Senkou Span A (which forms the top of the Ichimoku cloud). The sell signal is generated by checking whether the close price is less than the Chikou Span and less than the Senkou Span A. The buy and sell signals are added as new columns to the DataFrame and can be used to make trading decisions. Please note that this is a basic example and that you should adapt it to your specific use case, and that the column names in your dataframe may vary than the one in this example.","title":"Generate signal to buy when close price above chiku span and above ichimoku cloudu and sell signal when close price is below chiku span and below ichimoku cloud"},{"location":"explore/generate-stock-schema-vbt/#generate-same-strategy-using-vectorbtpro","text":"Here is an example of how to generate the same buy and sell signals using the vectorbtpro library: Python import vectorbt as vbt import vectorbt.signals as signals # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the buy signal buy_signal = signals . CrossSignal . from_boolean ( ( df [ 'close' ] > indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] > indicator_df [ 'senkou_span_a' ]), win_type = 'expanding' , center = True ) # Create the sell signal sell_signal = signals . CrossSignal . from_boolean ( ( df [ 'close' ] < indicator_df [ 'chikou_span' ]) & ( df [ 'close' ] < indicator_df [ 'senkou_span_a' ]), win_type = 'expanding' , center = True ) # Run the strategy strategy = vbt . Strategy ( df , entry_signal = buy_signal , exit_signal = sell_signal ) results_df = strategy . run () In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. The buy and sell signals are generated by creating instances of the CrossSignal class from the boolean conditions that check if close price is above chiku span and above ichimoku cloud or below chiku span and below ichimoku cloud. The win_type is set to 'expanding' and center=True so that the signal will be generated when the condition is met for the first time and will be held until the condition is no longer met. The signals are passed to the vbt.Strategy class as the entry_signal and exit_signal, respectively. The run() method is then called on the strategy, which returns a DataFrame containing the results of the strategy, such as the entry and exit signals, PnL, and performance metrics. Please note that this is a basic example and that you should adapt it to your specific use case, and that the column names in your dataframe may vary than the one in this example.","title":"Generate same strategy using vectorbtpro"},{"location":"explore/generate-stock-schema-vbt/#strategy-to-generate-signal-for-unusual-volume-for-the-assets","text":"Here is an example of how to generate a signal for unusual volume using the vectorbt library: Python import vectorbt as vbt # Compute the average volume over a rolling window mean_volume = df [ 'volume' ] . rolling ( window = 10 ) . mean () # Compute the standard deviation of volume over a rolling window std_volume = df [ 'volume' ] . rolling ( window = 10 ) . std () # Create the signal unusual_volume_signal = df [ 'volume' ] > mean_volume + 2 * std_volume # Add the signal to the DataFrame df [ 'unusual_volume_signal' ] = unusual_volume_signal In this example, the df variable is a DataFrame containing the historical volume and other relevant data for the assets you are trading. The average volume is computed over a rolling window of 10 days using the rolling() method and the mean() function, and assigned to the mean_volume variable. Similarly, the standard deviation of volume is computed over a rolling window of 10 days using the rolling() method and the std() function, and assigned to the std_volume variable. The signal for unusual volume is generated by checking whether the volume is greater than the average volume plus 2 times the standard deviation of volume. The signal is added as a new column to the DataFrame and can be used to make trading decisions. Please note that this is a basic example and that you should adapt it to your specific use case, and that the window size and the threshold for unusual volume might vary depending on the assets and the market conditions. Also, the column names in your dataframe may vary than the one in this example.","title":"Strategy to generate signal for unusual volume for the assets"},{"location":"explore/generate-stock-schema-vbt/#backtest-both-strategies-using-vectorbt","text":"Here is an example of how to backtest both the Ichimoku strategy and the unusual volume strategy using the vectorbt library: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy ichimoku_strategy = vbt . Strategy ( indicator_df , entry_rule = lambda x : x . tenkan_sen_cross_kijun_sen , exit_rule = lambda x : x . chikou_span_cross_price ) # Backtest the Ichimoku strategy ichimoku_results_df = ichimoku_strategy . run ( df ) # Compute the average volume over a rolling window mean_volume = df [ 'volume' ] . rolling ( window = 10 ) . mean () # Compute the standard deviation of volume over a rolling window std_volume = df [ 'volume' ] . rolling ( window = 10 ) . std () # Create the unusual volume signal unusual_volume_signal = df [ 'volume' ] > mean_volume + 2 * std_volume # Create the unusual volume strategy unusual_volume_strategy = vbt . Strategy ( df , entry_signal = unusual_volume_signal ) # Backtest the unusual volume strategy unusual_volume_results_df = unusual_volume_strategy . run () In this example, the df variable is a DataFrame containing the historical high, low, and close prices for the stock or instrument you are trading. The Ichimoku indicators are calculated using the vbt.indicators.Ichimoku.run() method and assigned to the indicator_df variable. A vbt.Strategy object is then created using the indicator_df and the entry and exit rules defined as lambda functions. Then the run() method is called on the strategy which returns the DataFrame of the results. The signal for unusual volume is generated by checking whether the volume is greater than the average volume plus 2 times the standard deviation of volume. A vbt.Strategy object is then created using the DataFrame and the entry signal for unusual volume. Then the run() method is called on the strategy which returns the DataFrame of the results. You can then use the results DataFrames to analyze the performance of the strategies, such as the PnL, the number of trades, the win rate, the maximum drawdown, etc. Please note that this is a basic example and that you should adapt it to your specific use case, and that the window size and the threshold for unusual volume might vary depending on the assets and the market conditions. Also, the column names in your dataframe may vary than the one in this example.","title":"Backtest both strategies using vectorbt"},{"location":"explore/generate-stock-schema-vbt/#generate-alerts-for-signals","text":"Once you have generated the signals, you can use the vectorbt library to create alerts based on them. Here is an example of how to create alerts for the Ichimoku strategy and the unusual volume strategy: Python import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy ichimoku_strategy = vbt . Strategy ( indicator_df , entry_rule = lambda x : x . tenkan_sen_cross_kijun_sen , exit_rule = lambda x : x . chikou_span_cross_price ) # Backtest the Ichimoku strategy ichimoku_results_df = ichimoku_strategy . run ( df ) # Create an alert for the Ichimoku strategy ichimoku_alert = ichimoku_strategy . alerts . for_signals ( 'entry' , send_alert = lambda x : print ( f 'Ichimoku strategy: Buy signal at { x . name } ' ) ) # Compute the average volume over a rolling window mean_volume = df [ 'volume' ] . rolling ( window = 10 ) . mean () # Compute the standard deviation of volume over a rolling window std_volume = df [ 'volume' ] . rolling ( window = 10 ) . std () # Create the unusual volume signal unusual_volume_signal = df [ 'volume' ] > mean_volume + 2 * std_volume # Create the unusual volume strategy unusual_volume_strategy = vbt . Strategy ( df , entry_signal = unusual_volume_signal ) # Backtest the unusual volume strategy unusual_volume_results_df = unusual_volume_strategy . run () # Create an alert for the unusual volume strategy unusual_volume_alert = unusual_volume_strategy . alerts . for_signals ( 'entry' , send_alert = lambda x : print ( f 'Unusual volume strategy: Buy signal at { x . name } ' ) ) In this example, the vectorbt library is used to create alerts for each of the two strategies. The alerts.for_signals() method is called on the strategy object, which accepts the signal type ('entry' in this case) and a function send_alert that will be called when the signal is triggered. The function send_alert is passed a lambda function that prints a message to the console when the signal is triggered. This can be replaced with any other method of alerting, such as sending an email or a text message, or updating a database. Please note that this is a basic example and that you should adapt it to your specific use case and that the column names in your dataframe may vary than the one in this example.","title":"Generate alerts for signals"},{"location":"explore/generate-stock-schema-vbt/#optimize-performance-of-strategies-using-numba","text":"You can use the numba library to optimize the performance of your strategies built using vectorbt. numba is a just-in-time (JIT) compiler that can be used to speed up the execution of Python code. Here is an example of how to optimize the performance of the Ichimoku strategy using numba: Python import numba as nb import vectorbt as vbt # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy @nb . njit def ichimoku_strategy ( indicator_df ): entry_rule = indicator_df [ 'tenkan_sen_cross_kijun_sen' ] exit_rule = indicator_df [ 'chikou_span_cross_price' ] return entry_rule , exit_rule ichimoku_strategy = vbt . Strategy ( indicator_df , entry_rule = ichimoku_strategy ) # Backtest the Ichimoku strategy ichimoku_results_df = ichimoku_strategy . run ( df ) In this example, the numba.njit decorator is used to optimize the performance of the ichimoku_strategy function. This function contains the entry and exit rules for the Ichimoku strategy. The @nb .njit decorator tells numba to compile the function, which results in faster execution times. You can also use numba.jit decorator that will try to optimize the code, but it will not throw an error if it fails. You can use this approach to optimize any other part of your code that is computationally intensive. Please note that this is a basic example and that you should adapt it to your specific use case. Also, that numba optimization may not always be the best option, as it can have some overhead and the performance gains may be minimal for small data sets.","title":"Optimize performance of strategies using numba"},{"location":"explore/generate-stock-schema-vbt/#create-strategies-and-alerts-in-pyspark","text":"Here is an example of how to create the Ichimoku strategy and the unusual volume strategy, as well as the associated alerts, using PySpark: Python from pyspark.sql import SparkSession from pyspark.sql.functions import col , when , lead # Create a SparkSession spark = SparkSession . builder . appName ( \"Ichimoku_Unusual_Volume\" ) . getOrCreate () # Read the data into a DataFrame df = spark . read . csv ( \"path_to_data.csv\" , header = True , inferSchema = True ) # Create the Ichimoku indicators indicator_df = vbt . indicators . Ichimoku . run ( df , high_col = 'high' , low_col = 'low' , close_col = 'close' ) # Create the Ichimoku strategy ichimoku_strategy = indicator_df . withColumn ( \"buy_signal\" , when (( col ( \"tenkan_sen\" ) > col ( \"kijun_sen\" )) & ( col ( \"close\" ) > col ( \"senkou_span_a\" )), 1 ) . otherwise ( 0 )) # Create the alert for the Ichimoku strategy ichimoku_alert = ichimoku_strategy . filter ( col ( \"buy_signal\" ) == 1 ) . select ( \"timestamp\" , \"stock\" ) . show () # Compute the average volume over a rolling window mean_volume = df . select ( avg ( \"volume\" ) . over ( Window . orderBy ( \"timestamp\" ) . rowsBetween ( - 10 , 0 ))) . alias ( \"mean_volume\" ) df = df . join ( mean_volume , \"timestamp\" ) # Compute the standard deviation of volume over a rolling window std_volume = df . select ( stddev ( \"volume\" ) . over ( Window . orderBy ( \"timestamp\" ) . rowsBetween ( - 10 , 0 ))) . alias ( \"std_volume\" ) df = df . join ( std_volume , \"timestamp\" ) # Create the unusual volume strategy unusual_volume_strategy = df . withColumn ( \"buy_signal\" , when (( col ( \"volume\" ) > col ( \"mean_volume\" ) + 2 * col ( \"std_volume\" )), 1 ) . otherwise ( 0 )) # Create the alert for the unusual volume strategy unusual_volume_alert = unusual_volume_strategy . filter ( col ( \"buy_signal\" ) == 1 ) . select ( \"timestamp\" , \"stock\" ) . show () In this example, the data is read into a DataFrame using the spark.read.csv() method. The Ichimoku indicators are calculated using the `vbt.indicators","title":"Create strategies and alerts in pyspark"},{"location":"explore/signal_processing/","text":"Stock Data Analysis using Signal Processing \u00b6 Signal processing techniques can be applied to time series data, including stock data. Time series data refers to a series of data points collected over time, such as daily stock prices, monthly sales figures, or hourly temperature readings. In the case of stock data, signal processing techniques can be used to extract meaningful information and make predictions about future trends. For example, techniques such as smoothing, filtering, and decomposition can be used to remove noise and identify underlying patterns in the data. Another example is the use of technical analysis, which is a method of evaluating securities by analyzing statistics generated by market activity, such as past prices and volume. Technical analysis makes use of various signal processing techniques, such as moving averages and trend analysis, to identify patterns and make predictions about future price movements. There are many other ways in which signal processing can be applied to stock data, and the specific techniques used will depend on the goals of the analysis and the characteristics of the data itself. Overall, the goal of applying signal processing to stock data is to improve the accuracy of predictions and make more informed investment decisions. An example of how you might use PySpark to apply smoothing, filtering, and decomposition techniques to stock data stored in a CSV file: \u00b6 Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd # Start a Spark session spark = SparkSession . builder . appName ( \"SignalProcessingExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Apply smoothing using a rolling average data_pd [ 'rolling_avg' ] = data_pd [ 'close' ] . rolling ( window = 10 ) . mean () # Apply filtering using a low-pass filter def low_pass_filter ( signal , alpha ): filtered_signal = [ signal [ 0 ]] for i in range ( 1 , len ( signal )): filtered_signal . append ( alpha * signal [ i ] + ( 1 - alpha ) * filtered_signal [ i - 1 ]) return filtered_signal data_pd [ 'filtered' ] = low_pass_filter ( data_pd [ 'close' ], 0.1 ) # Apply decomposition using a Fourier transform def fourier_transform ( signal ): n = len ( signal ) fft = np . fft . fft ( signal ) return np . abs ( fft [: n // 2 ]) / ( n / 2 ) data_pd [ 'spectrum' ] = fourier_transform ( data_pd [ 'close' ]) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"processed_ticker_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file into a Spark DataFrame, which is then converted to a Pandas DataFrame for easier processing. Smoothing is performed using a rolling average, filtering is performed using a low-pass filter, and decomposition is performed using a Fourier transform. The resulting processed data is then written back to a CSV file. Detecting patterns \u00b6 Once you have applied smoothing, filtering, and decomposition techniques to the stock data, the next step is to detect patterns in the data. There are several methods you can use for this, including: Trend analysis: This involves identifying trends in the data, such as upward or downward movements over time, and using these trends to make predictions about future behavior. Statistical analysis: This involves calculating various statistical measures, such as mean, median, and standard deviation, and using these measures to identify patterns and make predictions. Machine learning: This involves using algorithms such as regression analysis, decision trees, or neural networks to train a model on the data and make predictions. Pattern recognition: This involves identifying recurring patterns in the data, such as peaks, valleys, or repeating sequences, and using these patterns to make predictions. The specific method you use will depend on the goals of your analysis and the characteristics of the data itself. For example, if you want to identify long-term trends, trend analysis may be the best approach, whereas if you want to identify short-term patterns, pattern recognition may be more appropriate. Once you have identified patterns in the data, you can use this information to make predictions about future behavior. For example, you might use the trend in the data to make predictions about future stock prices, or use pattern recognition to identify periods of high volatility. Ultimately, the goal is to make more informed investment decisions based on the information contained in the stock data. 1. Trend analysis \u00b6 Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd # Start a Spark session spark = SparkSession . builder . appName ( \"TrendAnalysisExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Calculate the daily returns data_pd [ 'returns' ] = data_pd [ 'close' ] . pct_change () # Calculate the moving average of the returns data_pd [ 'returns_avg' ] = data_pd [ 'returns' ] . rolling ( window = 10 ) . mean () # Identify the trend by comparing the current return to the average return data_pd [ 'trend' ] = np . where ( data_pd [ 'returns' ] > data_pd [ 'returns_avg' ], 'Up' , 'Down' ) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"trend_analysis_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The daily returns are calculated, and the moving average of the returns is calculated over a 10-day window. The trend is identified by comparing the current return to the average return, and a new column is added to the data to indicate whether the trend is up or down. The resulting processed data is then written back to a CSV file. This is just one example of how you might perform trend analysis on stock data, and there are many other methods you could use to identify trends in the data. The specific approach you take will depend on the goals of your analysis and the characteristics of the data itself. Using Signal Processing Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd from scipy import signal # Start a Spark session spark = SparkSession . builder . appName ( \"TrendAnalysisExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Apply smoothing to the close price data using a Gaussian filter close_price = data_pd [ 'close' ] . values close_price_smooth = signal . convolve ( close_price , signal . windows . gaussian ( 10 ), mode = 'same' ) / sum ( signal . windows . gaussian ( 10 )) # Calculate the daily returns data_pd [ 'returns' ] = ( close_price - close_price_smooth ) / close_price_smooth # Calculate the moving average of the returns data_pd [ 'returns_avg' ] = data_pd [ 'returns' ] . rolling ( window = 10 ) . mean () # Identify the trend by comparing the current return to the average return data_pd [ 'trend' ] = np . where ( data_pd [ 'returns' ] > data_pd [ 'returns_avg' ], 'Up' , 'Down' ) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"trend_analysis_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The close price data is smoothed using a Gaussian filter to remove noise, and the daily returns are calculated based on the smoothed close price data. The moving average of the returns is calculated over a 10-day window, and the trend is identified by comparing the current return to the average return. A new column is added to the data to indicate whether the trend is up or down. The resulting processed data is then written back to a CSV file. 4. Pattern recognition \u00b6 Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd from scipy import signal # Start a Spark session spark = SparkSession . builder . appName ( \"PatternRecognitionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Apply smoothing to the close price data using a Gaussian filter close_price = data_pd [ 'close' ] . values close_price_smooth = signal . convolve ( close_price , signal . windows . gaussian ( 10 ), mode = 'same' ) / sum ( signal . windows . gaussian ( 10 )) # Apply the Fast Fourier Transform (FFT) to the smoothed close price data close_price_smooth_fft = np . abs ( np . fft . fft ( close_price_smooth )) # Identify the frequency components with the highest power peak_frequencies = np . argsort ( close_price_smooth_fft )[ - 5 :] # Add a new column to the data to indicate the dominant frequency component data_pd [ 'dominant_frequency' ] = peak_frequencies [ 0 ] # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"pattern_recognition_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The close price data is smoothed using a Gaussian filter to remove noise, and the Fast Fourier Transform (FFT) is applied to the smoothed close price data to identify the frequency components with the highest power. The dominant frequency component is identified, and a new column is added to the data to indicate this information. The resulting processed data is then written back to a CSV file. This is just one example of how you might perform pattern recognition on stock data using signal processing techniques, and there are many other methods you could use to identify patterns in the data. The specific approach you take will depend on the goals of your analysis and the characteristics of the data itself. Peak detection: Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from scipy.signal import argrelextrema # Start a Spark session spark = SparkSession . builder . appName ( \"PeakDetectionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Find the indices of the peaks in the close price data close_price = data_pd [ 'close' ] . values peak_indices = argrelextrema ( close_price , np . greater ) # Add a new column to the data to indicate whether each data point is a peak data_pd [ 'peak' ] = 0 data_pd . loc [ peak_indices , 'peak' ] = 1 # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"peak_detection_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The close price data is analyzed to find the indices of the peaks, and a new column is added to the data to indicate whether each data point is a peak. The resulting processed data is then written back to a CSV file. Valley detection: Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from scipy.signal import argrelextrema # Start a Spark session spark = SparkSession . builder . appName ( \"ValleyDetectionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Find the indices of the valleys in the close price data close_price = data_pd [ 'close' ] . values valley_indices = argrelextrema ( close_price , np . less ) # Add a new column to the data to indicate whether each data point is a valley data_pd [ 'valley' ] = 0 data_pd . loc [ valley_indices , 'valley' ] = 1 # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"valley_detection_data.csv\" , header = True ) # Stop the Spark session spark . stop () Repeating sequences: Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from scipy.signal import find_peaks # Start a Spark session spark = SparkSession . builder . appName ( \"RepeatingSequenceExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Find the repeating sequences in the close price data close_price = data_pd [ 'close' ] . values peaks , _ = find_peaks ( close_price ) # Calculate the difference between each peak and the next peak peak_differences = np . diff ( peaks ) # Find the repeating sequence by calculating the mode of the peak differences repeating_sequence = int ( peak_differences . mode ()) # Add a new column to the data to indicate the repeating sequence data_pd [ 'repeating_sequence' ] = repeating_sequence # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"repeating_sequence_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The repeating sequence is found by finding the peaks in the close price data and calculating the mode of the differences between the peaks. The repeating sequence is then added as a new column to the data and written back to a CSV file. There are several other patterns that can be detected from stock data using signal processing techniques, including: Trend analysis: This involves detecting the overall direction of the stock's price over a period of time. This can be done using techniques such as moving averages, exponential smoothing, and linear regression. Cycles and seasonality: This involves identifying repeating patterns in the stock's price over a certain period of time, such as weekly or yearly cycles. This can be done using Fourier analysis or wavelet analysis. Support and resistance levels: This involves identifying price levels at which the stock's price has previously stopped falling and started rising, or vice versa. This can be done using techniques such as trend lines and Fibonacci retracements. Volatility: This involves measuring the amount of variation in the stock's price over a certain period of time. This can be done using techniques such as Bollinger Bands and average true range. Market patterns: This involves identifying patterns in the stock market as a whole, such as bull and bear markets. This can be done using techniques such as moving averages and trend lines on market indices. By detecting these patterns in stock data, traders and investors can make more informed investment decisions and develop better trading strategies. Support and resistance levels Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np # Start a Spark session spark = SparkSession . builder . appName ( \"SupportResistanceExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Define the moving average window size window_size = 20 # Calculate the moving average of the close price data_pd [ 'moving_average' ] = data_pd [ 'close' ] . rolling ( window = window_size ) . mean () # Identify the support and resistance levels data_pd [ 'support' ] = np . where ( data_pd [ 'close' ] >= data_pd [ 'moving_average' ], 1 , 0 ) data_pd [ 'resistance' ] = np . where ( data_pd [ 'close' ] < data_pd [ 'moving_average' ], 1 , 0 ) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"support_resistance_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The moving average of the close price is calculated over a window of 20 days. The support and resistance levels are then identified by comparing the close price to the moving average and adding columns to the data indicating whether the close price is above or below the moving average. The processed data is then written back to a CSV file. Use Accumulator to apply all pattern detection at once Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from pyspark.accumulators import AccumulatorParam # Define an accumulator class for pattern detection class PatternDetectionAccumulator ( AccumulatorParam ): # Initialize the accumulator def zero ( self , initialValue ): return { 'close_price' : [], 'moving_average' : [], 'support' : [], 'resistance' : [], 'volatility' : []} # Accumulate the data def addInPlace ( self , v1 , v2 ): v1 [ 'close_price' ] . extend ( v2 [ 'close_price' ]) v1 [ 'moving_average' ] . extend ( v2 [ 'moving_average' ]) v1 [ 'support' ] . extend ( v2 [ 'support' ]) v1 [ 'resistance' ] . extend ( v2 [ 'resistance' ]) v1 [ 'volatility' ] . extend ( v2 [ 'volatility' ]) return v1 # Start a Spark session spark = SparkSession . builder . appName ( \"PatternDetectionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Initialize the accumulator pattern_detection_accumulator = spark . sparkContext . accumulator ({}, PatternDetectionAccumulator ()) # Define the moving average window size window_size = 20 # Define the volatility window size volatility_window_size = 20 # Calculate the moving average of the close price and detect the support and resistance levels def calculate_moving_average_and_support_resistance ( row ): global pattern_detection_accumulator , window_size close_price = row [ 'close' ] moving_average = np . average ( pattern_detection_accumulator . value [ 'close_price' ][ - window_size :]) support = int ( close_price >= moving_average ) resistance = int ( close_price < moving_average ) # Accumulate the data pattern_detection_accumulator += { 'close_price' : [ close_price ], 'moving_average' : [ moving_average ], 'support' : [ support ], 'resistance' : [ resistance ]} return row # Calculate the volatility of the close price def calculate_volatility ( row ): global pattern_detection_accumulator , volatility_window_size close_price = row [ 'close' ] volatility = np . std ( pattern_detection_accumulator . value [ 'close_price' ][ - volatility_window_size :]) # Accumulate the data pattern_detection_accumulator += { 'volatility' : [ volatility ]} return row # Apply the calculation functions to each row of the data data = data . rdd . map ( calculate_moving_average_and_support_resistance ) . map ( calculate_volatility ) . toDF () # Convert the Spark DataFrame to a Pandas pd_data = data . toPandas () # Add the accumulated data to the Pandas DataFrame pd_data [ 'moving_average' ] = pattern_detection_accumulator . value [ 'moving_average' ] pd_data [ 'support' ] = pattern_detection_accumulator . value [ 'support' ] pd_data [ 'resistance' ] = pattern_detection_accumulator . value [ 'resistance' ] pd_data [ 'volatility' ] = pattern_detection_accumulator . value [ 'volatility' ] # Plot the data to visualize the results import matplotlib.pyplot as plt plt . plot ( pd_data [ 'date' ], pd_data [ 'close' ], label = 'Close Price' ) plt . plot ( pd_data [ 'date' ], pd_data [ 'moving_average' ], label = 'Moving Average' ) plt . scatter ( pd_data [ pd_data [ 'support' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'support' ] == 1 ][ 'close' ], label = 'Support' ) plt . scatter ( pd_data [ pd_data [ 'resistance' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'resistance' ] == 1 ][ 'close' ], label = 'Resistance' ) plt . legend () plt . show () # Plot the volatility of the close price plt . plot ( pd_data [ 'date' ], pd_data [ 'volatility' ], label = 'Volatility' ) plt . legend () plt . show () # Stop the Spark session spark . stop () For multiple symbols: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize the Spark context and session spark = SparkSession . builder . appName ( \"SignalProcessingStockData\" ) . getOrCreate () # Define a list of symbols to process symbols = [ 'AAPL' , 'GOOG' , 'MSFT' , 'IBM' ] # Loop over each symbol for symbol in symbols : # Load the stock data for the symbol data = spark . read . csv ( \"data/ {} _data.csv\" . format ( symbol ), header = True , inferSchema = True ) # Perform pattern detection on the data for this symbol pd_data = data . toPandas () pd_data [ 'moving_average' ] = calculate_moving_average ( pd_data [ 'close' ]) pd_data [ 'support' ], pd_data [ 'resistance' ] = calculate_support_resistance ( pd_data [ 'close' ], pd_data [ 'moving_average' ]) pd_data [ 'volatility' ] = calculate_volatility ( pd_data [ 'close' ]) # Plot the results for this symbol import matplotlib.pyplot as plt plt . plot ( pd_data [ 'date' ], pd_data [ 'close' ], label = 'Close Price' ) plt . plot ( pd_data [ 'date' ], pd_data [ 'moving_average' ], label = 'Moving Average' ) plt . scatter ( pd_data [ pd_data [ 'support' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'support' ] == 1 ][ 'close' ], label = 'Support' ) plt . scatter ( pd_data [ pd_data [ 'resistance' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'resistance' ] == 1 ][ 'close' ], label = 'Resistance' ) plt . legend () plt . title ( symbol ) plt . show () # Plot the volatility of the close price plt . plot ( pd_data [ 'date' ], pd_data [ 'volatility' ], label = 'Volatility' ) plt . legend () plt . title ( symbol ) plt . show () # Stop the Spark session spark . stop () This code loops over a list of symbols and performs pattern detection on each symbol's data. The code uses the calculate_moving_average, calculate_support_resistance, and calculate_volatility functions to perform the pattern detection on the data. The results for each symbol are visualized using Matplotlib. Tips to optimize the performance \u00b6 There is no single best way to perform pattern detection using signal processing in PySpark, as the performance will depend on the specifics of your data and the patterns you are trying to detect. However, there are some general tips you can follow to optimize performance: Parallelize the processing: Use PySpark's built-in support for parallel processing to perform the signal processing on multiple symbols in parallel. This will allow you to take advantage of multiple cores and distribute the processing load across multiple nodes in a cluster, leading to faster performance. Optimize the algorithms: Optimize the signal processing algorithms used to perform the pattern detection. This may involve tuning the parameters of the algorithms, or re-writing the algorithms to take advantage of Spark's parallel processing capabilities. Use an accumulator: An accumulator is a variable in Spark that can be used to accumulate values across multiple nodes in a cluster. By using an accumulator, you can avoid the overhead of sending intermediate results between nodes, leading to faster performance. Use caching: Use Spark's caching functionality to persist intermediate results in memory, so they do not need to be recalculated each time they are needed. Use broadcast variables: Broadcast variables are read-only variables in Spark that are stored on each node, rather than being sent over the network. By using broadcast variables, you can avoid the overhead of sending data between nodes, leading to faster performance. Avoid shuffling data unnecessarily: Shuffling data across nodes is a time-consuming operation in Spark, so it is best to avoid shuffling data whenever possible. If shuffling is necessary, try to minimize the amount of data that is shuffled, or use Spark's optimizations for shuffling, such as the use of bucketing or partitioning. 1. Parallelize the processing Here's an example to demonstrate how the processing can be parallelized in PySpark. Let's say we have a CSV file containing data for multiple symbols. We want to perform some signal processing on each symbol in parallel. Here's how we can do it: Python from pyspark import SparkContext from pyspark.sql import SparkSession # Initialize the Spark context and Spark session sc = SparkContext ( \"local\" , \"signal processing example\" ) spark = SparkSession . builder . appName ( \"SignalProcessing\" ) . getOrCreate () # Load the data from the CSV file df = spark . read . csv ( \"symbol_data.csv\" , header = True , inferSchema = True ) # Create a list of symbols to process in parallel symbols = df . select ( \"symbol\" ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () # Define the signal processing function def process_symbol ( symbol ): # Filter the data for the current symbol symbol_data = df . filter ( df [ \"symbol\" ] == symbol ) # Perform the signal processing on the symbol data # ... return processed_data # Parallelize the processing of each symbol rdd = sc . parallelize ( symbols ) processed_data = rdd . map ( process_symbol ) # Collect the processed data on the driver node result = processed_data . collect () In this example, we first create a list of symbols to process. We then define a signal processing function process_symbol that takes a symbol as input and performs the signal processing on the corresponding data. Finally, we use the parallelize method of the Spark context to create an RDD containing the symbols, and use the map method to apply the signal processing function to each symbol in parallel. The result is collected on the driver node and can be further processed or analyzed. 2. Optimize the algorithms To optimize the algorithms for faster performance, you can use various techniques, including vectorization, parallel processing, and algorithmic optimization. Here's an example in PySpark to demonstrate the optimization of algorithms for signal processing: Python from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import udf from pyspark.sql.types import DoubleType import numpy as np # Initialize the Spark context and Spark session sc = SparkContext ( \"local\" , \"signal processing example\" ) spark = SparkSession . builder . appName ( \"SignalProcessing\" ) . getOrCreate () # Load the data from the CSV file df = spark . read . csv ( \"symbol_data.csv\" , header = True , inferSchema = True ) # Define a user-defined function to perform smoothing def smoothing ( values ): # Use NumPy's smoothing function to perform the smoothing smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Register the smoothing UDF smoothing_udf = udf ( smoothing , ArrayType ( DoubleType ())) # Apply the smoothing UDF to the data smoothed_data = df . withColumn ( \"smoothed\" , smoothing_udf ( df [ \"values\" ])) # Collect the smoothed data on the driver node result = smoothed_data . collect () In this example, we use a user-defined function (UDF) to perform smoothing on the data. The UDF takes a list of values as input and returns a smoothed version of the data using NumPy's convolve function. By using a UDF, we can take advantage of the optimized algorithms in NumPy and perform the smoothing on large arrays efficiently. To further optimize the performance, you can consider the following suggestions: Use optimized libraries like NumPy or SciPy for signal processing algorithms. Python import numpy as np def smoothing ( values ): smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Register the smoothing UDF smoothing_udf = udf ( smoothing , ArrayType ( DoubleType ())) # Apply the smoothing UDF to the data smoothed_data = df . withColumn ( \"smoothed\" , smoothing_udf ( df [ \"values\" ])) In this example, we use the convolve function from NumPy to perform the smoothing operation on the data. By using this optimized library, we can take advantage of the optimized algorithms for performing signal processing operations. Use vectorized operations instead of loop-based operations. Python import numpy as np def smoothing ( values ): smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Convert the data into a NumPy array values = df . select ( \"values\" ) . toPandas () . values # Use NumPy's vectorized operations to perform the smoothing smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) In this example, we use vectorized operations from NumPy to perform the smoothing operation on the data. By using vectorized operations, we can perform the calculation on large arrays efficiently and reduce the time it takes to process the data. Use parallel processing wherever possible to speed up the processing of large datasets. Python from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import udf from pyspark.sql.types import DoubleType import numpy as np # Initialize the Spark context and Spark session sc = SparkContext ( \"local\" , \"signal processing example\" ) spark = SparkSession . builder . appName ( \"SignalProcessing\" ) . getOrCreate () # Load the data from the CSV file df = spark . read . csv ( \"symbol_data.csv\" , header = True , inferSchema = True ) # Define a user-defined function to perform smoothing def smoothing ( values ): # Use NumPy's smoothing function to perform the smoothing smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Register the smoothing UDF smoothing_udf = udf ( smoothing , ArrayType ( DoubleType ())) # Apply the smoothing UDF in parallel across the data smoothed_data = df . repartition ( 10 ) . withColumn ( \"smoothed\" , smoothing_udf ( df [ \"values\" ])) In this example, we use Spark's support for parallel processing to perform the smoothing operation on the data in parallel. By using Spark, we can take advantage of its distributed computing framework to process the data in parallel and reduce the time it takes to perform the calculation. Use algorithms with lower complexity, such as the Fast Fourier Transform (FFT), to perform computationally intensive operations. Python import numpy as np def smoothing ( values ): smoothed = np . fft . irfft ( np . fft . rfft ( values ) It's also important to note that the optimal approach will depend on the specific requirements of your use case and the characteristics of your data. You may need to experiment with different algorithms and approaches to find the best solution for your specific needs. 3. Use an accumulator: In Apache Spark, an accumulator is a write-only variable that can be used to accumulate values from multiple tasks into a single final result. Accumulators are used to efficiently implement algorithms that need to aggregate information from many tasks into a single result. Here's an example of using an accumulator in PySpark to detect support and resistance levels in stock data: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize Spark conf = SparkConf () . setAppName ( \"StockPatternDetection\" ) sc = SparkContext ( conf = conf ) spark = SparkSession ( sc ) # Read in stock data as a DataFrame df = spark . read . csv ( \"stock_data.csv\" , header = True ) # Create an accumulator to store the support and resistance levels support_and_resistance_accumulator = sc . accumulator ([], \"Support and Resistance Accumulator\" ) # Define a function that will update the accumulator with the support and resistance levels def update_accumulator ( record ): # Perform signal processing to detect support and resistance levels support , resistance = detect_support_and_resistance ( record ) # Update the accumulator with the support and resistance levels support_and_resistance_accumulator . add (( support , resistance )) # Use the .foreach() function to process each record in the DataFrame and update the accumulator df . rdd . foreach ( update_accumulator ) # Get the final result of the accumulator support_and_resistance = support_and_resistance_accumulator . value # Perform additional processing on the support and resistance levels (if needed) ... # Stop Spark sc . stop () In this example, the detect_support_and_resistance function would perform the signal processing to detect the support and resistance levels for a given record. The update_accumulator function would then update the support_and_resistance_accumulator with the results of the signal processing. The df.rdd.foreach(update_accumulator) line would then process each record in the DataFrame and call the update_accumulator function to update the accumulator. Finally, the final result of the accumulator could be obtained with support_and_resistance_accumulator.value. 4. Use Spark's caching Caching is a powerful feature in Apache Spark that allows you to persist intermediate data in memory across multiple stages of a pipeline. This can greatly improve the performance of your Spark application, especially when you are processing large datasets and need to reuse the same data multiple times. Here's an example of using Spark's caching in PySpark to detect support and resistance levels in stock data: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize Spark conf = SparkConf () . setAppName ( \"StockPatternDetection\" ) sc = SparkContext ( conf = conf ) spark = SparkSession ( sc ) # Read in stock data as a DataFrame df = spark . read . csv ( \"stock_data.csv\" , header = True ) # Cache the DataFrame in memory for faster access df . persist ( StorageLevel . MEMORY_ONLY ) # Create an accumulator to store the support and resistance levels support_and_resistance_accumulator = sc . accumulator ([], \"Support and Resistance Accumulator\" ) # Define a function that will update the accumulator with the support and resistance levels def update_accumulator ( record ): # Perform signal processing to detect support and resistance levels support , resistance = detect_support_and_resistance ( record ) # Update the accumulator with the support and resistance levels support_and_resistance_accumulator . add (( support , resistance )) # Use the .foreach() function to process each record in the DataFrame and update the accumulator df . rdd . foreach ( update_accumulator ) # Get the final result of the accumulator support_and_resistance = support_and_resistance_accumulator . value # Perform additional processing on the support and resistance levels (if needed) ... # Unpersist the cached DataFrame df . unpersist () # Stop Spark sc . stop () In this example, the df.persist(StorageLevel.MEMORY_ONLY) line caches the DataFrame in memory for faster access. The df.rdd.foreach(update_accumulator) line would then process each record in the DataFrame and call the update_accumulator function to update the accumulator. Finally, the cached DataFrame can be unpersisted with df.unpersist() to free up memory when it is no longer needed. 5. Use broadcast variable Broadcast variables are read-only variables in Apache Spark that are cached on each worker node rather than being sent with each task. Using broadcast variables can greatly improve the performance of your Spark application, especially when you are using the same large data structures or lookup tables in multiple stages of a pipeline. Here's an example of using a broadcast variable in PySpark to detect support and resistance levels in stock data: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize Spark conf = SparkConf () . setAppName ( \"StockPatternDetection\" ) sc = SparkContext ( conf = conf ) spark = SparkSession ( sc ) # Read in stock data as a DataFrame df = spark . read . csv ( \"stock_data.csv\" , header = True ) # Cache the DataFrame in memory for faster access df . persist ( StorageLevel . MEMORY_ONLY ) # Create a broadcast variable to store a lookup table of stock symbols stock_symbols_broadcast = sc . broadcast ([ \"AAPL\" , \"GOOG\" , \"MSFT\" , \"AMZN\" , \"FB\" ]) # Create an accumulator to store the support and resistance levels support_and_resistance_accumulator = sc . accumulator ([], \"Support and Resistance Accumulator\" ) # Define a function that will update the accumulator with the support and resistance levels def update_accumulator ( record ): # Check if the stock symbol is in the list of stock symbols stock_symbol = record [ 0 ] if stock_symbol in stock_symbols_broadcast . value : # Perform signal processing to detect support and resistance levels support , resistance = detect_support_and_resistance ( record ) # Update the accumulator with the support and resistance levels support_and_resistance_accumulator . add (( support , resistance )) # Use the .foreach() function to process each record in the DataFrame and update the accumulator df . rdd . foreach ( update_accumulator ) # Get the final result of the accumulator support_and_resistance = support_and_resistance_accumulator . value # Perform additional processing on the support and resistance levels (if needed) ... # Unpersist the cached DataFrame df . unpersist () # Stop Spark sc . stop () In this example, the stock_symbols_broadcast broadcast variable is used to store a list of stock symbols. The df.rdd.foreach(update_accumulator) line would then process each record in the DataFrame and call the update_accumulator function to update the accumulator. The stock_symbols_broadcast.value line would then check if the stock symbol is in the list of stock symbols. By using a broadcast variable, we can avoid the overhead of sending the lookup table with each task and instead cache it on each worker node. 6. Avoid shuffling data unnecessarily Python from pyspark import SparkContext from pyspark.sql import SparkSession # Initialize SparkContext and SparkSession sc = SparkContext ( \"local\" , \"avoid shuffling data unnecessarily example\" ) spark = SparkSession . builder . appName ( \"avoid shuffling data unnecessarily example\" ) . getOrCreate () # Load data into a DataFrame df = spark . read . csv ( \"path/to/data.csv\" , header = True ) # Repartition the data for efficient processing df = df . repartition ( 100 ) # Do any necessary preprocessing and transformations on the data df = df . filter ( df [ \"column_name\" ] > some_threshold ) df = df . groupBy ( \"column_name\" ) . agg ({ \"column_name\" : \"mean\" }) # Cache the data to avoid unnecessary shuffling df . cache () # Do any additional processing on the data result = df . count () # Unpersist the data after use df . unpersist () In this example, we first repartition the data into a smaller number of partitions to avoid shuffling data unnecessarily. Then, we cache the data in memory to avoid recomputing it each time it's used. Finally, we unpersist the data after use to free up memory resources. Note that it's important to carefully evaluate the trade-off between caching data and using limited memory resources. Caching too much data can cause out-of-memory errors, so it's a good idea to monitor memory usage and adjust the caching strategy as needed.","title":"Stock Data Analysis using Signal Processing"},{"location":"explore/signal_processing/#stock-data-analysis-using-signal-processing","text":"Signal processing techniques can be applied to time series data, including stock data. Time series data refers to a series of data points collected over time, such as daily stock prices, monthly sales figures, or hourly temperature readings. In the case of stock data, signal processing techniques can be used to extract meaningful information and make predictions about future trends. For example, techniques such as smoothing, filtering, and decomposition can be used to remove noise and identify underlying patterns in the data. Another example is the use of technical analysis, which is a method of evaluating securities by analyzing statistics generated by market activity, such as past prices and volume. Technical analysis makes use of various signal processing techniques, such as moving averages and trend analysis, to identify patterns and make predictions about future price movements. There are many other ways in which signal processing can be applied to stock data, and the specific techniques used will depend on the goals of the analysis and the characteristics of the data itself. Overall, the goal of applying signal processing to stock data is to improve the accuracy of predictions and make more informed investment decisions.","title":"Stock Data Analysis using Signal Processing"},{"location":"explore/signal_processing/#an-example-of-how-you-might-use-pyspark-to-apply-smoothing-filtering-and-decomposition-techniques-to-stock-data-stored-in-a-csv-file","text":"Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd # Start a Spark session spark = SparkSession . builder . appName ( \"SignalProcessingExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Apply smoothing using a rolling average data_pd [ 'rolling_avg' ] = data_pd [ 'close' ] . rolling ( window = 10 ) . mean () # Apply filtering using a low-pass filter def low_pass_filter ( signal , alpha ): filtered_signal = [ signal [ 0 ]] for i in range ( 1 , len ( signal )): filtered_signal . append ( alpha * signal [ i ] + ( 1 - alpha ) * filtered_signal [ i - 1 ]) return filtered_signal data_pd [ 'filtered' ] = low_pass_filter ( data_pd [ 'close' ], 0.1 ) # Apply decomposition using a Fourier transform def fourier_transform ( signal ): n = len ( signal ) fft = np . fft . fft ( signal ) return np . abs ( fft [: n // 2 ]) / ( n / 2 ) data_pd [ 'spectrum' ] = fourier_transform ( data_pd [ 'close' ]) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"processed_ticker_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file into a Spark DataFrame, which is then converted to a Pandas DataFrame for easier processing. Smoothing is performed using a rolling average, filtering is performed using a low-pass filter, and decomposition is performed using a Fourier transform. The resulting processed data is then written back to a CSV file.","title":"An example of how you might use PySpark to apply smoothing, filtering, and decomposition techniques to stock data stored in a CSV file:"},{"location":"explore/signal_processing/#detecting-patterns","text":"Once you have applied smoothing, filtering, and decomposition techniques to the stock data, the next step is to detect patterns in the data. There are several methods you can use for this, including: Trend analysis: This involves identifying trends in the data, such as upward or downward movements over time, and using these trends to make predictions about future behavior. Statistical analysis: This involves calculating various statistical measures, such as mean, median, and standard deviation, and using these measures to identify patterns and make predictions. Machine learning: This involves using algorithms such as regression analysis, decision trees, or neural networks to train a model on the data and make predictions. Pattern recognition: This involves identifying recurring patterns in the data, such as peaks, valleys, or repeating sequences, and using these patterns to make predictions. The specific method you use will depend on the goals of your analysis and the characteristics of the data itself. For example, if you want to identify long-term trends, trend analysis may be the best approach, whereas if you want to identify short-term patterns, pattern recognition may be more appropriate. Once you have identified patterns in the data, you can use this information to make predictions about future behavior. For example, you might use the trend in the data to make predictions about future stock prices, or use pattern recognition to identify periods of high volatility. Ultimately, the goal is to make more informed investment decisions based on the information contained in the stock data.","title":"Detecting patterns"},{"location":"explore/signal_processing/#1-trend-analysis","text":"Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd # Start a Spark session spark = SparkSession . builder . appName ( \"TrendAnalysisExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Calculate the daily returns data_pd [ 'returns' ] = data_pd [ 'close' ] . pct_change () # Calculate the moving average of the returns data_pd [ 'returns_avg' ] = data_pd [ 'returns' ] . rolling ( window = 10 ) . mean () # Identify the trend by comparing the current return to the average return data_pd [ 'trend' ] = np . where ( data_pd [ 'returns' ] > data_pd [ 'returns_avg' ], 'Up' , 'Down' ) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"trend_analysis_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The daily returns are calculated, and the moving average of the returns is calculated over a 10-day window. The trend is identified by comparing the current return to the average return, and a new column is added to the data to indicate whether the trend is up or down. The resulting processed data is then written back to a CSV file. This is just one example of how you might perform trend analysis on stock data, and there are many other methods you could use to identify trends in the data. The specific approach you take will depend on the goals of your analysis and the characteristics of the data itself. Using Signal Processing Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd from scipy import signal # Start a Spark session spark = SparkSession . builder . appName ( \"TrendAnalysisExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Apply smoothing to the close price data using a Gaussian filter close_price = data_pd [ 'close' ] . values close_price_smooth = signal . convolve ( close_price , signal . windows . gaussian ( 10 ), mode = 'same' ) / sum ( signal . windows . gaussian ( 10 )) # Calculate the daily returns data_pd [ 'returns' ] = ( close_price - close_price_smooth ) / close_price_smooth # Calculate the moving average of the returns data_pd [ 'returns_avg' ] = data_pd [ 'returns' ] . rolling ( window = 10 ) . mean () # Identify the trend by comparing the current return to the average return data_pd [ 'trend' ] = np . where ( data_pd [ 'returns' ] > data_pd [ 'returns_avg' ], 'Up' , 'Down' ) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"trend_analysis_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The close price data is smoothed using a Gaussian filter to remove noise, and the daily returns are calculated based on the smoothed close price data. The moving average of the returns is calculated over a 10-day window, and the trend is identified by comparing the current return to the average return. A new column is added to the data to indicate whether the trend is up or down. The resulting processed data is then written back to a CSV file.","title":"1. Trend analysis"},{"location":"explore/signal_processing/#4-pattern-recognition","text":"Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np import pandas as pd from scipy import signal # Start a Spark session spark = SparkSession . builder . appName ( \"PatternRecognitionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Apply smoothing to the close price data using a Gaussian filter close_price = data_pd [ 'close' ] . values close_price_smooth = signal . convolve ( close_price , signal . windows . gaussian ( 10 ), mode = 'same' ) / sum ( signal . windows . gaussian ( 10 )) # Apply the Fast Fourier Transform (FFT) to the smoothed close price data close_price_smooth_fft = np . abs ( np . fft . fft ( close_price_smooth )) # Identify the frequency components with the highest power peak_frequencies = np . argsort ( close_price_smooth_fft )[ - 5 :] # Add a new column to the data to indicate the dominant frequency component data_pd [ 'dominant_frequency' ] = peak_frequencies [ 0 ] # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"pattern_recognition_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The close price data is smoothed using a Gaussian filter to remove noise, and the Fast Fourier Transform (FFT) is applied to the smoothed close price data to identify the frequency components with the highest power. The dominant frequency component is identified, and a new column is added to the data to indicate this information. The resulting processed data is then written back to a CSV file. This is just one example of how you might perform pattern recognition on stock data using signal processing techniques, and there are many other methods you could use to identify patterns in the data. The specific approach you take will depend on the goals of your analysis and the characteristics of the data itself. Peak detection: Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from scipy.signal import argrelextrema # Start a Spark session spark = SparkSession . builder . appName ( \"PeakDetectionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Find the indices of the peaks in the close price data close_price = data_pd [ 'close' ] . values peak_indices = argrelextrema ( close_price , np . greater ) # Add a new column to the data to indicate whether each data point is a peak data_pd [ 'peak' ] = 0 data_pd . loc [ peak_indices , 'peak' ] = 1 # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"peak_detection_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The close price data is analyzed to find the indices of the peaks, and a new column is added to the data to indicate whether each data point is a peak. The resulting processed data is then written back to a CSV file. Valley detection: Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from scipy.signal import argrelextrema # Start a Spark session spark = SparkSession . builder . appName ( \"ValleyDetectionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Find the indices of the valleys in the close price data close_price = data_pd [ 'close' ] . values valley_indices = argrelextrema ( close_price , np . less ) # Add a new column to the data to indicate whether each data point is a valley data_pd [ 'valley' ] = 0 data_pd . loc [ valley_indices , 'valley' ] = 1 # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"valley_detection_data.csv\" , header = True ) # Stop the Spark session spark . stop () Repeating sequences: Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from scipy.signal import find_peaks # Start a Spark session spark = SparkSession . builder . appName ( \"RepeatingSequenceExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Find the repeating sequences in the close price data close_price = data_pd [ 'close' ] . values peaks , _ = find_peaks ( close_price ) # Calculate the difference between each peak and the next peak peak_differences = np . diff ( peaks ) # Find the repeating sequence by calculating the mode of the peak differences repeating_sequence = int ( peak_differences . mode ()) # Add a new column to the data to indicate the repeating sequence data_pd [ 'repeating_sequence' ] = repeating_sequence # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"repeating_sequence_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The repeating sequence is found by finding the peaks in the close price data and calculating the mode of the differences between the peaks. The repeating sequence is then added as a new column to the data and written back to a CSV file. There are several other patterns that can be detected from stock data using signal processing techniques, including: Trend analysis: This involves detecting the overall direction of the stock's price over a period of time. This can be done using techniques such as moving averages, exponential smoothing, and linear regression. Cycles and seasonality: This involves identifying repeating patterns in the stock's price over a certain period of time, such as weekly or yearly cycles. This can be done using Fourier analysis or wavelet analysis. Support and resistance levels: This involves identifying price levels at which the stock's price has previously stopped falling and started rising, or vice versa. This can be done using techniques such as trend lines and Fibonacci retracements. Volatility: This involves measuring the amount of variation in the stock's price over a certain period of time. This can be done using techniques such as Bollinger Bands and average true range. Market patterns: This involves identifying patterns in the stock market as a whole, such as bull and bear markets. This can be done using techniques such as moving averages and trend lines on market indices. By detecting these patterns in stock data, traders and investors can make more informed investment decisions and develop better trading strategies. Support and resistance levels Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np # Start a Spark session spark = SparkSession . builder . appName ( \"SupportResistanceExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Convert the Spark DataFrame to a Pandas DataFrame data_pd = data . toPandas () # Define the moving average window size window_size = 20 # Calculate the moving average of the close price data_pd [ 'moving_average' ] = data_pd [ 'close' ] . rolling ( window = window_size ) . mean () # Identify the support and resistance levels data_pd [ 'support' ] = np . where ( data_pd [ 'close' ] >= data_pd [ 'moving_average' ], 1 , 0 ) data_pd [ 'resistance' ] = np . where ( data_pd [ 'close' ] < data_pd [ 'moving_average' ], 1 , 0 ) # Convert the Pandas DataFrame back to a Spark DataFrame processed_data = spark . createDataFrame ( data_pd ) # Write the processed data back to a CSV file processed_data . write . csv ( \"support_resistance_data.csv\" , header = True ) # Stop the Spark session spark . stop () In this example, the stock data is loaded from a CSV file and converted to a Pandas DataFrame. The moving average of the close price is calculated over a window of 20 days. The support and resistance levels are then identified by comparing the close price to the moving average and adding columns to the data indicating whether the close price is above or below the moving average. The processed data is then written back to a CSV file. Use Accumulator to apply all pattern detection at once Python from pyspark.sql import SparkSession from pyspark.sql.functions import * import numpy as np from pyspark.accumulators import AccumulatorParam # Define an accumulator class for pattern detection class PatternDetectionAccumulator ( AccumulatorParam ): # Initialize the accumulator def zero ( self , initialValue ): return { 'close_price' : [], 'moving_average' : [], 'support' : [], 'resistance' : [], 'volatility' : []} # Accumulate the data def addInPlace ( self , v1 , v2 ): v1 [ 'close_price' ] . extend ( v2 [ 'close_price' ]) v1 [ 'moving_average' ] . extend ( v2 [ 'moving_average' ]) v1 [ 'support' ] . extend ( v2 [ 'support' ]) v1 [ 'resistance' ] . extend ( v2 [ 'resistance' ]) v1 [ 'volatility' ] . extend ( v2 [ 'volatility' ]) return v1 # Start a Spark session spark = SparkSession . builder . appName ( \"PatternDetectionExample\" ) . getOrCreate () # Load the data from a CSV file data = spark . read . csv ( \"processed_ticker_data.csv\" , header = True , inferSchema = True ) # Initialize the accumulator pattern_detection_accumulator = spark . sparkContext . accumulator ({}, PatternDetectionAccumulator ()) # Define the moving average window size window_size = 20 # Define the volatility window size volatility_window_size = 20 # Calculate the moving average of the close price and detect the support and resistance levels def calculate_moving_average_and_support_resistance ( row ): global pattern_detection_accumulator , window_size close_price = row [ 'close' ] moving_average = np . average ( pattern_detection_accumulator . value [ 'close_price' ][ - window_size :]) support = int ( close_price >= moving_average ) resistance = int ( close_price < moving_average ) # Accumulate the data pattern_detection_accumulator += { 'close_price' : [ close_price ], 'moving_average' : [ moving_average ], 'support' : [ support ], 'resistance' : [ resistance ]} return row # Calculate the volatility of the close price def calculate_volatility ( row ): global pattern_detection_accumulator , volatility_window_size close_price = row [ 'close' ] volatility = np . std ( pattern_detection_accumulator . value [ 'close_price' ][ - volatility_window_size :]) # Accumulate the data pattern_detection_accumulator += { 'volatility' : [ volatility ]} return row # Apply the calculation functions to each row of the data data = data . rdd . map ( calculate_moving_average_and_support_resistance ) . map ( calculate_volatility ) . toDF () # Convert the Spark DataFrame to a Pandas pd_data = data . toPandas () # Add the accumulated data to the Pandas DataFrame pd_data [ 'moving_average' ] = pattern_detection_accumulator . value [ 'moving_average' ] pd_data [ 'support' ] = pattern_detection_accumulator . value [ 'support' ] pd_data [ 'resistance' ] = pattern_detection_accumulator . value [ 'resistance' ] pd_data [ 'volatility' ] = pattern_detection_accumulator . value [ 'volatility' ] # Plot the data to visualize the results import matplotlib.pyplot as plt plt . plot ( pd_data [ 'date' ], pd_data [ 'close' ], label = 'Close Price' ) plt . plot ( pd_data [ 'date' ], pd_data [ 'moving_average' ], label = 'Moving Average' ) plt . scatter ( pd_data [ pd_data [ 'support' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'support' ] == 1 ][ 'close' ], label = 'Support' ) plt . scatter ( pd_data [ pd_data [ 'resistance' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'resistance' ] == 1 ][ 'close' ], label = 'Resistance' ) plt . legend () plt . show () # Plot the volatility of the close price plt . plot ( pd_data [ 'date' ], pd_data [ 'volatility' ], label = 'Volatility' ) plt . legend () plt . show () # Stop the Spark session spark . stop () For multiple symbols: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize the Spark context and session spark = SparkSession . builder . appName ( \"SignalProcessingStockData\" ) . getOrCreate () # Define a list of symbols to process symbols = [ 'AAPL' , 'GOOG' , 'MSFT' , 'IBM' ] # Loop over each symbol for symbol in symbols : # Load the stock data for the symbol data = spark . read . csv ( \"data/ {} _data.csv\" . format ( symbol ), header = True , inferSchema = True ) # Perform pattern detection on the data for this symbol pd_data = data . toPandas () pd_data [ 'moving_average' ] = calculate_moving_average ( pd_data [ 'close' ]) pd_data [ 'support' ], pd_data [ 'resistance' ] = calculate_support_resistance ( pd_data [ 'close' ], pd_data [ 'moving_average' ]) pd_data [ 'volatility' ] = calculate_volatility ( pd_data [ 'close' ]) # Plot the results for this symbol import matplotlib.pyplot as plt plt . plot ( pd_data [ 'date' ], pd_data [ 'close' ], label = 'Close Price' ) plt . plot ( pd_data [ 'date' ], pd_data [ 'moving_average' ], label = 'Moving Average' ) plt . scatter ( pd_data [ pd_data [ 'support' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'support' ] == 1 ][ 'close' ], label = 'Support' ) plt . scatter ( pd_data [ pd_data [ 'resistance' ] == 1 ][ 'date' ], pd_data [ pd_data [ 'resistance' ] == 1 ][ 'close' ], label = 'Resistance' ) plt . legend () plt . title ( symbol ) plt . show () # Plot the volatility of the close price plt . plot ( pd_data [ 'date' ], pd_data [ 'volatility' ], label = 'Volatility' ) plt . legend () plt . title ( symbol ) plt . show () # Stop the Spark session spark . stop () This code loops over a list of symbols and performs pattern detection on each symbol's data. The code uses the calculate_moving_average, calculate_support_resistance, and calculate_volatility functions to perform the pattern detection on the data. The results for each symbol are visualized using Matplotlib.","title":"4. Pattern recognition"},{"location":"explore/signal_processing/#tips-to-optimize-the-performance","text":"There is no single best way to perform pattern detection using signal processing in PySpark, as the performance will depend on the specifics of your data and the patterns you are trying to detect. However, there are some general tips you can follow to optimize performance: Parallelize the processing: Use PySpark's built-in support for parallel processing to perform the signal processing on multiple symbols in parallel. This will allow you to take advantage of multiple cores and distribute the processing load across multiple nodes in a cluster, leading to faster performance. Optimize the algorithms: Optimize the signal processing algorithms used to perform the pattern detection. This may involve tuning the parameters of the algorithms, or re-writing the algorithms to take advantage of Spark's parallel processing capabilities. Use an accumulator: An accumulator is a variable in Spark that can be used to accumulate values across multiple nodes in a cluster. By using an accumulator, you can avoid the overhead of sending intermediate results between nodes, leading to faster performance. Use caching: Use Spark's caching functionality to persist intermediate results in memory, so they do not need to be recalculated each time they are needed. Use broadcast variables: Broadcast variables are read-only variables in Spark that are stored on each node, rather than being sent over the network. By using broadcast variables, you can avoid the overhead of sending data between nodes, leading to faster performance. Avoid shuffling data unnecessarily: Shuffling data across nodes is a time-consuming operation in Spark, so it is best to avoid shuffling data whenever possible. If shuffling is necessary, try to minimize the amount of data that is shuffled, or use Spark's optimizations for shuffling, such as the use of bucketing or partitioning. 1. Parallelize the processing Here's an example to demonstrate how the processing can be parallelized in PySpark. Let's say we have a CSV file containing data for multiple symbols. We want to perform some signal processing on each symbol in parallel. Here's how we can do it: Python from pyspark import SparkContext from pyspark.sql import SparkSession # Initialize the Spark context and Spark session sc = SparkContext ( \"local\" , \"signal processing example\" ) spark = SparkSession . builder . appName ( \"SignalProcessing\" ) . getOrCreate () # Load the data from the CSV file df = spark . read . csv ( \"symbol_data.csv\" , header = True , inferSchema = True ) # Create a list of symbols to process in parallel symbols = df . select ( \"symbol\" ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () # Define the signal processing function def process_symbol ( symbol ): # Filter the data for the current symbol symbol_data = df . filter ( df [ \"symbol\" ] == symbol ) # Perform the signal processing on the symbol data # ... return processed_data # Parallelize the processing of each symbol rdd = sc . parallelize ( symbols ) processed_data = rdd . map ( process_symbol ) # Collect the processed data on the driver node result = processed_data . collect () In this example, we first create a list of symbols to process. We then define a signal processing function process_symbol that takes a symbol as input and performs the signal processing on the corresponding data. Finally, we use the parallelize method of the Spark context to create an RDD containing the symbols, and use the map method to apply the signal processing function to each symbol in parallel. The result is collected on the driver node and can be further processed or analyzed. 2. Optimize the algorithms To optimize the algorithms for faster performance, you can use various techniques, including vectorization, parallel processing, and algorithmic optimization. Here's an example in PySpark to demonstrate the optimization of algorithms for signal processing: Python from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import udf from pyspark.sql.types import DoubleType import numpy as np # Initialize the Spark context and Spark session sc = SparkContext ( \"local\" , \"signal processing example\" ) spark = SparkSession . builder . appName ( \"SignalProcessing\" ) . getOrCreate () # Load the data from the CSV file df = spark . read . csv ( \"symbol_data.csv\" , header = True , inferSchema = True ) # Define a user-defined function to perform smoothing def smoothing ( values ): # Use NumPy's smoothing function to perform the smoothing smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Register the smoothing UDF smoothing_udf = udf ( smoothing , ArrayType ( DoubleType ())) # Apply the smoothing UDF to the data smoothed_data = df . withColumn ( \"smoothed\" , smoothing_udf ( df [ \"values\" ])) # Collect the smoothed data on the driver node result = smoothed_data . collect () In this example, we use a user-defined function (UDF) to perform smoothing on the data. The UDF takes a list of values as input and returns a smoothed version of the data using NumPy's convolve function. By using a UDF, we can take advantage of the optimized algorithms in NumPy and perform the smoothing on large arrays efficiently. To further optimize the performance, you can consider the following suggestions: Use optimized libraries like NumPy or SciPy for signal processing algorithms. Python import numpy as np def smoothing ( values ): smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Register the smoothing UDF smoothing_udf = udf ( smoothing , ArrayType ( DoubleType ())) # Apply the smoothing UDF to the data smoothed_data = df . withColumn ( \"smoothed\" , smoothing_udf ( df [ \"values\" ])) In this example, we use the convolve function from NumPy to perform the smoothing operation on the data. By using this optimized library, we can take advantage of the optimized algorithms for performing signal processing operations. Use vectorized operations instead of loop-based operations. Python import numpy as np def smoothing ( values ): smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Convert the data into a NumPy array values = df . select ( \"values\" ) . toPandas () . values # Use NumPy's vectorized operations to perform the smoothing smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) In this example, we use vectorized operations from NumPy to perform the smoothing operation on the data. By using vectorized operations, we can perform the calculation on large arrays efficiently and reduce the time it takes to process the data. Use parallel processing wherever possible to speed up the processing of large datasets. Python from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import udf from pyspark.sql.types import DoubleType import numpy as np # Initialize the Spark context and Spark session sc = SparkContext ( \"local\" , \"signal processing example\" ) spark = SparkSession . builder . appName ( \"SignalProcessing\" ) . getOrCreate () # Load the data from the CSV file df = spark . read . csv ( \"symbol_data.csv\" , header = True , inferSchema = True ) # Define a user-defined function to perform smoothing def smoothing ( values ): # Use NumPy's smoothing function to perform the smoothing smoothed = np . convolve ( values , np . ones ( 10 ) / 10 , mode = 'valid' ) return smoothed # Register the smoothing UDF smoothing_udf = udf ( smoothing , ArrayType ( DoubleType ())) # Apply the smoothing UDF in parallel across the data smoothed_data = df . repartition ( 10 ) . withColumn ( \"smoothed\" , smoothing_udf ( df [ \"values\" ])) In this example, we use Spark's support for parallel processing to perform the smoothing operation on the data in parallel. By using Spark, we can take advantage of its distributed computing framework to process the data in parallel and reduce the time it takes to perform the calculation. Use algorithms with lower complexity, such as the Fast Fourier Transform (FFT), to perform computationally intensive operations. Python import numpy as np def smoothing ( values ): smoothed = np . fft . irfft ( np . fft . rfft ( values ) It's also important to note that the optimal approach will depend on the specific requirements of your use case and the characteristics of your data. You may need to experiment with different algorithms and approaches to find the best solution for your specific needs. 3. Use an accumulator: In Apache Spark, an accumulator is a write-only variable that can be used to accumulate values from multiple tasks into a single final result. Accumulators are used to efficiently implement algorithms that need to aggregate information from many tasks into a single result. Here's an example of using an accumulator in PySpark to detect support and resistance levels in stock data: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize Spark conf = SparkConf () . setAppName ( \"StockPatternDetection\" ) sc = SparkContext ( conf = conf ) spark = SparkSession ( sc ) # Read in stock data as a DataFrame df = spark . read . csv ( \"stock_data.csv\" , header = True ) # Create an accumulator to store the support and resistance levels support_and_resistance_accumulator = sc . accumulator ([], \"Support and Resistance Accumulator\" ) # Define a function that will update the accumulator with the support and resistance levels def update_accumulator ( record ): # Perform signal processing to detect support and resistance levels support , resistance = detect_support_and_resistance ( record ) # Update the accumulator with the support and resistance levels support_and_resistance_accumulator . add (( support , resistance )) # Use the .foreach() function to process each record in the DataFrame and update the accumulator df . rdd . foreach ( update_accumulator ) # Get the final result of the accumulator support_and_resistance = support_and_resistance_accumulator . value # Perform additional processing on the support and resistance levels (if needed) ... # Stop Spark sc . stop () In this example, the detect_support_and_resistance function would perform the signal processing to detect the support and resistance levels for a given record. The update_accumulator function would then update the support_and_resistance_accumulator with the results of the signal processing. The df.rdd.foreach(update_accumulator) line would then process each record in the DataFrame and call the update_accumulator function to update the accumulator. Finally, the final result of the accumulator could be obtained with support_and_resistance_accumulator.value. 4. Use Spark's caching Caching is a powerful feature in Apache Spark that allows you to persist intermediate data in memory across multiple stages of a pipeline. This can greatly improve the performance of your Spark application, especially when you are processing large datasets and need to reuse the same data multiple times. Here's an example of using Spark's caching in PySpark to detect support and resistance levels in stock data: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize Spark conf = SparkConf () . setAppName ( \"StockPatternDetection\" ) sc = SparkContext ( conf = conf ) spark = SparkSession ( sc ) # Read in stock data as a DataFrame df = spark . read . csv ( \"stock_data.csv\" , header = True ) # Cache the DataFrame in memory for faster access df . persist ( StorageLevel . MEMORY_ONLY ) # Create an accumulator to store the support and resistance levels support_and_resistance_accumulator = sc . accumulator ([], \"Support and Resistance Accumulator\" ) # Define a function that will update the accumulator with the support and resistance levels def update_accumulator ( record ): # Perform signal processing to detect support and resistance levels support , resistance = detect_support_and_resistance ( record ) # Update the accumulator with the support and resistance levels support_and_resistance_accumulator . add (( support , resistance )) # Use the .foreach() function to process each record in the DataFrame and update the accumulator df . rdd . foreach ( update_accumulator ) # Get the final result of the accumulator support_and_resistance = support_and_resistance_accumulator . value # Perform additional processing on the support and resistance levels (if needed) ... # Unpersist the cached DataFrame df . unpersist () # Stop Spark sc . stop () In this example, the df.persist(StorageLevel.MEMORY_ONLY) line caches the DataFrame in memory for faster access. The df.rdd.foreach(update_accumulator) line would then process each record in the DataFrame and call the update_accumulator function to update the accumulator. Finally, the cached DataFrame can be unpersisted with df.unpersist() to free up memory when it is no longer needed. 5. Use broadcast variable Broadcast variables are read-only variables in Apache Spark that are cached on each worker node rather than being sent with each task. Using broadcast variables can greatly improve the performance of your Spark application, especially when you are using the same large data structures or lookup tables in multiple stages of a pipeline. Here's an example of using a broadcast variable in PySpark to detect support and resistance levels in stock data: Python from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession # Initialize Spark conf = SparkConf () . setAppName ( \"StockPatternDetection\" ) sc = SparkContext ( conf = conf ) spark = SparkSession ( sc ) # Read in stock data as a DataFrame df = spark . read . csv ( \"stock_data.csv\" , header = True ) # Cache the DataFrame in memory for faster access df . persist ( StorageLevel . MEMORY_ONLY ) # Create a broadcast variable to store a lookup table of stock symbols stock_symbols_broadcast = sc . broadcast ([ \"AAPL\" , \"GOOG\" , \"MSFT\" , \"AMZN\" , \"FB\" ]) # Create an accumulator to store the support and resistance levels support_and_resistance_accumulator = sc . accumulator ([], \"Support and Resistance Accumulator\" ) # Define a function that will update the accumulator with the support and resistance levels def update_accumulator ( record ): # Check if the stock symbol is in the list of stock symbols stock_symbol = record [ 0 ] if stock_symbol in stock_symbols_broadcast . value : # Perform signal processing to detect support and resistance levels support , resistance = detect_support_and_resistance ( record ) # Update the accumulator with the support and resistance levels support_and_resistance_accumulator . add (( support , resistance )) # Use the .foreach() function to process each record in the DataFrame and update the accumulator df . rdd . foreach ( update_accumulator ) # Get the final result of the accumulator support_and_resistance = support_and_resistance_accumulator . value # Perform additional processing on the support and resistance levels (if needed) ... # Unpersist the cached DataFrame df . unpersist () # Stop Spark sc . stop () In this example, the stock_symbols_broadcast broadcast variable is used to store a list of stock symbols. The df.rdd.foreach(update_accumulator) line would then process each record in the DataFrame and call the update_accumulator function to update the accumulator. The stock_symbols_broadcast.value line would then check if the stock symbol is in the list of stock symbols. By using a broadcast variable, we can avoid the overhead of sending the lookup table with each task and instead cache it on each worker node. 6. Avoid shuffling data unnecessarily Python from pyspark import SparkContext from pyspark.sql import SparkSession # Initialize SparkContext and SparkSession sc = SparkContext ( \"local\" , \"avoid shuffling data unnecessarily example\" ) spark = SparkSession . builder . appName ( \"avoid shuffling data unnecessarily example\" ) . getOrCreate () # Load data into a DataFrame df = spark . read . csv ( \"path/to/data.csv\" , header = True ) # Repartition the data for efficient processing df = df . repartition ( 100 ) # Do any necessary preprocessing and transformations on the data df = df . filter ( df [ \"column_name\" ] > some_threshold ) df = df . groupBy ( \"column_name\" ) . agg ({ \"column_name\" : \"mean\" }) # Cache the data to avoid unnecessary shuffling df . cache () # Do any additional processing on the data result = df . count () # Unpersist the data after use df . unpersist () In this example, we first repartition the data into a smaller number of partitions to avoid shuffling data unnecessarily. Then, we cache the data in memory to avoid recomputing it each time it's used. Finally, we unpersist the data after use to free up memory resources. Note that it's important to carefully evaluate the trade-off between caching data and using limited memory resources. Caching too much data can cause out-of-memory errors, so it's a good idea to monitor memory usage and adjust the caching strategy as needed.","title":"Tips to optimize the performance"},{"location":"explore/spark-delta-partition/","text":"Creating partition in delta lake with spark \u00b6 How to create such a partition and insert into delta lake \u00b6 To create a partition in Delta Lake and insert data, you can use the following steps: Create a table with the partition column specified, in this case, \"event_dt\": SQL spark . sql ( \"\"\" CREATE TABLE table_name ( ... ) USING DELTA PARTITIONED BY (event_dt string) \"\"\" ) Insert data into the table and specify the partition value: data.write.format(\"delta\").partitionBy(\"event_dt\").mode(\"append\").save(\"/path/to/table_name\") You can also use the insertInto method to insert data into the table with partition data.write.format(\"delta\").insertInto(\"table_name\",overwrite=False) Now you can query your data by event_dt spark.sql(\"SELECT * FROM table_name WHERE event_dt = '2022-01-01'\") Node data.write.partitionBy(\"event_dt\").save(\"/path/to/table_name\", format=\"delta\", mode=\"append\") instead of data.write.format(\"delta\").partitionBy(\"event_dt\").mode(\"append\").save(\"/path/to/table_name\") It's important to note that when you partition the table by 'event_dt', it will create a subdirectory for each unique value of 'event_dt' in the table, which will allow for faster queries on that column. How to partition by event_dt and symbol for stock data to make query performant. \u00b6 To partition a Delta Lake table by multiple columns, such as \"event_dt\" and \"symbol\" for stock data, you can use the following steps: Create a table with the partition columns specified, in this case, \"event_dt\" and \"symbol\": SQL spark . sql ( \"\"\" CREATE TABLE table_name ( ... ) USING DELTA PARTITIONED BY (event_dt string, symbol string) \"\"\" ) Insert data into the table and specify the partition values: data.write.format(\"delta\").partitionBy(\"event_dt\",\"symbol\").mode(\"append\").save(\"/path/to/table_name\") You can also use the insertInto method to insert data into the table with partition data.write.format(\"delta\").insertInto(\"table_name\",overwrite=False) Now you can query your data by event_dt and symbol spark.sql(\"SELECT * FROM table_name WHERE event_dt = '2022-01-01' and symbol = 'AAPL'\") It's important to note that when you partition the table by 'event_dt' and 'symbol', it will create a subdirectory for each unique combination of 'event_dt' and 'symbol' in the table, which will allow for faster queries on those columns. Also, you can use the data.write.partitionBy(\"event_dt\",\"symbol\").save(\"/path/to/table_name\", format=\"delta\", mode=\"append\") instead of data.write.format(\"delta\").partitionBy(\"event_dt\",\"symbol\").mode(\"append\").save(\"/path/to/table_name\") It is advisable to partition on the columns that are most frequently used in filter conditions or join conditions. what's the performance impact if we query only by event_dt \u00b6 If you query a Delta Lake table that is partitioned by both \"event_dt\" and \"symbol\" but only include a filter on \"event_dt\", the performance of the query will still be relatively good. When you filter on only one of the partition columns, the query engine will scan only the partitions that match the filter condition, which will significantly reduce the amount of data that needs to be scanned. For example, if you have 100 partitions in total and 90 of them are filtered out based on the event_dt, only 10 partitions will be scanned. However, if you filter on both partition columns, the performance will be even better. This is because the query engine will be able to scan even fewer partitions, reducing the amount of data that needs to be read and processed. Also, when you partition the table by 'event_dt' and 'symbol', it will create a subdirectory for each unique combination of 'event_dt' and 'symbol' in the table. When you filter on only one partition column, it will scan all subdirectories for that partition column which increases the IO operations and a bit slower than filtering on both partition columns. In short, partitioning can speed up the query performance even when you filter on only one partition column, but the performance will be even better when filtering on both partition columns. What's the best practice to partition time series data like stock data \u00b6 When partitioning time series data like stock data, it's generally best practice to partition on the time-based column, such as \"event_dt\" or \"timestamp\", as well as any other columns that are frequently used in filter conditions or join conditions. Here are some best practices to consider when partitioning time series data: Partition on the time-based column: This allows for efficient querying of data based on time ranges, as well as efficient archiving of older data. Consider partitioning on additional columns: For example, if you frequently filter or join on a stock symbol column, you should also partition on that column. Choose the right granularity for partitioning: The granularity of the partitioning should match the granularity of the queries that will be run against the data. For example, if most of your queries are at a daily granularity, then partitioning on the day is a good choice. Keep the number of partitions manageable: Too many partitions can lead to poor performance and increased management overhead. So it's important to strike a balance between the granularity of the partitioning and the number of partitions created. Use bucketing: Bucketing is a technique for grouping data into more manageable chunks. It can be used in conjunction with partitioning to improve query performance on specific columns by creating additional subdirectories under each partition. Keep the data fresh: Partitioning makes it easy to delete old partitions, which is important for keeping the data fresh and the storage space used by the table small. It's also important to keep in mind that partitioning strategy may change over time based on the usage pattern of the data and the performance of the queries. How to create partition with bucketing \u00b6 Bucketing is a technique for grouping data into more manageable chunks, and can be used in conjunction with partitioning to improve query performance on specific columns. Here is an example of how to create a Delta Lake table that is partitioned by \"event_dt\" and bucketing on \"symbol\" using Spark SQL: SQL spark . sql ( \"\"\" CREATE TABLE table_name ( ... ) USING DELTA PARTITIONED BY (event_dt string) CLUSTERED BY (symbol string) INTO 32 BUCKETS \"\"\" ) This creates a table that is partitioned by \"event_dt\" and bucketing on \"symbol\" into 32 buckets. The \"CLUSTERED BY\" clause tells the table to group data by the \"symbol\" column, and \"INTO 32 BUCKETS\" tells it to create 32 buckets to group the data. You can also use the data.write.format(\"delta\").partitionBy(\"event_dt\").bucketBy(32, \"symbol\").save(\"/path/to/table_name\") method to create the table and insert data at the same time. When you query the table, the query engine will first filter on the partitions based on the \"event_dt\" column, and then it will further filter on the buckets based on the \"symbol\" column. This can significantly reduce the amount of data that needs to be scanned, as the query engine only needs to look at a subset of the data within each partition. It's important to note that bucketing works best when the distribution of the values in the bucketing column is relatively even. If the distribution is skewed, the bucketing will not be as effective. Also, the number of buckets should be selected based on the data distribution and the query pattern. It's also important to keep in mind that bucketing strategy may change over time based on the usage pattern of the data and the performance of the queries. How to append data with partition and bucketing \u00b6 To append data to a Delta Lake table that is partitioned by \"event_dt\" and bucketing on \"symbol\", you can use the following method: data.write.format(\"delta\").partitionBy(\"event_dt\").bucketBy(32, \"symbol\").mode(\"append\").save(\"/path/to/table_name\") This method uses the bucketBy() method to specify the bucketing column, in this case \"symbol\" and the number of buckets, in this case 32. The partitionBy() method is used to specify the partition column, in this case \"event_dt\" This is similar to the method used to write data to the table initially. The difference is that the mode(\"append\") method is used to tell Delta Lake to append the data to the existing table, rather than overwriting it. It's important to note that when you append data to a table that is bucketing, the data will be written to the appropriate bucket based on the value of the bucketing column. If the data contains new values for the bucketing column that do not match any existing buckets, new buckets will be created to accommodate the new data. Also, for the better performance, you can use the data.write.format(\"delta\").mode(\"append\").save(\"/path/to/table_name\") instead of using data.write.format(\"delta\").partitionBy(\"event_dt\").bucketBy(32, \"symbol\").mode(\"append\").save(\"/path/to/table_name\") . This will append the data to the existing table without specifying partition or bucketing and the data will be written to the existing partitions and buckets. It's important to keep in mind that data should be appended with the same bucketing strategy as the existing table, otherwise, it may lead to poor performance and increased management overhead. How to delete old partitions \u00b6 To delete old partitions in a Delta Lake table that is partitioned by \"event_dt\" and bucketing on \"symbol\", you can use the delete method. Here is an example of how to delete all partitions older than a certain date: Python from delta.tables import * deltaTable = DeltaTable . forPath ( \"/path/to/table_name\" ) deltaTable . delete ( \"event_dt < '2022-01-01'\" ) This will delete all the partitions with event_dt less than '2022-01-01' from the table. You can also use the deleteExpired method which deletes all the partitions that are older than the retention threshold. Python from delta.tables import * deltaTable = DeltaTable . forPath ( \"/path/to/table_name\" ) deltaTable . deleteExpired ( 180 , \"D\" ) This will delete all the partitions that are older than 180 days from the current date It's important to note that deleting old partitions in a bucketing table, will also delete the associated buckets and data. Therefore, it's important to make sure that you have a backup of the data before deleting it. Also, when you delete old partitions in a table that is being continuously written to, it's a good idea to do it during a period of low write activity to avoid any performance impact on the write operations. It's also important to keep in mind that the retention policy and the way to delete the old partitions may change over time based on the usage pattern of the data and the storage constraints. which one is better option, symbol as part of partition or bucketing? \u00b6 Whether to use \"symbol\" as a partition or bucketing column depends on the usage pattern of the data and the performance of the queries. If the data is frequently queried and filtered based on the \"symbol\" column, it would be more efficient to partition the data on that column. This allows the query engine to quickly identify the partitions that contain the relevant data and only scan those partitions. However, partitioning on symbol may lead to a large number of small partitions if the number of symbols is large, which can lead to poor performance and increased management overhead. On the other hand, if the data is queried and filtered based on the time-based column like \"event_dt\" and \"symbol\" is used for grouping the data, bucketing is a better option. Bucketing allows you to group similar data together, and reduce the number of files that need to be read by the query engine. Additionally, bucketing works best when the distribution of the values in the bucketing column is relatively even. In summary, if the data is frequently queried based on the \"symbol\" column, partitioning on that column would be more efficient. But if the data is queried based on the time-based column and \"symbol\" is used for grouping the data, bucketing is more appropriate. It's also important to keep in mind that partitioning and bucketing strategy may change over time based on the usage pattern of the data and the performance of the queries. It's a good idea to monitor the performance of the queries and make adjustments accordingly.","title":"Creating partition in delta lake with spark"},{"location":"explore/spark-delta-partition/#creating-partition-in-delta-lake-with-spark","text":"","title":"Creating partition in delta lake with spark"},{"location":"explore/spark-delta-partition/#how-to-create-such-a-partition-and-insert-into-delta-lake","text":"To create a partition in Delta Lake and insert data, you can use the following steps: Create a table with the partition column specified, in this case, \"event_dt\": SQL spark . sql ( \"\"\" CREATE TABLE table_name ( ... ) USING DELTA PARTITIONED BY (event_dt string) \"\"\" ) Insert data into the table and specify the partition value: data.write.format(\"delta\").partitionBy(\"event_dt\").mode(\"append\").save(\"/path/to/table_name\") You can also use the insertInto method to insert data into the table with partition data.write.format(\"delta\").insertInto(\"table_name\",overwrite=False) Now you can query your data by event_dt spark.sql(\"SELECT * FROM table_name WHERE event_dt = '2022-01-01'\") Node data.write.partitionBy(\"event_dt\").save(\"/path/to/table_name\", format=\"delta\", mode=\"append\") instead of data.write.format(\"delta\").partitionBy(\"event_dt\").mode(\"append\").save(\"/path/to/table_name\") It's important to note that when you partition the table by 'event_dt', it will create a subdirectory for each unique value of 'event_dt' in the table, which will allow for faster queries on that column.","title":"How to create such a partition and insert into delta lake"},{"location":"explore/spark-delta-partition/#how-to-partition-by-event_dt-and-symbol-for-stock-data-to-make-query-performant","text":"To partition a Delta Lake table by multiple columns, such as \"event_dt\" and \"symbol\" for stock data, you can use the following steps: Create a table with the partition columns specified, in this case, \"event_dt\" and \"symbol\": SQL spark . sql ( \"\"\" CREATE TABLE table_name ( ... ) USING DELTA PARTITIONED BY (event_dt string, symbol string) \"\"\" ) Insert data into the table and specify the partition values: data.write.format(\"delta\").partitionBy(\"event_dt\",\"symbol\").mode(\"append\").save(\"/path/to/table_name\") You can also use the insertInto method to insert data into the table with partition data.write.format(\"delta\").insertInto(\"table_name\",overwrite=False) Now you can query your data by event_dt and symbol spark.sql(\"SELECT * FROM table_name WHERE event_dt = '2022-01-01' and symbol = 'AAPL'\") It's important to note that when you partition the table by 'event_dt' and 'symbol', it will create a subdirectory for each unique combination of 'event_dt' and 'symbol' in the table, which will allow for faster queries on those columns. Also, you can use the data.write.partitionBy(\"event_dt\",\"symbol\").save(\"/path/to/table_name\", format=\"delta\", mode=\"append\") instead of data.write.format(\"delta\").partitionBy(\"event_dt\",\"symbol\").mode(\"append\").save(\"/path/to/table_name\") It is advisable to partition on the columns that are most frequently used in filter conditions or join conditions.","title":"How to partition by event_dt and symbol for stock data to make query performant."},{"location":"explore/spark-delta-partition/#whats-the-performance-impact-if-we-query-only-by-event_dt","text":"If you query a Delta Lake table that is partitioned by both \"event_dt\" and \"symbol\" but only include a filter on \"event_dt\", the performance of the query will still be relatively good. When you filter on only one of the partition columns, the query engine will scan only the partitions that match the filter condition, which will significantly reduce the amount of data that needs to be scanned. For example, if you have 100 partitions in total and 90 of them are filtered out based on the event_dt, only 10 partitions will be scanned. However, if you filter on both partition columns, the performance will be even better. This is because the query engine will be able to scan even fewer partitions, reducing the amount of data that needs to be read and processed. Also, when you partition the table by 'event_dt' and 'symbol', it will create a subdirectory for each unique combination of 'event_dt' and 'symbol' in the table. When you filter on only one partition column, it will scan all subdirectories for that partition column which increases the IO operations and a bit slower than filtering on both partition columns. In short, partitioning can speed up the query performance even when you filter on only one partition column, but the performance will be even better when filtering on both partition columns.","title":"what's the performance impact if we query only by event_dt"},{"location":"explore/spark-delta-partition/#whats-the-best-practice-to-partition-time-series-data-like-stock-data","text":"When partitioning time series data like stock data, it's generally best practice to partition on the time-based column, such as \"event_dt\" or \"timestamp\", as well as any other columns that are frequently used in filter conditions or join conditions. Here are some best practices to consider when partitioning time series data: Partition on the time-based column: This allows for efficient querying of data based on time ranges, as well as efficient archiving of older data. Consider partitioning on additional columns: For example, if you frequently filter or join on a stock symbol column, you should also partition on that column. Choose the right granularity for partitioning: The granularity of the partitioning should match the granularity of the queries that will be run against the data. For example, if most of your queries are at a daily granularity, then partitioning on the day is a good choice. Keep the number of partitions manageable: Too many partitions can lead to poor performance and increased management overhead. So it's important to strike a balance between the granularity of the partitioning and the number of partitions created. Use bucketing: Bucketing is a technique for grouping data into more manageable chunks. It can be used in conjunction with partitioning to improve query performance on specific columns by creating additional subdirectories under each partition. Keep the data fresh: Partitioning makes it easy to delete old partitions, which is important for keeping the data fresh and the storage space used by the table small. It's also important to keep in mind that partitioning strategy may change over time based on the usage pattern of the data and the performance of the queries.","title":"What's the best practice to partition time series data like stock data"},{"location":"explore/spark-delta-partition/#how-to-create-partition-with-bucketing","text":"Bucketing is a technique for grouping data into more manageable chunks, and can be used in conjunction with partitioning to improve query performance on specific columns. Here is an example of how to create a Delta Lake table that is partitioned by \"event_dt\" and bucketing on \"symbol\" using Spark SQL: SQL spark . sql ( \"\"\" CREATE TABLE table_name ( ... ) USING DELTA PARTITIONED BY (event_dt string) CLUSTERED BY (symbol string) INTO 32 BUCKETS \"\"\" ) This creates a table that is partitioned by \"event_dt\" and bucketing on \"symbol\" into 32 buckets. The \"CLUSTERED BY\" clause tells the table to group data by the \"symbol\" column, and \"INTO 32 BUCKETS\" tells it to create 32 buckets to group the data. You can also use the data.write.format(\"delta\").partitionBy(\"event_dt\").bucketBy(32, \"symbol\").save(\"/path/to/table_name\") method to create the table and insert data at the same time. When you query the table, the query engine will first filter on the partitions based on the \"event_dt\" column, and then it will further filter on the buckets based on the \"symbol\" column. This can significantly reduce the amount of data that needs to be scanned, as the query engine only needs to look at a subset of the data within each partition. It's important to note that bucketing works best when the distribution of the values in the bucketing column is relatively even. If the distribution is skewed, the bucketing will not be as effective. Also, the number of buckets should be selected based on the data distribution and the query pattern. It's also important to keep in mind that bucketing strategy may change over time based on the usage pattern of the data and the performance of the queries.","title":"How to create partition with bucketing"},{"location":"explore/spark-delta-partition/#how-to-append-data-with-partition-and-bucketing","text":"To append data to a Delta Lake table that is partitioned by \"event_dt\" and bucketing on \"symbol\", you can use the following method: data.write.format(\"delta\").partitionBy(\"event_dt\").bucketBy(32, \"symbol\").mode(\"append\").save(\"/path/to/table_name\") This method uses the bucketBy() method to specify the bucketing column, in this case \"symbol\" and the number of buckets, in this case 32. The partitionBy() method is used to specify the partition column, in this case \"event_dt\" This is similar to the method used to write data to the table initially. The difference is that the mode(\"append\") method is used to tell Delta Lake to append the data to the existing table, rather than overwriting it. It's important to note that when you append data to a table that is bucketing, the data will be written to the appropriate bucket based on the value of the bucketing column. If the data contains new values for the bucketing column that do not match any existing buckets, new buckets will be created to accommodate the new data. Also, for the better performance, you can use the data.write.format(\"delta\").mode(\"append\").save(\"/path/to/table_name\") instead of using data.write.format(\"delta\").partitionBy(\"event_dt\").bucketBy(32, \"symbol\").mode(\"append\").save(\"/path/to/table_name\") . This will append the data to the existing table without specifying partition or bucketing and the data will be written to the existing partitions and buckets. It's important to keep in mind that data should be appended with the same bucketing strategy as the existing table, otherwise, it may lead to poor performance and increased management overhead.","title":"How to append data with partition and bucketing"},{"location":"explore/spark-delta-partition/#how-to-delete-old-partitions","text":"To delete old partitions in a Delta Lake table that is partitioned by \"event_dt\" and bucketing on \"symbol\", you can use the delete method. Here is an example of how to delete all partitions older than a certain date: Python from delta.tables import * deltaTable = DeltaTable . forPath ( \"/path/to/table_name\" ) deltaTable . delete ( \"event_dt < '2022-01-01'\" ) This will delete all the partitions with event_dt less than '2022-01-01' from the table. You can also use the deleteExpired method which deletes all the partitions that are older than the retention threshold. Python from delta.tables import * deltaTable = DeltaTable . forPath ( \"/path/to/table_name\" ) deltaTable . deleteExpired ( 180 , \"D\" ) This will delete all the partitions that are older than 180 days from the current date It's important to note that deleting old partitions in a bucketing table, will also delete the associated buckets and data. Therefore, it's important to make sure that you have a backup of the data before deleting it. Also, when you delete old partitions in a table that is being continuously written to, it's a good idea to do it during a period of low write activity to avoid any performance impact on the write operations. It's also important to keep in mind that the retention policy and the way to delete the old partitions may change over time based on the usage pattern of the data and the storage constraints.","title":"How to delete old partitions"},{"location":"explore/spark-delta-partition/#which-one-is-better-option-symbol-as-part-of-partition-or-bucketing","text":"Whether to use \"symbol\" as a partition or bucketing column depends on the usage pattern of the data and the performance of the queries. If the data is frequently queried and filtered based on the \"symbol\" column, it would be more efficient to partition the data on that column. This allows the query engine to quickly identify the partitions that contain the relevant data and only scan those partitions. However, partitioning on symbol may lead to a large number of small partitions if the number of symbols is large, which can lead to poor performance and increased management overhead. On the other hand, if the data is queried and filtered based on the time-based column like \"event_dt\" and \"symbol\" is used for grouping the data, bucketing is a better option. Bucketing allows you to group similar data together, and reduce the number of files that need to be read by the query engine. Additionally, bucketing works best when the distribution of the values in the bucketing column is relatively even. In summary, if the data is frequently queried based on the \"symbol\" column, partitioning on that column would be more efficient. But if the data is queried based on the time-based column and \"symbol\" is used for grouping the data, bucketing is more appropriate. It's also important to keep in mind that partitioning and bucketing strategy may change over time based on the usage pattern of the data and the performance of the queries. It's a good idea to monitor the performance of the queries and make adjustments accordingly.","title":"which one is better option, symbol as part of partition or bucketing?"},{"location":"explore/trading-view/","text":"Trading view and prinescirpt \u00b6 Create trading view pinescript strategy to generate signals \u00b6 buy signal when the price is above chiku span and above ichimoku cloud sell signal when the price is below chiku span and below ichimoku cloud Here is an example of a TradingView PineScript strategy that generates buy and sell signals based on the conditions you specified: Python // Chikou Span chikou = close [ 26 ] // Ichimoku Cloud tenkan = ( highest ( high , 9 ) + lowest ( low , 9 )) / 2 kijun = ( highest ( high , 26 ) + lowest ( low , 26 )) / 2 senkou_a = ( tenkan + kijun ) / 2 senkou_b = ( highest ( high , 52 ) + lowest ( low , 52 )) / 2 // Buy and Sell Signals if close > chikou and close > senkou_a and close > senkou_b strategy . entry ( \"Buy\" , strategy . long ) if close < chikou and close < senkou_a and close < senkou_b strategy . entry ( \"Sell\" , strategy . short ) Please note that this is a basic example and you can always customize it according to your specific requirements. Create singal in trading view prinescript for unusual volume \u00b6 Here is an example of a TradingView PineScript strategy that generates a signal based on unusual volume: Python // Define the length of the lookback period for volume comparison lookback = 20 // Calculate the average volume for the lookback period avgVolume = sma ( volume , lookback ) // Compare current volume to average volume if ( volume > avgVolume * 2 ) // Signal unusual volume strategy . entry ( \"Unusual Volume\" , strategy . long ) In this example, the strategy generates a \"Unusual Volume\" signal when the current volume is more than twice the average volume over the past 20 periods. You can adjust the lookback period, and the multiplier of the average volume to suit your needs. Also please note that this is a basic example, and you can always add other indicators or conditions to filter these signals and make them more accurate, and you can also adjust the strategy to suit your needs. How to calculate support and resistance in trading view prinscript from ichimoku strategy \u00b6 In TradingView PineScript, you can calculate support and resistance levels using the Ichimoku Kink\u014d Hy\u014d indicator by analyzing the positions of the Kijun-sen and Senkou Span B lines. The Kijun-sen line, also known as the base line, is a moving average of the highest high and lowest low over a period of 26 periods, and is commonly used as a support and resistance level. When the price is above the Kijun-sen line, it is considered to be in a bullish trend and the Kijun-sen line acts as a support level. When the price is below the Kijun-sen line, it is considered to be in a bearish trend and the Kijun-sen line acts as a resistance level. The Senkou Span B line, also known as the leading span B, is the average of the highest high and lowest low over a period of 52 periods, plotted 26 periods ahead. This line can also act as a support and resistance level. When the price is above the Senkou Span B line, it is considered to be in a bullish trend and the Senkou Span B line acts as a support level. When the price is below the Senkou Span B line, it is considered to be in a bearish trend and the Senkou Span B line acts as a resistance level. Here is an example of how you can calculate support and resistance levels using the Ichimoku Kink\u014d Hy\u014d indicator in TradingView PineScript: Python // Ichimoku Cloud tenkan = ( highest ( high , 9 ) + lowest ( low , 9 )) / 2 kijun = ( highest ( high , 26 ) + lowest ( low , 26 )) / 2 senkou_a = ( tenkan + kijun ) / 2 senkou_b = ( highest ( high , 52 ) + lowest ( low , 52 )) / 2 // Support and Resistance Levels if ( close > kijun ) plot ( kijun , color = blue , linewidth = 2 ) else plot ( kijun , color = red , linewidth = 2 ) if ( close > senkou_b ) plot ( senkou_b , color = blue , linewidth = 2 ) else plot ( senkou_b , color = red , linewidth = 2 ) In this example, the Kijun-sen line is plotted in blue when the price is above it and in red when the price is below it, indicating a support level when it's blue and resistance level when it's red. Similarly, the Senkou Span B line is plotted in blue when the price is above it and in red when the price is below it, indicating a support level when it's blue and resistance level when it's red. It's important to note that support and resistance levels are not always exact and can be broken. Traders often use these levels as a guide for making trading decisions, and also it's important to have a good understanding of the market and to use multiple indicators and other market data to form a more complete picture of the market. What are the best strategies for day option trading \u00b6 Day trading options can be a high-risk, high-reward strategy that can be profitable for experienced traders. Here are a few strategies that day traders may use when trading options: Covered call: This strategy involves holding a long position in an underlying asset and selling call options against that position. The goal of this strategy is to generate income from the option premium while also limiting the potential loss on the underlying asset. Bullish vertical spread: This strategy involves buying a call option at a lower strike price and selling a call option at a higher strike price. The goal of this strategy is to profit from a bullish market move while also limiting the potential loss. Bearish vertical spread: This strategy involves buying a put option at a higher strike price and selling a put option at a lower strike price. The goal of this strategy is to profit from a bearish market move while also limiting the potential loss. Iron condor: This strategy involves selling a call option and a put option at different strike prices to generate income from the option premium. The goal of this strategy is to profit from a neutral market move while also limiting the potential loss. Straddle: This strategy involves buying both a call and a put option at the same strike price, with the goal of profiting from a large move in either direction in the underlying asset. It's important to keep in mind that options trading is a complex and high-risk strategy, and it's important to have a good understanding of the market and to use proper risk management techniques. Additionally, it's important to use multiple indicators and other market data to form a more complete picture of the market. It's also highly recommended to paper trade and backtest any strategy before using it in live trading. Create pinescript for those strategies \u00b6 It's important to keep in mind that these are just examples and you would need to customize them to suit your specific requirements and also it's important to backtest and validate any strategy before using it in live trading. Covered call: ```python // Define the strike price and expiration date of the call option strikePrice = 50 expiration = timestamp(2022,12,31) // Buy a long position in the underlying asset strategy.entry(\"Long Position\", strategy.long) // Sell a call option at the defined strike price and expiration date strategy.entry(\"Call Option\", strategy.short, when = strategy.position_size > 0, limit_price = strikePrice, expiry = expiration) 2. Bullish vertical spread: python // Define the strike prices of the options lowerStrike = 50 upperStrike = 55 expiration = timestamp(2022,12,31) // Buy a call option at the lower strike price strategy.entry(\"Call Option\", strategy.long, limit_price = lowerStrike, expiry = expiration) // Sell a call option at the upper strike price strategy.entry(\"Call Option\", strategy.short, limit_price = upperStrike, expiry = expiration) 3. Bearish vertical spread: python // Define the strike prices of the options lowerStrike = 45 upperStrike = 50 expiration = timestamp(2022,12,31) // Buy a put option at the upper strike price strategy.entry(\"Put Option\", strategy.long, limit_price = upperStrike, expiry = expiration) // Sell a put option at the lower strike price strategy.entry(\"Put Option\", strategy.short, limit_price = lowerStrike, expiry = expiration) ``` 4. Iron condor: Python // Define the strike prices and expiration date of the options lowerCallStrike = 50 upperCallStrike = 55 lowerPutStrike = 45 upperPutStrike = 50 expiration = timestamp ( 2022 , 12 , 31 ) // Sell a call option at the lower strike price strategy . entry ( \"Call Option\" , strategy . short , limit_price = lowerCallStrike , expiry = expiration ) // Sell a put option at the lower strike price strategy . entry ( \"Put Option\" , strategy . short , limit_price = lowerPutStrike , expiry = expiration ) // Buy a call option at the upper strike price strategy . entry ( \"Call Option\" , strategy . long , limit_price = upperCallStrike , expiry = expiration ) // Buy a put option at the upper strike price strategy . entry ( \"Put Option\" , strategy . long , limit_price = upperPutStrike , expiry = expiration ) 5. Straddle: Python // Define the strike price and expiration date of the options strikePrice = 50 expiration = timestamp ( 2022 , 12 , 31 ) // Buy a call option at the defined strike price and expiration date strategy . entry ( \"Call Option\" , strategy . long , limit_price = strikePrice , expiry = expiration ) // Buy a put option at the defined strike price and expiration date strategy . entry ( \"Put Option\" , strategy . long , limit_price = strikePrice , expiry = expiration ) It's important to note that options trading is a complex and high-risk strategy, and it's important to have a good understanding","title":"Trading view and prinescirpt"},{"location":"explore/trading-view/#trading-view-and-prinescirpt","text":"","title":"Trading view and prinescirpt"},{"location":"explore/trading-view/#create-trading-view-pinescript-strategy-to-generate-signals","text":"buy signal when the price is above chiku span and above ichimoku cloud sell signal when the price is below chiku span and below ichimoku cloud Here is an example of a TradingView PineScript strategy that generates buy and sell signals based on the conditions you specified: Python // Chikou Span chikou = close [ 26 ] // Ichimoku Cloud tenkan = ( highest ( high , 9 ) + lowest ( low , 9 )) / 2 kijun = ( highest ( high , 26 ) + lowest ( low , 26 )) / 2 senkou_a = ( tenkan + kijun ) / 2 senkou_b = ( highest ( high , 52 ) + lowest ( low , 52 )) / 2 // Buy and Sell Signals if close > chikou and close > senkou_a and close > senkou_b strategy . entry ( \"Buy\" , strategy . long ) if close < chikou and close < senkou_a and close < senkou_b strategy . entry ( \"Sell\" , strategy . short ) Please note that this is a basic example and you can always customize it according to your specific requirements.","title":"Create trading view pinescript strategy to generate signals"},{"location":"explore/trading-view/#create-singal-in-trading-view-prinescript-for-unusual-volume","text":"Here is an example of a TradingView PineScript strategy that generates a signal based on unusual volume: Python // Define the length of the lookback period for volume comparison lookback = 20 // Calculate the average volume for the lookback period avgVolume = sma ( volume , lookback ) // Compare current volume to average volume if ( volume > avgVolume * 2 ) // Signal unusual volume strategy . entry ( \"Unusual Volume\" , strategy . long ) In this example, the strategy generates a \"Unusual Volume\" signal when the current volume is more than twice the average volume over the past 20 periods. You can adjust the lookback period, and the multiplier of the average volume to suit your needs. Also please note that this is a basic example, and you can always add other indicators or conditions to filter these signals and make them more accurate, and you can also adjust the strategy to suit your needs.","title":"Create singal in trading view prinescript for unusual volume"},{"location":"explore/trading-view/#how-to-calculate-support-and-resistance-in-trading-view-prinscript-from-ichimoku-strategy","text":"In TradingView PineScript, you can calculate support and resistance levels using the Ichimoku Kink\u014d Hy\u014d indicator by analyzing the positions of the Kijun-sen and Senkou Span B lines. The Kijun-sen line, also known as the base line, is a moving average of the highest high and lowest low over a period of 26 periods, and is commonly used as a support and resistance level. When the price is above the Kijun-sen line, it is considered to be in a bullish trend and the Kijun-sen line acts as a support level. When the price is below the Kijun-sen line, it is considered to be in a bearish trend and the Kijun-sen line acts as a resistance level. The Senkou Span B line, also known as the leading span B, is the average of the highest high and lowest low over a period of 52 periods, plotted 26 periods ahead. This line can also act as a support and resistance level. When the price is above the Senkou Span B line, it is considered to be in a bullish trend and the Senkou Span B line acts as a support level. When the price is below the Senkou Span B line, it is considered to be in a bearish trend and the Senkou Span B line acts as a resistance level. Here is an example of how you can calculate support and resistance levels using the Ichimoku Kink\u014d Hy\u014d indicator in TradingView PineScript: Python // Ichimoku Cloud tenkan = ( highest ( high , 9 ) + lowest ( low , 9 )) / 2 kijun = ( highest ( high , 26 ) + lowest ( low , 26 )) / 2 senkou_a = ( tenkan + kijun ) / 2 senkou_b = ( highest ( high , 52 ) + lowest ( low , 52 )) / 2 // Support and Resistance Levels if ( close > kijun ) plot ( kijun , color = blue , linewidth = 2 ) else plot ( kijun , color = red , linewidth = 2 ) if ( close > senkou_b ) plot ( senkou_b , color = blue , linewidth = 2 ) else plot ( senkou_b , color = red , linewidth = 2 ) In this example, the Kijun-sen line is plotted in blue when the price is above it and in red when the price is below it, indicating a support level when it's blue and resistance level when it's red. Similarly, the Senkou Span B line is plotted in blue when the price is above it and in red when the price is below it, indicating a support level when it's blue and resistance level when it's red. It's important to note that support and resistance levels are not always exact and can be broken. Traders often use these levels as a guide for making trading decisions, and also it's important to have a good understanding of the market and to use multiple indicators and other market data to form a more complete picture of the market.","title":"How to calculate support and resistance in trading view prinscript from ichimoku strategy"},{"location":"explore/trading-view/#what-are-the-best-strategies-for-day-option-trading","text":"Day trading options can be a high-risk, high-reward strategy that can be profitable for experienced traders. Here are a few strategies that day traders may use when trading options: Covered call: This strategy involves holding a long position in an underlying asset and selling call options against that position. The goal of this strategy is to generate income from the option premium while also limiting the potential loss on the underlying asset. Bullish vertical spread: This strategy involves buying a call option at a lower strike price and selling a call option at a higher strike price. The goal of this strategy is to profit from a bullish market move while also limiting the potential loss. Bearish vertical spread: This strategy involves buying a put option at a higher strike price and selling a put option at a lower strike price. The goal of this strategy is to profit from a bearish market move while also limiting the potential loss. Iron condor: This strategy involves selling a call option and a put option at different strike prices to generate income from the option premium. The goal of this strategy is to profit from a neutral market move while also limiting the potential loss. Straddle: This strategy involves buying both a call and a put option at the same strike price, with the goal of profiting from a large move in either direction in the underlying asset. It's important to keep in mind that options trading is a complex and high-risk strategy, and it's important to have a good understanding of the market and to use proper risk management techniques. Additionally, it's important to use multiple indicators and other market data to form a more complete picture of the market. It's also highly recommended to paper trade and backtest any strategy before using it in live trading.","title":"What are the best strategies for day option trading"},{"location":"explore/trading-view/#create-pinescript-for-those-strategies","text":"It's important to keep in mind that these are just examples and you would need to customize them to suit your specific requirements and also it's important to backtest and validate any strategy before using it in live trading. Covered call: ```python // Define the strike price and expiration date of the call option strikePrice = 50 expiration = timestamp(2022,12,31) // Buy a long position in the underlying asset strategy.entry(\"Long Position\", strategy.long) // Sell a call option at the defined strike price and expiration date strategy.entry(\"Call Option\", strategy.short, when = strategy.position_size > 0, limit_price = strikePrice, expiry = expiration) 2. Bullish vertical spread: python // Define the strike prices of the options lowerStrike = 50 upperStrike = 55 expiration = timestamp(2022,12,31) // Buy a call option at the lower strike price strategy.entry(\"Call Option\", strategy.long, limit_price = lowerStrike, expiry = expiration) // Sell a call option at the upper strike price strategy.entry(\"Call Option\", strategy.short, limit_price = upperStrike, expiry = expiration) 3. Bearish vertical spread: python // Define the strike prices of the options lowerStrike = 45 upperStrike = 50 expiration = timestamp(2022,12,31) // Buy a put option at the upper strike price strategy.entry(\"Put Option\", strategy.long, limit_price = upperStrike, expiry = expiration) // Sell a put option at the lower strike price strategy.entry(\"Put Option\", strategy.short, limit_price = lowerStrike, expiry = expiration) ``` 4. Iron condor: Python // Define the strike prices and expiration date of the options lowerCallStrike = 50 upperCallStrike = 55 lowerPutStrike = 45 upperPutStrike = 50 expiration = timestamp ( 2022 , 12 , 31 ) // Sell a call option at the lower strike price strategy . entry ( \"Call Option\" , strategy . short , limit_price = lowerCallStrike , expiry = expiration ) // Sell a put option at the lower strike price strategy . entry ( \"Put Option\" , strategy . short , limit_price = lowerPutStrike , expiry = expiration ) // Buy a call option at the upper strike price strategy . entry ( \"Call Option\" , strategy . long , limit_price = upperCallStrike , expiry = expiration ) // Buy a put option at the upper strike price strategy . entry ( \"Put Option\" , strategy . long , limit_price = upperPutStrike , expiry = expiration ) 5. Straddle: Python // Define the strike price and expiration date of the options strikePrice = 50 expiration = timestamp ( 2022 , 12 , 31 ) // Buy a call option at the defined strike price and expiration date strategy . entry ( \"Call Option\" , strategy . long , limit_price = strikePrice , expiry = expiration ) // Buy a put option at the defined strike price and expiration date strategy . entry ( \"Put Option\" , strategy . long , limit_price = strikePrice , expiry = expiration ) It's important to note that options trading is a complex and high-risk strategy, and it's important to have a good understanding","title":"Create pinescript for those strategies"},{"location":"explore/tradingview-setup/","text":"Trading set for API automation \u00b6 Setup the condition or screener for the alert \u00b6 Construct message JSON { \"type\" : \"superTrendScreener\" , \"secret\" : \"d4a8a427-5d19-45a1-9365-ecc19324fb91\" , \"message\" :[ { \"trendType\" : \"upTrend\" , \"ticker\" : \"{{exchange}}:{{ticker}}\" , \"open\" : \"{{open}}\" , \"close\" : \"{{close}}\" , \"percentage\" : \"1\" , \"timenow\" : \"{{timenow}}\" } ], \"time\" : \"{{time}}\" , \"timenow\" : \"{{timenow}}\" , \"interval\" : \"{{interval}}\" } Add webhook \u00b6 https://ailgo-web.ailgo.org/TelegramViewHttpTrigger List inbound IP addresses \u00b6 JavaScript 52.89.214.238 34.212.75.30 54.218.53.128 52.32.178.7 Ref: https://www.tradingview.com/support/solutions/43000529348-about-webhooks/ Screener Ref \u00b6 https://github.com/lth-elm/TradingView-Webhook-Trading-Bot/blob/main/pinestrategies/Quick-Mean-Reversion-Strat.pine JavaScript // This source code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/ // \u00a9 lth_elm //@version=4 strategy ( \"Mean reversion strategy\" , overlay = true , margin_long = 100 , margin_short = 100 ) // ----- INPUTS ----- // EMA ema_len = input ( 200 , minval = 1 , title = \"EMA Length\" ) ema = ema ( close , ema_len ) plot ( ema , title = \"EMA\" , color = color . blue ) // RSI rsi_len = input ( 14 , minval = 1 , title = \"RSI Length\" ) r = rsi ( close , rsi_len ) // ATR tp_sl_mul = input ( false , \"\u2500\u2500\u2500\u2500\u2003TP/SL multiplicator\" ) risk = input ( title = \"Risk (%)\" , type = input . float , minval = 0 , defval = 1 ) risk := risk / 100 atr_bars = input ( title = \"ATR Bars\" , defval = 14 , minval = 1 ) stop_mult = input ( title = \"\u2003\u25aa Stoploss Multiplicator\" , type = input . float , defval = 3 ) tp_mult = input ( title = \"\u2003\u25aa Takeprofit Multiplicator\" , type = input . float , defval = 5 ) tp1_mult = input ( title = \"\u2003\u25aa TP1 Multiplicator\" , type = input . float , defval = 0.75 ) tp2_mult = input ( title = \"\u2003\u25aa TP2 Multiplicator\" , type = input . float , defval = 1.5 ) tp_close = input ( title = \"\u25ba TPx close (%)\" , type = input . float , defval = 15 ) atr = atr ( atr_bars ) plot ( tp_close , title = \"TP Close\" , display = display . none ) plot ( tp1_mult , title = \"TP1 Mult\" , display = display . none ) plot ( tp2_mult , title = \"TP2 Mult\" , display = display . none ) // ----- CONDITIONS ----- bullishEng = open <= close [ 1 ] and close > open [ 1 ] and open [ 1 ] > close [ 1 ] bearishEng = open >= close [ 1 ] and close < open [ 1 ] and open [ 1 ] < close [ 1 ] goLong = bullishEng and close < ema and r [ 1 ] < 30 goShort = bearishEng and close > ema and r [ 1 ] > 70 // ------ GET IN/OUT A TRADE ------ inLongPosition = strategy . position_size > 0 inShortPosition = strategy . position_size < 0 notInTrade = strategy . position_size == 0 // ----- MANAGE TP/SL ----- // For long float long_stop_level = na float long_profit_level = na long_stop_level := ( goLong and notInTrade ) ? close - atr * stop_mult : long_stop_level [ 1 ] long_profit_level := ( goLong and notInTrade ) ? close + atr * tp_mult : long_profit_level [ 1 ] // For short float short_stop_level = na float short_profit_level = na short_stop_level := ( goShort and notInTrade ) ? close + atr * stop_mult : short_stop_level [ 1 ] short_profit_level := ( goShort and notInTrade ) ? close - atr * tp_mult : short_profit_level [ 1 ] // Execute buy or sell order if goLong and notInTrade size = ( strategy . equity * risk ) / abs ( close - long_stop_level ) strategy . entry ( \"long\" , strategy . long , qty = size , alert_message = \"entry\" ) // strategy.exit(\"exit\", \"long\", stop=long_stop_level, limit=long_profit_level, alert_message=\"exit\") inLongPosition := true notInTrade := false else if goShort and notInTrade size = ( strategy . equity * risk ) / abs ( close - short_stop_level ) strategy . entry ( \"short\" , strategy . short , qty = size , when = notInTrade , alert_message = \"entry\" ) // strategy.exit(\"exit\", \"short\", stop=short_stop_level, limit=short_profit_level, alert_message=\"exit\") inShortPosition := true notInTrade := false // ----- MULTIPLE TP ----- // Set multiple takeprofit and the stoploss ticksConversion ( x ) => strategy . position_size != 0 ? ( x / syminfo . mintick ) : float ( na ) long_sl = abs ( strategy . position_avg_price - long_stop_level ) long_tp = abs ( strategy . position_avg_price - long_profit_level ) long_tp1 = tp1_mult * long_sl long_tp2 = tp2_mult * long_sl if not notInTrade [ 1 ] and goLong [ 1 ] strategy . exit ( \"tp1\" , \"long\" , qty_percent = tp_close , profit = ticksConversion ( long_tp1 ), loss = ticksConversion ( long_sl ), alert_message = \"tp1_breakeven\" ) strategy . exit ( \"tp2\" , \"long\" , qty_percent = tp_close , profit = ticksConversion ( long_tp2 ), loss = ticksConversion ( long_sl ), alert_message = \"tp2\" ) strategy . exit ( \"exit\" , \"long\" , qty_percent = 100 , profit = ticksConversion ( long_tp ), loss = ticksConversion ( long_sl ), alert_message = \"exit\" ) short_sl = abs ( strategy . position_avg_price - short_stop_level ) short_tp = abs ( strategy . position_avg_price - short_profit_level ) short_tp1 = tp1_mult * short_sl short_tp2 = tp2_mult * short_sl if not notInTrade [ 1 ] and goShort [ 1 ] strategy . exit ( \"tp1\" , \"short\" , qty_percent = tp_close , profit = ticksConversion ( short_tp1 ), loss = ticksConversion ( short_sl ), alert_message = \"tp1_breakeven\" ) strategy . exit ( \"tp2\" , \"short\" , qty_percent = tp_close , profit = ticksConversion ( short_tp2 ), loss = ticksConversion ( short_sl ), alert_message = \"tp2\" ) strategy . exit ( \"exit\" , \"short\" , qty_percent = 100 , profit = ticksConversion ( short_tp ), loss = ticksConversion ( short_sl ), alert_message = \"exit\" ) // Exit or breakeven longbreakstop = strategy . position_avg_price + 2 * ( strategy . position_avg_price * 0.001 ) shortbreakstop = strategy . position_avg_price - 2 * ( strategy . position_avg_price * 0.001 ) if inLongPosition if close > ema // or we can keep our atr multiplicator first objective strategy . close ( \"long\" , alert_message = \"exit\" ) breakeven_level = strategy . position_avg_price + long_tp1 // change if breakeven tpx if high > breakeven_level strategy . exit ( \"exit\" , \"long\" , qty_percent = 100 , stop = longbreakstop , limit = long_profit_level , alert_message = \"exit\" ) strategy . exit ( \"tp2\" , \"long\" , qty_percent = tp_close , stop = longbreakstop , profit = ticksConversion ( long_tp2 ), alert_message = \"tp2\" ) if inShortPosition if close < ema // or we can keep our atr multiplicator first objective strategy . close ( \"short\" , alert_message = \"exit\" ) breakeven_level = strategy . position_avg_price - short_tp1 // change if breakeven tpx if low < breakeven_level strategy . exit ( \"exit\" , \"short\" , qty_percent = 100 , stop = shortbreakstop , limit = short_profit_level , alert_message = \"exit\" ) strategy . exit ( \"tp2\" , \"short\" , qty_percent = tp_close , stop = shortbreakstop , profit = ticksConversion ( short_tp2 ), alert_message = \"tp2\" ) // ----- Plot TP/SL ----- plot ( longbreakstop , color = color . yellow , title = \"Long Breakeven\" , display = display . none ) plot ( shortbreakstop , color = color . yellow , title = \"Short Breakeven\" , display = display . none ) invisible = color . new ( color . white , 100 ) p_price = plot ( strategy . position_avg_price , color = invisible ) p_long_stop = plot ( long_stop_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Long SL\" ) p_long_profit = plot ( long_profit_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Long TP\" ) p_short_stop = plot ( short_stop_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Short SL\" ) p_short_profit = plot ( short_profit_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Short TP\" ) fill ( p_long_stop , p_price , color = inLongPosition ? color . red : na ) fill ( p_long_profit , p_price , color = inLongPosition ? color . green : na ) fill ( p_short_stop , p_price , color = inShortPosition ? color . red : na ) fill ( p_short_profit , p_price , color = inShortPosition ? color . green : na ) JavaScript //@version=4 strategy ( title = \"Ichimoku & Williams Fractals Screener\" , overlay = false ) i_timeZone = input ( \"GMT-8\" ) screenerFunc () => // Ichimoku Cloud conversionLinePeriods = 9 baseLinePeriods = 26 lagSpan2Periods = 52 conversionLine = ( highest ( high , conversionLinePeriods ) + lowest ( low , conversionLinePeriods )) / 2 baseLine = ( highest ( high , baseLinePeriods ) + lowest ( low , baseLinePeriods )) / 2 leadLine1 = ( conversionLine + baseLine ) / 2 // Williams Fractals fractalLookBack = 2 isHighFractal = high [ fractalLookBack ] < high and high > high [ 1 ] and high [ 1 ] > high [ 2 ] isLowFractal = low [ fractalLookBack ] > low and low < low [ 1 ] and low [ 1 ] < low [ 2 ] // Trading Logic longCondition = crossover ( close , leadLine1 ) and isLowFractal [ fractalLookBack ] shortCondition = crossunder ( close , leadLine1 ) and isHighFractal [ fractalLookBack ] [ longCondition , shortCondition , open , close ] getPacificCurrentTime () => pacificTime = tostring ( hour ( timenow , i_timeZone ), \"00.\" ) + tostring ( minute ( timenow , i_timeZone ), \"00.\" ) + tostring ( second ( timenow , i_timeZone ), \"00\" ) roundn ( x , n ) => mult = 1 if n != 0 for i = 1 to abs ( n ) mult := mult * 10 n >= 0 ? round ( x * mult ) / mult : round ( x / mult ) * mult formatLabel ( ticker01 , longCondition01 , shortCondition01 , open01 , close01 ) => lbl = '\\n ticker:' + ticker01 + ' open:' + tostring ( roundn ( open01 , 3 )) + ' close:' + tostring ( roundn ( close01 , 3 )) + ' timenow:' + getPacificCurrentTime () lbl s01 = input ( title = \"COINBASE:BTCUSD\" , type = input . symbol , defval = \"COINBASE:BTCUSD\" ) s02 = input ( title = \"COINBASE:ETHUSD\" , type = input . symbol , defval = \"COINBASE:ETHUSD\" ) s03 = input ( title = \"COINBASE:LTCUSD\" , type = input . symbol , defval = \"COINBASE:LTCUSD\" ) [ longCondition01 , shortCondition01 , open01 , close01 ] = security ( s01 , timeframe . period , screenerFunc ()) [ longCondition02 , shortCondition02 , open02 , close02 ] = security ( s02 , timeframe . period , screenerFunc ()) [ longCondition03 , shortCondition03 , open03 , close03 ] = security ( s03 , timeframe . period , screenerFunc ()) scr_label_long = '\"trendType:Long' scr_label_long := longCondition01 ? scr_label_long + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_long scr_label_long := longCondition02 ? scr_label_long + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_long scr_label_long := longCondition03 ? scr_label_long + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_long scr_label_long := scr_label_long + '\"' scr_label_short = '\"trendType:Short' scr_label_short := shortCondition01 ? scr_label_short + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_short scr_label_short := shortCondition02 ? scr_label_short + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_short scr_label_short := shortCondition03 ? scr_label_short + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_short scr_label_short := scr_label_short + '\"' lab_l = label . new ( bar_index , 0 , scr_label_long , color = color . gray , textcolor = color . black , style = label . style_labeldown , yloc = yloc . price ) label . delete ( lab_l [ 1 ]) plot ( 0 , color = color . new ( color . gray , 0 )) if ( longCondition01 or longCondition02 or longCondition03 ) strategy . entry ( \"long\" , long = true , alert_message = scr_label_long ) if ( shortCondition01 or shortCondition02 or shortCondition03 ) strategy . entry ( \"short\" , long = false , alert_message = scr_label_short ) JavaScript //@version=5 strategy ( title = 'Ichimoku & Williams Fractals Screener' , overlay = false ) i_timeZone = input ( 'GMT-8' ) screenerFunc () => // Ichimoku Cloud conversionLinePeriods = 9 baseLinePeriods = 26 lagSpan2Periods = 52 conversionLine = ( ta . highest ( high , conversionLinePeriods ) + ta . lowest ( low , conversionLinePeriods )) / 2 baseLine = ( ta . highest ( high , baseLinePeriods ) + ta . lowest ( low , baseLinePeriods )) / 2 leadLine1 = ( conversionLine + baseLine ) / 2 // Williams Fractals fractalLookBack = 2 isHighFractal = high [ fractalLookBack ] < high and high > high [ 1 ] and high [ 1 ] > high [ 2 ] isLowFractal = low [ fractalLookBack ] > low and low < low [ 1 ] and low [ 1 ] < low [ 2 ] // Trading Logic longCondition = ta . crossover ( close , leadLine1 ) and isLowFractal [ fractalLookBack ] shortCondition = ta . crossunder ( close , leadLine1 ) and isHighFractal [ fractalLookBack ] [ longCondition , shortCondition , open , close ] getPacificCurrentTime () => pacificTime = str . tostring ( hour ( timenow , i_timeZone ), '00.' ) + str . tostring ( minute ( timenow , i_timeZone ), '00.' ) + str . tostring ( second ( timenow , i_timeZone ), '00' ) pacificTime roundn ( x , n ) => mult = 1 if n != 0 for i = 1 to math . abs ( n ) by 1 mult *= 10 mult n >= 0 ? math . round ( x * mult ) / mult : math . round ( x / mult ) * mult formatLabel ( ticker01 , longCondition01 , shortCondition01 , open01 , close01 ) => lbl = '\\n ticker:' + ticker01 + ' open:' + str . tostring ( roundn ( open01 , 3 )) + ' close:' + str . tostring ( roundn ( close01 , 3 )) + ' timenow:' + getPacificCurrentTime () lbl s01 = input . symbol ( title = 'COINBASE:BTCUSD' , defval = 'COINBASE:BTCUSD' ) s02 = input . symbol ( title = 'COINBASE:ETHUSD' , defval = 'COINBASE:ETHUSD' ) s03 = input . symbol ( title = 'COINBASE:LTCUSD' , defval = 'COINBASE:LTCUSD' ) [ longCondition01 , shortCondition01 , open01 , close01 ] = request . security ( s01 , timeframe . period , screenerFunc ()) [ longCondition02 , shortCondition02 , open02 , close02 ] = request . security ( s02 , timeframe . period , screenerFunc ()) [ longCondition03 , shortCondition03 , open03 , close03 ] = request . security ( s03 , timeframe . period , screenerFunc ()) scr_label_long = '\"trendType:Long' scr_label_long := longCondition01 ? scr_label_long + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_long scr_label_long := longCondition02 ? scr_label_long + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_long scr_label_long := longCondition03 ? scr_label_long + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_long scr_label_long += '\"' scr_label_short = '\"trendType:Short' scr_label_short := shortCondition01 ? scr_label_short + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_short scr_label_short := shortCondition02 ? scr_label_short + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_short scr_label_short := shortCondition03 ? scr_label_short + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_short scr_label_short += '\"' lab_l = label . new ( bar_index , 0 , scr_label_long , color = color . gray , textcolor = color . black , style = label . style_label_down , yloc = yloc . price ) label . delete ( lab_l [ 1 ]) plot ( 0 , color = color . new ( color . gray , 0 )) if longCondition01 or longCondition02 or longCondition03 strategy . entry ( 'long' , direction = strategy . long , alert_message = scr_label_long ) if shortCondition01 or shortCondition02 or shortCondition03 strategy . entry ( 'short' , direction = strategy . short , alert_message = scr_label_short )","title":"Trading set for API automation"},{"location":"explore/tradingview-setup/#trading-set-for-api-automation","text":"","title":"Trading set for API automation"},{"location":"explore/tradingview-setup/#setup-the-condition-or-screener-for-the-alert","text":"Construct message JSON { \"type\" : \"superTrendScreener\" , \"secret\" : \"d4a8a427-5d19-45a1-9365-ecc19324fb91\" , \"message\" :[ { \"trendType\" : \"upTrend\" , \"ticker\" : \"{{exchange}}:{{ticker}}\" , \"open\" : \"{{open}}\" , \"close\" : \"{{close}}\" , \"percentage\" : \"1\" , \"timenow\" : \"{{timenow}}\" } ], \"time\" : \"{{time}}\" , \"timenow\" : \"{{timenow}}\" , \"interval\" : \"{{interval}}\" }","title":"Setup the condition or screener for the alert"},{"location":"explore/tradingview-setup/#add-webhook","text":"https://ailgo-web.ailgo.org/TelegramViewHttpTrigger","title":"Add webhook"},{"location":"explore/tradingview-setup/#list-inbound-ip-addresses","text":"JavaScript 52.89.214.238 34.212.75.30 54.218.53.128 52.32.178.7 Ref: https://www.tradingview.com/support/solutions/43000529348-about-webhooks/","title":"List inbound IP addresses"},{"location":"explore/tradingview-setup/#screener-ref","text":"https://github.com/lth-elm/TradingView-Webhook-Trading-Bot/blob/main/pinestrategies/Quick-Mean-Reversion-Strat.pine JavaScript // This source code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/ // \u00a9 lth_elm //@version=4 strategy ( \"Mean reversion strategy\" , overlay = true , margin_long = 100 , margin_short = 100 ) // ----- INPUTS ----- // EMA ema_len = input ( 200 , minval = 1 , title = \"EMA Length\" ) ema = ema ( close , ema_len ) plot ( ema , title = \"EMA\" , color = color . blue ) // RSI rsi_len = input ( 14 , minval = 1 , title = \"RSI Length\" ) r = rsi ( close , rsi_len ) // ATR tp_sl_mul = input ( false , \"\u2500\u2500\u2500\u2500\u2003TP/SL multiplicator\" ) risk = input ( title = \"Risk (%)\" , type = input . float , minval = 0 , defval = 1 ) risk := risk / 100 atr_bars = input ( title = \"ATR Bars\" , defval = 14 , minval = 1 ) stop_mult = input ( title = \"\u2003\u25aa Stoploss Multiplicator\" , type = input . float , defval = 3 ) tp_mult = input ( title = \"\u2003\u25aa Takeprofit Multiplicator\" , type = input . float , defval = 5 ) tp1_mult = input ( title = \"\u2003\u25aa TP1 Multiplicator\" , type = input . float , defval = 0.75 ) tp2_mult = input ( title = \"\u2003\u25aa TP2 Multiplicator\" , type = input . float , defval = 1.5 ) tp_close = input ( title = \"\u25ba TPx close (%)\" , type = input . float , defval = 15 ) atr = atr ( atr_bars ) plot ( tp_close , title = \"TP Close\" , display = display . none ) plot ( tp1_mult , title = \"TP1 Mult\" , display = display . none ) plot ( tp2_mult , title = \"TP2 Mult\" , display = display . none ) // ----- CONDITIONS ----- bullishEng = open <= close [ 1 ] and close > open [ 1 ] and open [ 1 ] > close [ 1 ] bearishEng = open >= close [ 1 ] and close < open [ 1 ] and open [ 1 ] < close [ 1 ] goLong = bullishEng and close < ema and r [ 1 ] < 30 goShort = bearishEng and close > ema and r [ 1 ] > 70 // ------ GET IN/OUT A TRADE ------ inLongPosition = strategy . position_size > 0 inShortPosition = strategy . position_size < 0 notInTrade = strategy . position_size == 0 // ----- MANAGE TP/SL ----- // For long float long_stop_level = na float long_profit_level = na long_stop_level := ( goLong and notInTrade ) ? close - atr * stop_mult : long_stop_level [ 1 ] long_profit_level := ( goLong and notInTrade ) ? close + atr * tp_mult : long_profit_level [ 1 ] // For short float short_stop_level = na float short_profit_level = na short_stop_level := ( goShort and notInTrade ) ? close + atr * stop_mult : short_stop_level [ 1 ] short_profit_level := ( goShort and notInTrade ) ? close - atr * tp_mult : short_profit_level [ 1 ] // Execute buy or sell order if goLong and notInTrade size = ( strategy . equity * risk ) / abs ( close - long_stop_level ) strategy . entry ( \"long\" , strategy . long , qty = size , alert_message = \"entry\" ) // strategy.exit(\"exit\", \"long\", stop=long_stop_level, limit=long_profit_level, alert_message=\"exit\") inLongPosition := true notInTrade := false else if goShort and notInTrade size = ( strategy . equity * risk ) / abs ( close - short_stop_level ) strategy . entry ( \"short\" , strategy . short , qty = size , when = notInTrade , alert_message = \"entry\" ) // strategy.exit(\"exit\", \"short\", stop=short_stop_level, limit=short_profit_level, alert_message=\"exit\") inShortPosition := true notInTrade := false // ----- MULTIPLE TP ----- // Set multiple takeprofit and the stoploss ticksConversion ( x ) => strategy . position_size != 0 ? ( x / syminfo . mintick ) : float ( na ) long_sl = abs ( strategy . position_avg_price - long_stop_level ) long_tp = abs ( strategy . position_avg_price - long_profit_level ) long_tp1 = tp1_mult * long_sl long_tp2 = tp2_mult * long_sl if not notInTrade [ 1 ] and goLong [ 1 ] strategy . exit ( \"tp1\" , \"long\" , qty_percent = tp_close , profit = ticksConversion ( long_tp1 ), loss = ticksConversion ( long_sl ), alert_message = \"tp1_breakeven\" ) strategy . exit ( \"tp2\" , \"long\" , qty_percent = tp_close , profit = ticksConversion ( long_tp2 ), loss = ticksConversion ( long_sl ), alert_message = \"tp2\" ) strategy . exit ( \"exit\" , \"long\" , qty_percent = 100 , profit = ticksConversion ( long_tp ), loss = ticksConversion ( long_sl ), alert_message = \"exit\" ) short_sl = abs ( strategy . position_avg_price - short_stop_level ) short_tp = abs ( strategy . position_avg_price - short_profit_level ) short_tp1 = tp1_mult * short_sl short_tp2 = tp2_mult * short_sl if not notInTrade [ 1 ] and goShort [ 1 ] strategy . exit ( \"tp1\" , \"short\" , qty_percent = tp_close , profit = ticksConversion ( short_tp1 ), loss = ticksConversion ( short_sl ), alert_message = \"tp1_breakeven\" ) strategy . exit ( \"tp2\" , \"short\" , qty_percent = tp_close , profit = ticksConversion ( short_tp2 ), loss = ticksConversion ( short_sl ), alert_message = \"tp2\" ) strategy . exit ( \"exit\" , \"short\" , qty_percent = 100 , profit = ticksConversion ( short_tp ), loss = ticksConversion ( short_sl ), alert_message = \"exit\" ) // Exit or breakeven longbreakstop = strategy . position_avg_price + 2 * ( strategy . position_avg_price * 0.001 ) shortbreakstop = strategy . position_avg_price - 2 * ( strategy . position_avg_price * 0.001 ) if inLongPosition if close > ema // or we can keep our atr multiplicator first objective strategy . close ( \"long\" , alert_message = \"exit\" ) breakeven_level = strategy . position_avg_price + long_tp1 // change if breakeven tpx if high > breakeven_level strategy . exit ( \"exit\" , \"long\" , qty_percent = 100 , stop = longbreakstop , limit = long_profit_level , alert_message = \"exit\" ) strategy . exit ( \"tp2\" , \"long\" , qty_percent = tp_close , stop = longbreakstop , profit = ticksConversion ( long_tp2 ), alert_message = \"tp2\" ) if inShortPosition if close < ema // or we can keep our atr multiplicator first objective strategy . close ( \"short\" , alert_message = \"exit\" ) breakeven_level = strategy . position_avg_price - short_tp1 // change if breakeven tpx if low < breakeven_level strategy . exit ( \"exit\" , \"short\" , qty_percent = 100 , stop = shortbreakstop , limit = short_profit_level , alert_message = \"exit\" ) strategy . exit ( \"tp2\" , \"short\" , qty_percent = tp_close , stop = shortbreakstop , profit = ticksConversion ( short_tp2 ), alert_message = \"tp2\" ) // ----- Plot TP/SL ----- plot ( longbreakstop , color = color . yellow , title = \"Long Breakeven\" , display = display . none ) plot ( shortbreakstop , color = color . yellow , title = \"Short Breakeven\" , display = display . none ) invisible = color . new ( color . white , 100 ) p_price = plot ( strategy . position_avg_price , color = invisible ) p_long_stop = plot ( long_stop_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Long SL\" ) p_long_profit = plot ( long_profit_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Long TP\" ) p_short_stop = plot ( short_stop_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Short SL\" ) p_short_profit = plot ( short_profit_level , color = invisible , style = plot . style_linebr , linewidth = 1 , title = \"Short TP\" ) fill ( p_long_stop , p_price , color = inLongPosition ? color . red : na ) fill ( p_long_profit , p_price , color = inLongPosition ? color . green : na ) fill ( p_short_stop , p_price , color = inShortPosition ? color . red : na ) fill ( p_short_profit , p_price , color = inShortPosition ? color . green : na ) JavaScript //@version=4 strategy ( title = \"Ichimoku & Williams Fractals Screener\" , overlay = false ) i_timeZone = input ( \"GMT-8\" ) screenerFunc () => // Ichimoku Cloud conversionLinePeriods = 9 baseLinePeriods = 26 lagSpan2Periods = 52 conversionLine = ( highest ( high , conversionLinePeriods ) + lowest ( low , conversionLinePeriods )) / 2 baseLine = ( highest ( high , baseLinePeriods ) + lowest ( low , baseLinePeriods )) / 2 leadLine1 = ( conversionLine + baseLine ) / 2 // Williams Fractals fractalLookBack = 2 isHighFractal = high [ fractalLookBack ] < high and high > high [ 1 ] and high [ 1 ] > high [ 2 ] isLowFractal = low [ fractalLookBack ] > low and low < low [ 1 ] and low [ 1 ] < low [ 2 ] // Trading Logic longCondition = crossover ( close , leadLine1 ) and isLowFractal [ fractalLookBack ] shortCondition = crossunder ( close , leadLine1 ) and isHighFractal [ fractalLookBack ] [ longCondition , shortCondition , open , close ] getPacificCurrentTime () => pacificTime = tostring ( hour ( timenow , i_timeZone ), \"00.\" ) + tostring ( minute ( timenow , i_timeZone ), \"00.\" ) + tostring ( second ( timenow , i_timeZone ), \"00\" ) roundn ( x , n ) => mult = 1 if n != 0 for i = 1 to abs ( n ) mult := mult * 10 n >= 0 ? round ( x * mult ) / mult : round ( x / mult ) * mult formatLabel ( ticker01 , longCondition01 , shortCondition01 , open01 , close01 ) => lbl = '\\n ticker:' + ticker01 + ' open:' + tostring ( roundn ( open01 , 3 )) + ' close:' + tostring ( roundn ( close01 , 3 )) + ' timenow:' + getPacificCurrentTime () lbl s01 = input ( title = \"COINBASE:BTCUSD\" , type = input . symbol , defval = \"COINBASE:BTCUSD\" ) s02 = input ( title = \"COINBASE:ETHUSD\" , type = input . symbol , defval = \"COINBASE:ETHUSD\" ) s03 = input ( title = \"COINBASE:LTCUSD\" , type = input . symbol , defval = \"COINBASE:LTCUSD\" ) [ longCondition01 , shortCondition01 , open01 , close01 ] = security ( s01 , timeframe . period , screenerFunc ()) [ longCondition02 , shortCondition02 , open02 , close02 ] = security ( s02 , timeframe . period , screenerFunc ()) [ longCondition03 , shortCondition03 , open03 , close03 ] = security ( s03 , timeframe . period , screenerFunc ()) scr_label_long = '\"trendType:Long' scr_label_long := longCondition01 ? scr_label_long + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_long scr_label_long := longCondition02 ? scr_label_long + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_long scr_label_long := longCondition03 ? scr_label_long + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_long scr_label_long := scr_label_long + '\"' scr_label_short = '\"trendType:Short' scr_label_short := shortCondition01 ? scr_label_short + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_short scr_label_short := shortCondition02 ? scr_label_short + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_short scr_label_short := shortCondition03 ? scr_label_short + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_short scr_label_short := scr_label_short + '\"' lab_l = label . new ( bar_index , 0 , scr_label_long , color = color . gray , textcolor = color . black , style = label . style_labeldown , yloc = yloc . price ) label . delete ( lab_l [ 1 ]) plot ( 0 , color = color . new ( color . gray , 0 )) if ( longCondition01 or longCondition02 or longCondition03 ) strategy . entry ( \"long\" , long = true , alert_message = scr_label_long ) if ( shortCondition01 or shortCondition02 or shortCondition03 ) strategy . entry ( \"short\" , long = false , alert_message = scr_label_short ) JavaScript //@version=5 strategy ( title = 'Ichimoku & Williams Fractals Screener' , overlay = false ) i_timeZone = input ( 'GMT-8' ) screenerFunc () => // Ichimoku Cloud conversionLinePeriods = 9 baseLinePeriods = 26 lagSpan2Periods = 52 conversionLine = ( ta . highest ( high , conversionLinePeriods ) + ta . lowest ( low , conversionLinePeriods )) / 2 baseLine = ( ta . highest ( high , baseLinePeriods ) + ta . lowest ( low , baseLinePeriods )) / 2 leadLine1 = ( conversionLine + baseLine ) / 2 // Williams Fractals fractalLookBack = 2 isHighFractal = high [ fractalLookBack ] < high and high > high [ 1 ] and high [ 1 ] > high [ 2 ] isLowFractal = low [ fractalLookBack ] > low and low < low [ 1 ] and low [ 1 ] < low [ 2 ] // Trading Logic longCondition = ta . crossover ( close , leadLine1 ) and isLowFractal [ fractalLookBack ] shortCondition = ta . crossunder ( close , leadLine1 ) and isHighFractal [ fractalLookBack ] [ longCondition , shortCondition , open , close ] getPacificCurrentTime () => pacificTime = str . tostring ( hour ( timenow , i_timeZone ), '00.' ) + str . tostring ( minute ( timenow , i_timeZone ), '00.' ) + str . tostring ( second ( timenow , i_timeZone ), '00' ) pacificTime roundn ( x , n ) => mult = 1 if n != 0 for i = 1 to math . abs ( n ) by 1 mult *= 10 mult n >= 0 ? math . round ( x * mult ) / mult : math . round ( x / mult ) * mult formatLabel ( ticker01 , longCondition01 , shortCondition01 , open01 , close01 ) => lbl = '\\n ticker:' + ticker01 + ' open:' + str . tostring ( roundn ( open01 , 3 )) + ' close:' + str . tostring ( roundn ( close01 , 3 )) + ' timenow:' + getPacificCurrentTime () lbl s01 = input . symbol ( title = 'COINBASE:BTCUSD' , defval = 'COINBASE:BTCUSD' ) s02 = input . symbol ( title = 'COINBASE:ETHUSD' , defval = 'COINBASE:ETHUSD' ) s03 = input . symbol ( title = 'COINBASE:LTCUSD' , defval = 'COINBASE:LTCUSD' ) [ longCondition01 , shortCondition01 , open01 , close01 ] = request . security ( s01 , timeframe . period , screenerFunc ()) [ longCondition02 , shortCondition02 , open02 , close02 ] = request . security ( s02 , timeframe . period , screenerFunc ()) [ longCondition03 , shortCondition03 , open03 , close03 ] = request . security ( s03 , timeframe . period , screenerFunc ()) scr_label_long = '\"trendType:Long' scr_label_long := longCondition01 ? scr_label_long + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_long scr_label_long := longCondition02 ? scr_label_long + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_long scr_label_long := longCondition03 ? scr_label_long + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_long scr_label_long += '\"' scr_label_short = '\"trendType:Short' scr_label_short := shortCondition01 ? scr_label_short + formatLabel ( s01 , longCondition01 , shortCondition01 , open01 , close01 ) : scr_label_short scr_label_short := shortCondition02 ? scr_label_short + formatLabel ( s02 , longCondition02 , shortCondition02 , open02 , close02 ) : scr_label_short scr_label_short := shortCondition03 ? scr_label_short + formatLabel ( s03 , longCondition03 , shortCondition03 , open03 , close03 ) : scr_label_short scr_label_short += '\"' lab_l = label . new ( bar_index , 0 , scr_label_long , color = color . gray , textcolor = color . black , style = label . style_label_down , yloc = yloc . price ) label . delete ( lab_l [ 1 ]) plot ( 0 , color = color . new ( color . gray , 0 )) if longCondition01 or longCondition02 or longCondition03 strategy . entry ( 'long' , direction = strategy . long , alert_message = scr_label_long ) if shortCondition01 or shortCondition02 or shortCondition03 strategy . entry ( 'short' , direction = strategy . short , alert_message = scr_label_short )","title":"Screener Ref"},{"location":"finance/use_of_ft/","text":"Fourier Transform in Price \u00b6 Transformers for Financial Analysis: Exploring the Potential \u00b6 This document summarizes our conversation on the potential of transformers in revolutionizing the field of financial analysis. We discussed various applications and explored how these powerful language models can be used to gain insights, improve decision-making, and automate tasks across the financial landscape. Table of Contents: Introduction Overview of Transformers Applications in Financial Analysis Portfolio Optimization Financial Stress Prediction Market Microstructure Analysis Financial Crisis Prediction Explainability and Interpretability Fraud Investigation and Risk Management Market Anomaly Detection Data Integration and Interoperability Data Augmentation and Generation Additional Applications Model Examples and Resources Dockerfile and Project Files for Testing Sample Test Code Downloadable Markdown File Introduction: Transformers are a class of deep learning models that have achieved remarkable results in various natural language processing tasks. Their ability to process and understand complex language makes them valuable tools for analyzing financial data and extracting valuable insights. Overview of Transformers: Transformers are based on the concept of \"attention,\" which allows them to focus on relevant parts of the input data. This enables them to learn complex relationships between words and sentences, leading to superior performance in tasks like sentiment analysis, question answering, and text summarization. Applications in Financial Analysis: Transformers can be applied to a wide range of tasks in financial analysis, including: Portfolio Optimization: Personalized investment portfolios tailored to individual risk profiles and financial goals. Financial Stress Prediction: Early warnings and interventions for individuals at risk of financial hardship. Market Microstructure Analysis: Identifying trading opportunities and developing algorithmic trading strategies. Financial Crisis Prediction: Proactive preparation and mitigation strategies for potential economic downturns. Explainability and Interpretability: Building trust and confidence in AI-driven financial applications. Fraud Investigation and Risk Management: Identifying suspicious activities and preventing financial losses. Market Anomaly Detection: Predicting market crashes and identifying opportunities for arbitrage. Data Integration and Interoperability: Combining information from diverse sources for comprehensive analysis. Data Augmentation and Generation: Enhancing model training and evaluation by creating synthetic data. Additional Applications: Personalized Tax Planning and Optimization: Recommending customized tax strategies for individuals. Financial Product Recommendation and Targeting: Providing relevant offerings to customers based on their preferences. Financial Market Sentiment Analysis and Prediction: Making informed investment decisions based on market sentiment. Financial Regulatory Compliance Automation and Reporting: Generating regulatory reports and ensuring compliance. Financial Data Democratization and Accessibility: Empowering individuals to make informed financial decisions. Model Examples and Resources: This document lists various pre-trained transformer models suitable for financial tasks, along with information about their functionalities and access points: Model Name Description Where to Get Details ProsusAI/finbert Finetuned for financial text data. Hugging Face Sentiment analysis, entity recognition, classification. t5-base Text-to-text transfer transformer Hugging Face Summarization, question answering, translation. nlpaueb/bert-base-ner Finetuned for named entity recognition Hugging Face Identifying and classifying entities in financial text. allenai/longformer-base-4096 Long sequences of text Hugging Face Analyzing financial documents and reports. bigscience/bloom-176b Large language model Hugging Face Complex tasks like creative text formats, translation, and informative answers. Dockerfile and Project Files for Testing: A Dockerfile and project files are provided to set up a testing environment for exploring these transformers. The files are downloadable and include instructions for building and running the container. Sample Test Code: Example test scripts demonstrate how to perform simple tests with the FinBert model. These examples can be adapted to test other transformers based on their functionalities. Downloadable Markdown File: This markdown file, containing the entire conversation, is downloadable for future reference and sharing. Conclusion: Transformers hold tremendous potential for transforming the way we analyze and operate within the financial world. Their ability to process large amounts of data, extract insights, and automate tasks can lead to improved decision-making, increased efficiency, and broader access to financial services. As research and development continue to advance, we can expect even more innovative applications to emerge, further shaping the future of finance. Dockerfile for Transformer Testing Environment \u00b6 This Dockerfile creates an environment suitable for testing various transformers listed above. Text Only FROM python:3.9 WORKDIR /app # Install basic dependencies RUN apt-get update && apt-get install -y curl unzip # Install PyTorch RUN pip install torch torchvision torchaudio # Install Transformers library RUN pip install transformers # Download and install additional dependencies for specific transformers RUN pip install -r requirements.txt # Copy project files COPY . . # Run your test scripts CMD [\"python\", \"run_tests.py\"] Project File (requirements.txt) \u00b6 This file lists additional dependencies needed for specific transformers mentioned in previous examples. Text Only # ProsusAI/finbert transformers[tokenizers]==4.21.0 torch-optimizer==0.9.0 # allenai/longformer-base-4096 tqdm==4.64.0 # bigscience/bloom-176b datasets==2.0.0 Usage \u00b6 Build the Docker image: Text Only docker build -t transformers-testing . Run the Docker container: Text Only docker run -it transformers-testing This will place you inside the container with all necessary dependencies installed. You can now run your tests or experiment with different transformers. To stop the container: Text Only docker stop <container_id> To remove the container: Text Only docker rm <container_id> Note: Remember to replace <container_id> with the actual ID of your container. run_tests.py \u00b6 Python import unittest # Import specific tests for each transformer from tests.test_finbert import TestFinBert from tests.test_t5_base import TestT5Base from tests.test_bert_base_ner import TestBertBaseNER from tests.test_longformer_base_4096 import TestLongformerBase4096 from tests.test_bloom_176b import TestBloom176B # Create a test suite suite = unittest . TestSuite () # Add tests for each transformer suite . addTest ( unittest . makeSuite ( TestFinBert )) suite . addTest ( unittest . makeSuite ( TestT5Base )) suite . addTest ( unittest . makeSuite ( TestBertBaseNER )) suite . addTest ( unittest . makeSuite ( TestLongformerBase4096 )) suite . addTest ( unittest . makeSuite ( TestBloom176B )) # Run the test suite runner = unittest . TextTestRunner () runner . run ( suite ) Test Code for Individual Transformers \u00b6 This example shows test code for the FinBert model. Similar test scripts should be created for other transformers mentioned earlier. Python # tests/test_finbert.py from transformers import AutoTokenizer , AutoModelForSequenceClassification # Define the FinBert model and tokenizer model_name = \"ProsusAI/finbert\" tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForSequenceClassification . from_pretrained ( model_name ) class TestFinBert ( unittest . TestCase ): def test_sentiment_analysis ( self ): # Define test sentence sentence = \"The company's financial performance is strong and the future looks bright.\" # Encode the sentence inputs = tokenizer ( sentence , return_tensors = \"pt\" ) # Predict the sentiment outputs = model ( ** inputs ) predicted_sentiment = outputs . logits . argmax ( - 1 ) . item () # Assert that the predicted sentiment is positive self . assertEqual ( predicted_sentiment , 1 ) # Add further tests for other FinBert functionalities # ... Note: Replace the FinBert tests with specific test cases for other transformers mentioned earlier. Remember to adapt the code based on the functionalities and tasks each transformer is designed for. Transformer Models for Financial Analysis \u00b6 Transformer models, a type of deep learning architecture, have gained immense popularity in natural language processing (NLP) tasks due to their remarkable ability to capture long-range dependencies and context in text data. These same capabilities can be leveraged to enhance financial analysis in several ways: 1. Sentiment Analysis: Analyzing financial news articles, social media posts, and other textual data to gauge market sentiment and identify potential trends. Identifying bullish or bearish sentiment surrounding specific companies or assets based on their textual mentions. Quantifying the sentiment of earnings reports and conference calls to understand investor sentiment towards a company. 2. Financial Text Summarization: Automatically generating concise summaries of financial reports, research papers, and other lengthy documents. Extracting key insights and actionable information from complex financial documents for faster and more efficient analysis. Providing a quick overview of financial news and events for busy professionals. 3. Financial Text Generation: Generating synthetic financial reports, market commentaries, and other relevant text based on historical data and trends. Creating automated reports and updates for financial institutions and investment firms. Personalizing financial news and information for individual investors based on their preferences and risk tolerance. 4. Question Answering Systems: Developing financial chatbots and virtual assistants that can answer questions about specific companies, markets, and financial concepts. Providing on-demand access to financial information and insights for investors and analysts. Enhancing customer service and support for financial institutions through interactive question-answering systems. 5. Entity Recognition and Relationship Extraction: Identifying and extracting key entities like companies, individuals, and locations from financial documents. Uncovering relationships between entities, such as mergers and acquisitions, partnerships, and investments. Creating knowledge graphs and network visualizations to provide a deeper understanding of financial relationships. Benefits of Using Transformer Models for Financial Analysis: High Accuracy: Transformer models achieve state-of-the-art performance in various NLP tasks, leading to more reliable and accurate analyses. Scalability: These models can handle large amounts of financial data, making them suitable for analyzing complex datasets. Adaptability: Transformers can be adapted to various financial analysis tasks by fine-tuning them on specific data and objectives. Automation: Automating tedious tasks like document summarization and sentiment analysis can save time and resources for financial professionals. Personalized Insights: Transformer-based systems can personalize financial information and recommendations based on individual needs and preferences. Challenges and Considerations: Data Requirements: Training accurate transformer models requires a large amount of labeled financial data, which can be expensive and time-consuming to acquire. Interpretability: The complex nature of transformer models can make it difficult to understand their decision-making process and interpret their results. Ethical Concerns: Bias and fairness issues in training data can lead to biased predictions and discriminatory outcomes. Model Explainability: Providing explanations for model predictions is crucial for building trust and understanding the rationale behind financial decisions. Overall, transformer models offer a powerful toolset for enhancing financial analysis by providing deeper insights, automating tasks, and personalizing information for diverse users. However, addressing data challenges, ensuring interpretability, and mitigating ethical concerns are crucial for responsible and successful implementation. Here are some resources to explore further: Financial Transformers Library: https://huggingface.co/models?other=finance Paper: Transformer-based Financial News Sentiment Analysis: https://arxiv.org/abs/2308.07935 Blog: Applying Transformers in Finance: https://www.mathworks.com/solutions/finance-and-risk-management/machine-learning.html By leveraging the capabilities of transformer models, financial analysts and institutions can unlock new opportunities for extracting valuable insights from vast amounts of financial data, leading to better decision-making and improved performance. Python Examples for Transformer Applications in Financial Analysis: \u00b6 Here are some examples showcasing the application of transformers for various financial analysis tasks: 1. Sentiment Analysis: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Example text text = \"Apple stock price is expected to rise sharply in the coming months.\" # Tokenize and encode the text inputs = tokenizer ( text , return_tensors = \"pt\" ) # Predict sentiment (positive, negative, neutral) outputs = model ( ** inputs ) predicted_sentiment = torch . nn . functional . softmax ( outputs . logits , dim =- 1 ) print ( f \"Predicted sentiment: { predicted_sentiment } \" ) This code snippet demonstrates sentiment analysis on a financial news article using the pre-trained FinBERT model. 2. Financial Text Summarization: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Example financial report text = \"The company's revenue increased by 20% year-over-year, driven by strong demand for its flagship product... The company expects to continue its growth trajectory in the next fiscal year...\" # Tokenize and encode the text inputs = tokenizer ( text , return_tensors = \"pt\" ) # Generate summary outputs = model . generate ( ** inputs ) # Decode and print the summary summary = tokenizer . decode ( outputs [ 0 ]) print ( f \"Summary: { summary } \" ) This code snippet shows how to summarize a financial report using the T5 model, which is pre-trained on a massive dataset of text and code. 3. Entity Recognition and Relationship Extraction: Python from transformers import AutoTokenizer , AutoModelForTokenClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Example text text = \"Apple announced a partnership with Google to develop new AI technologies.\" # Tokenize and encode the text inputs = tokenizer ( text , return_tensors = \"pt\" ) # Predict entity types (e.g., company, product, location) outputs = model ( ** inputs ) # Decode and extract entities and relationships predictions = torch . argmax ( outputs . logits , dim =- 1 ) entities = tokenizer . convert_ids_to_tokens ( predictions . tolist ()[ 0 ]) print ( f \"Entities: { entities } \" ) This code snippet demonstrates how to identify and extract entities and their relationships from financial news using a pre-trained NER model. 4. Question Answering System: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Example question and financial document question = \"What is the company's revenue in the last quarter?\" document = \"The company's revenue for the last quarter was $10 billion,...\" # Tokenize and encode the question and document inputs = tokenizer ( question , document , return_tensors = \"pt\" ) # Predict answer to the question outputs = model ( ** inputs ) # Decode and display the answer start_index = torch . argmax ( outputs . start_logits , dim =- 1 ) end_index = torch . argmax ( outputs . end_logits , dim =- 1 ) answer = tokenizer . decode ( inputs . input_ids [ 0 ][ start_index : end_index + 1 ]) print ( f \"Answer: { answer } \" ) This code snippet showcases a question-answering system built on the Longformer model, capable of answering questions about financial documents. These are just a few examples; many other applications for transformer models exist in the realm of financial analysis. As the technology evolves and more data becomes available, we can expect even more powerful and innovative applications emerging in the future. Additional Examples of Transformer Applications in Financial Analysis: \u00b6 5. Portfolio Optimization: Transformers can be used to analyze large datasets of financial instruments and identify optimal portfolio allocations. This can be done by learning the relationships between different assets and predicting their future performance. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Example financial data (historical prices, correlations, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict optimal portfolio allocation outputs = model ( ** inputs ) predicted_allocation = outputs . logits # Process and interpret the predicted allocation ... 6. Credit Risk Assessment: Transformers can be used to analyze loan applications and assess the risk of borrowers defaulting on their loans. This can be done by analyzing textual data in the loan application, such as income statements and credit reports. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Example loan application data (text and financial information) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict credit risk score outputs = model ( ** inputs ) # Decode and interpret the predicted score ... 7. Fraud Detection: Transformers can be used to detect fraudulent transactions in financial datasets. This can be done by analyzing patterns in transaction data and identifying anomalies. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Example transaction data (amount, timestamp, account information, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict fraudulent transactions outputs = model ( ** inputs ) # Decode and identify suspicious transactions ... 8. Regulatory Compliance: Transformers can be used to analyze financial documents and ensure compliance with regulations. This can be done by identifying and extracting relevant information from documents and checking for compliance with specific rules. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Example financial document (regulatory report, contract, etc.) document = ... # Tokenize and encode the document inputs = tokenizer ( document , return_tensors = \"pt\" ) # Identify and extract relevant information outputs = model ( ** inputs ) # Decode and compare extracted information with regulatory requirements ... These are just a few examples of the many potential applications of transformer models in financial analysis. As the technology continues to evolve, we can expect to see even more innovative and powerful applications emerge in the future. Additional Examples of Transformer Applications in Financial Analysis: \u00b6 9. High-Frequency Trading: Transformers can be used to analyze real-time market data and identify short-term trading opportunities. This can be done by learning patterns in order flow and trading activity, predicting price movements, and executing trades quickly. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Real-time market data feed (order book, trades, quotes, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict short-term price movements outputs = model ( ** inputs ) # Decode and interpret the predicted price movements ... 10. Market Anomaly Detection: Transformers can be used to identify unusual patterns in market data that may indicate potential anomalies or market manipulation. This can be done by analyzing large datasets of financial data and identifying deviations from historical norms. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Historical and real-time market data (prices, volumes, news, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict potential anomalies in market activity outputs = model ( ** inputs ) # Decode and identify potential anomalies for investigation ... 11. Algorithmic Trading: Transformers can be used to develop sophisticated algorithms for trading financial instruments. This can be done by combining the predictive power of transformers with other machine learning techniques to make data-driven trading decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Historical and real-time financial data, news, sentiment analysis data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate trading signals outputs = model ( ** inputs ) # Process and interpret the predicted signals ... 12. Investment Research: Transformers can be used to analyze vast amounts of textual data and identify potential investment opportunities. This can be done by analyzing company reports, news articles, and research papers to identify promising companies or trends. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Textual data (company reports, news articles, research papers) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and extract investment insights outputs = model ( ** inputs ) # Decode and analyze the extracted insights ... These additional examples demonstrate the diverse and powerful applications of transformers in financial analysis. As the field continues to evolve, we can expect even more innovative and impactful applications to emerge in the future. Other Examples of Transformer Applications in Financial Analysis: \u00b6 13. Risk Management: Transformers can be used to assess and manage various financial risks, including market risk, credit risk, and operational risk. This can be done by analyzing historical data, identifying potential risks, and developing mitigation strategies. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Historical financial data, news, regulatory updates, etc. data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict and analyze potential financial risks outputs = model ( ** inputs ) # Process and interpret the predicted risks ... 14. Algorithmic Insurance: Transformers can be used to develop algorithms for underwriting insurance policies, pricing risks, and managing claims. This can be done by analyzing vast amounts of data about policyholders, claims history, and risk factors. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Policyholder data, claims history, risk factors, etc. data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict insurance premiums and assess risk factors outputs = model ( ** inputs ) # Process and interpret the predicted premiums and risks ... 15. Financial Education and Literacy: Transformers can be used to develop personalized financial education resources and tools. This can be done by analyzing user data, identifying their financial goals and needs, and providing tailored learning materials and recommendations. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # User data, financial goals, questions, etc. data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial education content outputs = model ( ** inputs ) # Decode and deliver personalized learning materials and recommendations ... These additional examples illustrate the potential of transformers to revolutionize various aspects of the financial industry. By harnessing the power of these models, financial institutions and individuals can gain deeper insights, make better decisions, and navigate the complex financial landscape with greater confidence. It's important to note that the effectiveness of these applications depends on several factors, including data quality, model selection, and careful implementation. However, the potential benefits of using transformers in financial analysis are vast and far-reaching, shaping the future of how we interact with and manage our finances. Further Examples of Transformer Applications in Financial Analysis: \u00b6 16. Regulatory Technology (RegTech): Transformers can analyze vast amounts of legal and regulatory documents to identify compliance risks and automate tasks such as reporting and document review. This can help financial institutions comply with regulations more efficiently and cost-effectively. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Regulatory documents, financial data, legal reports data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify potential regulatory violations and risks outputs = model ( ** inputs ) # Decode and analyze the extracted information ... 17. Cybercrime Detection and Prevention: Transformers can be used to analyze financial transactions and identify patterns indicative of suspicious activity or fraud. This can help financial institutions detect and prevent cybercrime attempts and protect their customers' assets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Transaction data, account information, user behavior patterns data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify anomalies and potential cybercrime activity outputs = model ( ** inputs ) # Decode and investigate suspicious transactions ... 18. Financial Chatbots and Virtual Assistants: Transformers can power financial chatbots and virtual assistants that can answer users' questions about financial products and services, provide personalized recommendations, and assist with managing their finances. This can improve customer service and accessibility to financial information, especially for digitally-savvy users. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # User queries, historical financial data, product information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Answer user questions and provide financial guidance outputs = model ( ** inputs ) # Decode and deliver personalized financial insights and assistance ... 19. Democratizing financial data and analysis: Transformers can help make financial data and analysis more accessible to everyone, not just professional investors. By simplifying complex information and providing personalized insights, transformers can empower individuals to make informed financial decisions and participate in the financial markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial news articles, market data, user preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate summaries and explanations of financial news, trends, and concepts outputs = model ( ** inputs ) # Decode and deliver simplified financial information tailored to user needs ... These additional examples showcase the expansive potential of transformers in shaping the future of financial technology and democratizing access to financial knowledge and tools. As research and development continue, we can expect to see even more innovative and impactful applications emerge, transforming the way we interact with and manage our finances. It's important to remember that responsible development and ethical considerations are crucial for ensuring that the benefits of these powerful technologies are broadly shared and utilized for good. Additional Transformer Applications in Financial Analysis: \u00b6 20. Algorithmic Trading for Alternative Assets: Transformers can analyze and predict price movements of non-traditional assets like cryptocurrencies, commodities, and real estate. This can help investors make informed decisions and capitalize on emerging opportunities in these asset classes. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Market data, news articles, social media posts about alternative assets data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict price movements and identify trading opportunities outputs = model ( ** inputs ) # Process and interpret the predicted movements and risks ... 21. Personalized Financial Planning: Transformers can analyze users' financial data, goals, and risk tolerance to create personalized financial plans. This can help individuals achieve their financial goals by providing tailored recommendations and guidance. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # User data, financial goals, risk tolerance, financial history data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial plans and recommendations outputs = model ( ** inputs ) # Decode and deliver tailored financial strategies for individuals ... 22. Financial Market Forecasting: Transformers can analyze historical financial data and identify patterns to forecast future market trends. This can help investors and financial institutions make informed decisions about their investments and asset allocation strategies. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Historical market data, economic indicators, news articles data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict future market trends and identify potential risks and opportunities outputs = model ( ** inputs ) # Decode and analyze the predicted trends and market movements ... 23. Financial Stress Detection and Intervention: Transformers can analyze social media data and other forms of digital communication to identify individuals experiencing financial stress. This can help provide early intervention and support services to individuals facing financial difficulties. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Social media posts, financial conversations, user behavior data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify individuals exhibiting signs of financial stress outputs = model ( ** inputs ) # Decode and utilize insights to provide targeted financial support and advice ... 24. ESG Investing and Impact Analysis: Transformers can analyze vast amounts of social, environmental, and governance (ESG) data to assess the sustainability and ethical impact of investments. This can help investors make informed decisions that align with their values and contribute to a more sustainable future. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # ESG data, company reports, news articles, regulatory documents data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze ESG performance and impact of companies and investments outputs = model ( ** inputs ) # Decode and provide insights for making informed ethical investments ... These diverse examples demonstrate the expansive potential of transformers in revolutionizing how we interact with and analyze complex financial information. By unlocking deeper insights and automating tasks, transformers can empower individuals, financial institutions, and regulators to make better decisions, promote financial inclusion, and ensure a more sustainable and responsible financial system. As research and development in this field continue, we can expect even More Transformer Applications in Financial Analysis: \u00b6 25. Financial Fraud Detection: Transformers can analyze financial transactions and identify patterns that indicate fraudulent activity. This can help financial institutions detect and prevent fraud attempts, saving them substantial financial losses. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Financial transactions, user profiles, historical fraud data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and predict fraudulent transactions and anomalies outputs = model ( ** inputs ) # Decode and analyze suspicious transactions for further investigation ... 26. Tax Optimization: Transformers can analyze tax laws and regulations, identify deductions and credits, and suggest optimal tax-saving strategies. This can help individuals and businesses minimize their tax liabilities and maximize their tax benefits. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Tax codes, regulations, financial data, user income and expenses data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized tax optimization strategies and recommendations outputs = model ( ** inputs ) # Decode and assist users in optimizing their tax liabilities ... 27. Market Abuse Detection: Transformers can analyze large datasets of market data and identify patterns that indicate market manipulation or insider trading. This can help regulators detect and prevent market abuse, ensuring fair and efficient markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Market data, trading orders, social media chatter, news articles data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and predict potential market abuse activities outputs = model ( ** inputs ) # Decode and investigate suspicious activities for regulatory action ... 28. Asset Valuation and Pricing: Transformers can analyze financial data and company fundamentals to generate valuations and price estimates for various assets, including stocks, bonds, and real estate. This can help investors make informed decisions and ensure they are paying fair prices for assets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, company reports, market data, industry benchmarks data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate asset valuations and price estimates outputs = model ( ** inputs ) # Decode and deliver insights for informed investment decisions ... 29. Financial News Recommendation: Transformers can personalize financial news feeds based on individual user preferences and investment interests. This provides users with relevant and timely information, allowing them to stay informed and make informed decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # User preferences, investment interests, past news consumption data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Recommend personalized financial news articles and updates outputs = model ( ** inputs ) # Decode and deliver tailored news feeds for informed investment decisions ... These additional examples highlight the vast potential of transformers in shaping the future of financial analysis and decision-making. By leveraging their ability to process and analyze complex information, transformers can empower individuals, financial institutions, and regulators to navigate the ever-evolving financial landscape with greater accuracy, efficiency, and ethical responsibility. As research and development continue to advance, we can expect transformers to become increasingly sophisticated and integrated into the core fabric of financial systems, revolutionizing the way Expanding the Scope of Transformer Applications in Financial Analysis: \u00b6 30. Generating Financial Reports: Transformers can be used to automatically generate financial reports, such as income statements and balance sheets. This can save companies time and resources, and ensure accuracy and consistency in reporting. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, historical reports, industry benchmarks data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate financial reports outputs = model ( ** inputs ) # Decode and format the generated reports ... 31. Customer Financial Health Assessment: Transformers can analyze customer data, such as income, expenses, and credit history, to assess their financial health and creditworthiness. This can help financial institutions make informed lending decisions and offer personalized financial products. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Customer data, financial transactions, credit history data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict customer financial health and creditworthiness outputs = model ( ** inputs ) # Decode and analyze the predicted financial health for personalized offers ... 32. Financial Education and Literacy Programs: Transformers can be used to develop personalized financial education and literacy programs that cater to individual needs and learning styles. This can help individuals make informed financial decisions and achieve their financial goals. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # User financial knowledge level, learning style, financial goals data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial education content and learning paths outputs = model ( ** inputs ) # Decode and deliver tailored learning materials for improved financial literacy ... 33. Financial Time Series Forecasting: Transformers can analyze historical financial time series data to predict future trends and market movements. This can help investors and financial institutions make informed decisions about asset allocation and risk management. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Financial time series data, historical trends, economic indicators data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict future financial time series trends and market movements outputs = model ( ** inputs ) # Decode and analyze the predicted trends for informed financial decisions ... 34. Financial Data Visualization and Storytelling: Transformers can be used to generate insightful visualizations and narratives based on complex financial data. This can help users understand complex financial concepts and make informed decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, market trends, user preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate visualizations and narratives explaining financial data and trends outputs = model ( ** inputs ) # Decode and present financial insights in an engaging and informative way ... These examples showcase the diverse and innovative ways transformers can be employed in the financial domain. As technology advancements continue, we can anticipate even more sophisticated applications, leading to a future where transformers are seamlessly integrated into the financial ecosystem, empowering individuals and institutions to make informed decisions, achieve financial goals, and navigate the ever-evolving financial landscape with greater confidence and efficiency. Additional Potential Applications of Transformers in Financial Analysis: \u00b6 35. Decentralized Finance (DeFi) Analysis: Transformers can analyze on-chain data and smart contracts to assess the risks and opportunities associated with DeFi protocols and investments. This can help investors make informed decisions about participating in the DeFi ecosystem. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # On-chain data, smart contracts, DeFi protocol information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze smart contracts and assess DeFi protocol risks and opportunities outputs = model ( ** inputs ) # Decode and present insights for informed DeFi investment decisions ... 36. Algorithmic Portfolio Management with Reinforcement Learning: Transformers can be combined with reinforcement learning algorithms to create intelligent agents that manage investment portfolios. These agents can learn from market data and adapt their strategies over time to maximize returns and minimize risk. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Market data, historical portfolio performance, risk tolerance data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and optimize portfolio strategies using reinforcement learning outputs = model ( ** inputs ) # Decode and implement the optimized portfolio strategies ... 37. Financial Risk Modeling and Stress Testing: Transformers can be used to analyze vast amounts of financial data and model complex financial risks. This can help financial institutions stress test their portfolios and develop risk mitigation strategies. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, economic indicators, historical stress events data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and analyze financial risk models and stress test results outputs = model ( ** inputs ) # Decode and utilize insights to develop effective risk mitigation strategies ... 38. Anticipating Financial Crises: Transformers can analyze large datasets of financial and social media data to identify early warning signs of financial crises. This can help policymakers and financial institutions prepare for and mitigate the impact of crises. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, social media trends, economic indicators data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify early warning signs of potential financial crises outputs = model ( ** inputs ) # Decode and analyze the predicted risks to inform preventive measures ... 39. Financial Data Cleaning and Preprocessing: Transformers can be used to automate the cleaning and preprocessing of financial data, which is a crucial step in data analysis and modeling. This can save time and resources for financial analysts and researchers. Example: Python from transformers import AutoTokenizer , AutoModelForTokenClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Raw financial data, financial terms and entities data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and correct errors in the financial data outputs = model ( ** inputs ) # Decode and deliver clean and preprocessed financial data for analysis ... 40. Optimizing Financial Fraud Detection Systems: Transformers can be used to improve the accuracy and efficiency of financial fraud detection systems. This can help protect financial institutions and individuals from financial losses. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained ## Further Expanding the Scope of Transformers in Financial Analysis: ** 41. Personalized Debt Management and Debt Relief Options : ** Transformers can analyze individual financial data and recommend personalized debt management strategies and potential debt relief options . This can help individuals struggling with debt develop effective plans to manage their finances and achieve financial freedom . ** Example : ** ``` python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Individual financial data, debt obligations, income and expenses data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze debt situation and recommend personalized management strategies outputs = model ( ** inputs ) # Decode and recommend personalized debt relief options and solutions ... 42. Financial Regulatory Compliance and Reporting: Transformers can be used to automate financial regulatory compliance and reporting processes. This can save financial institutions time and resources and reduce the risk of regulatory violations. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, regulatory requirements, reporting templates data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and complete regulatory reports automatically outputs = model ( ** inputs ) # Decode and deliver compliant and accurate financial reports ... 43. Financial Customer Service and Chatbots: Transformers can power financial chatbots that can answer customer questions about financial products and services, troubleshoot issues, and provide personalized financial advice. This can improve customer service and reduce costs for financial institutions. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Customer queries, financial product information, FAQs data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Train and deploy financial chatbots for customer service and support outputs = model ( ** inputs ) # Decode and deliver accurate and personalized responses to customer inquiries ... 44. Building Financial Data Marketplaces: Transformers can facilitate the creation of efficient and secure financial data marketplaces where data providers and consumers can interact and exchange data. This can democratize access to financial data and accelerate financial innovation. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data provider information, data consumer requests, security protocols data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Facilitate secure and efficient financial data exchange and transactions outputs = model ( ** inputs ) # Decode and connect data providers and consumers for mutual benefit ... 45. Financial Education for Underserved Communities: Transformers can be used to develop culturally relevant and accessible financial education programs for underserved communities. This can help improve financial literacy and empower individuals to make informed financial decisions. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Financial information, cultural context, language preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate culturally relevant and accessible financial education materials outputs = model ( ** inputs ) # Decode and deliver personalized financial education in culturally appropriate ways ... These examples showcase the vast potential of transformers in shaping the future of financial services and empowering individuals to make informed financial decisions. As research and development continue, we can expect to see even more innovative and impactful applications emerge, fostering a more inclusive and equitable financial system for all. It's important to remember that the success of these applications hinges on responsible development, ethical considerations Additional Examples of Transformer Applications in Financial Analysis: \u00b6 46. Crypto-asset Analysis: Transformers can analyze cryptocurrency data, including blockchain transactions, social media sentiment, and news articles, to identify trends, predict price movements, and evaluate the feasibility and potential of new projects. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Cryptocurrency data, social media trends, news articles, project information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze cryptocurrency data and identify potential investment opportunities outputs = model ( ** inputs ) # Decode and extract insights for informed cryptocurrency investment decisions ... 47. Financial Market Simulation and Optimization: Transformers can be used to create realistic simulations of financial markets, allowing financial institutions to test and optimize their trading strategies and risk management models. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"ProsusAI/finbert\" ) # Historical market data, economic indicators, trading strategies data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and analyze simulations of different market scenarios outputs = model ( ** inputs ) # Decode and optimize trading strategies based on simulated market outcomes ... 48. Insurance Risk Assessment and Pricing: Transformers can analyze individual and historical data to assess insurance risks and personalize insurance premiums. This can lead to fairer and more efficient insurance markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Individual data, insurance history, medical records, risk factors data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Assess individual insurance risks and generate personalized premium quotes outputs = model ( ** inputs ) # Decode and deliver individualized insurance plans and pricing ... 49. Financial Data Summarization and Reporting: Transformers can automatically summarize and generate reports based on large datasets of financial data, reducing the time and resources required for financial analysts and auditors. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, reports, audit requirements data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate concise and informative financial summaries and reports outputs = model ( ** inputs ) # Decode and deliver summaries and reports for efficient analysis and decision-making ... 50. Financial Contract Review and Negotiation: Transformers can analyze and extract key information from financial contracts, helping lawyers and financial professionals understand the terms and negotiate more effectively. Example: Python from transformers import AutoTokenizer , AutoModelForTokenClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial contracts, legal terms, negotiation history data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify key clauses, extract relevant information, and facilitate contract analysis outputs = model ( ** inputs ) # Decode and assist with contract review and negotiation processes ... These examples demonstrate the vast potential of transformers in revolutionizing the financial industry across various sectors. By automating tasks, analyzing complex data, and generating insights, transformers can empower financial institutions, individuals, and regulators to make informed decisions, optimize processes, and navigate the ever-evolving financial landscape with greater efficiency and effectiveness. It's crucial to remember that the responsible development and ethical implementation of these applications are essential for ensuring that the benefits of transformers are distributed equitably and contribute to a more sustainable and inclusive financial ecosystem for Further Expanding the Scope of Transformer Applications in Financial Analysis: \u00b6 51. Real Estate Market Prediction: Transformers can analyze real estate data, including market trends, property listings, and economic indicators, to predict future market movements and identify potential investment opportunities. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Real estate data, market trends, economic indicators, property details data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict future real estate market trends and identify investment opportunities outputs = model ( ** inputs ) # Decode and provide insights for informed real estate investment decisions ... 52. Personal Finance Planning and Budgeting: Transformers can analyze individual financial data, spending habits, and goals to create personalized financial plans and budgets. This can help individuals manage their finances more effectively and achieve their financial goals. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Individual income, expenses, financial goals, financial data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial plans and budgeting recommendations outputs = model ( ** inputs ) # Decode and deliver tailored financial planning insights and strategies ... 53. Financial Inclusion and Access to Credit: Transformers can be used to develop alternative credit scoring models that can assess the creditworthiness of individuals who lack traditional credit history. This can help expand financial inclusion and provide access to credit for underserved communities. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Individual financial data, alternative data sources, social media activity data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Develop alternative credit scoring models for financial inclusion outputs = model ( ** inputs ) # Decode and use alternative credit scores for fairer and accessible credit decisions ... 54. Financial Communication and Content Creation: Transformers can be used to generate clear, concise, and engaging financial content, such as news articles, blog posts, and investment reports. This can make financial information more accessible to a wider audience. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, market trends, target audience preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate engaging and informative financial communication content outputs = model ( ** inputs ) # Decode and deliver tailored financial content to various audiences ... 55. Cybersecurity and Fraud Detection in Financial Services: Transformers can analyze financial transactions and other data sources to detect fraudulent activities and protect financial institutions and individuals from cybercrime. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial transactions, user behavior data, cybersecurity threats data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Detect and prevent cybersecurity threats and fraudulent activities outputs = model ( ** inputs ) # Decode and analyze suspicious activities for further investigation and mitigation ... These examples showcase the transformative potential of transformers in shaping the future of financial services and promoting a more inclusive, accessible, and secure financial ecosystem for all. As research and development progress, we can expect even more innovative applications to emerge, further transforming the financial landscape and empowering individuals and institutions to navigate the financial world with greater clarity, confidence, and efficiency. Additional Applications of Transformers in Financial Analysis: \u00b6 56. Sustainability and Impact Investing: Transformers can analyze companies' environmental, social, and governance (ESG) data to identify sustainable investments and assess the impact of investments on society and the environment. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Company ESG data, sustainability reports, investment goals data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify sustainable investment opportunities and assess their social impact outputs = model ( ** inputs ) # Decode and provide insights for informed sustainable investment decisions ... 57. Financial Education and Gamification: Transformers can be used to develop interactive and engaging games that teach users about financial concepts and make financial education more accessible and enjoyable. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial concepts, game mechanics, user preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate interactive financial education games for engaging learning outputs = model ( ** inputs ) # Decode and deliver personalized and gamified financial education experiences ... 58. Financial Regulatory Compliance and Automation: Transformers can automate the process of compliance with financial regulations, reducing the burden on financial institutions and improving accuracy and efficiency. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Regulatory rules and requirements, financial data, compliance reports data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Automate regulatory compliance processes and identify potential violations outputs = model ( ** inputs ) # Decode and generate reports and take corrective actions for compliance ... 59. Financial Data Privacy and Security: Transformers can be used to develop new methods for protecting financial data privacy and security, such as text anonymization and homomorphic encryption. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, privacy requirements, security protocols data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate anonymized financial data for improved data privacy outputs = model ( ** inputs ) # Decode and deliver secure and privacy-preserving financial data analysis ... 60. Financial Market Regulation and Supervision: Transformers can be used by regulators to analyze financial data and identify potential market manipulation or systemic risk, ensuring the stability and fairness of financial markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial market data, regulations, market surveillance data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Detect and analyze potential market manipulation and systemic risks outputs = model ( ** inputs ) # Decode and inform regulatory actions for a stable and fair financial market ... These additional examples showcase the vast potential of transformers in shaping the future of financial analysis and beyond. As research and development continue to advance, we can expect to see even more innovative and impactful applications emerge, leading to a more inclusive, efficient, and sustainable financial ecosystem for all. Further Expanding the Scope of Transformer Applications in Financial Analysis: \u00b6 61. Personalized Investment Portfolio Optimization: Transformers can be used to tailor investment portfolios to individual investor risk profiles and financial goals. They can analyze historical market data, current market trends, and individual preferences to recommend personalized investment strategies and asset allocations. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Investor risk profile, financial goals, market data, asset information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized investment portfolio recommendations outputs = model ( ** inputs ) # Decode and deliver tailored investment strategies for optimal returns ... 62. Financial Stress Prediction and Early Warning Systems: Transformers can analyze individual economic and financial data to predict financial stress and identify individuals at risk of financial hardship. This information can be used to develop early warning systems and provide timely interventions to prevent financial distress. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Individual financial data, economic indicators, credit history, employment data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict individual financial stress and identify risk factors outputs = model ( ** inputs ) # Decode and provide early warnings and interventions for financial risk mitigation ... 63. Financial Market Microstructure Analysis: Transformers can analyze high-frequency trading data and order book snapshots to gain insights into market microstructure, including order flow, liquidity, and trading patterns. This information can be used to develop algorithmic trading strategies and improve market efficiency. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # High-frequency trading data, order book snapshots, market microstructure features data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze market microstructure and identify trading opportunities outputs = model ( ** inputs ) # Decode and generate insights for informed algorithmic trading strategies ... 64. Financial Crisis Prediction and Mitigation: Transformers can analyze large datasets of historical financial data, news articles, and social media sentiment to predict financial crises and identify potential risk factors. This information can be used to develop mitigation strategies and prepare for potential economic downturns. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, news articles, social media sentiment, historical crises data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict financial crisis risks and identify early warning signs outputs = model ( ** inputs ) # Decode and inform crisis mitigation strategies and preparedness measures ... 65. Financial Data Explainability and Interpretability: Transformers can be used to explain the rationale behind their predictions and recommendations, making them more transparent and interpretable for financial analysts and investors. This can help build trust in AI-driven financial applications and enable better decision-making. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, model predictions, desired explanation level data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate explanations for model predictions and recommendations outputs = model ( ** inputs ) # Decode and provide clear and understandable explanations for improved trust and decision-making ... These additional examples showcase the diverse and innovative ways transformers can be employed in the financial domain. As Further Expanding the Scope of Transformer Applications in Financial Analysis: \u00b6 66. Financial Forensics and Fraud Investigation: Transformers can analyze financial transactions and communication data to identify patterns and anomalies that may indicate fraudulent activities. This can assist financial institutions in investigating fraud, recovering stolen assets, and preventing future incidents. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Financial transactions, communication data, fraud indicators, historical case data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and investigate potential financial fraud and suspicious activities outputs = model ( ** inputs ) # Decode and provide insights for effective fraud mitigation and investigation ... 67. Financial Risk Management and Stress Testing: Transformers can analyze complex financial data and models to assess risk exposures and vulnerabilities within financial institutions. This information can be used to develop risk management strategies, conduct stress tests, and ensure the financial stability of institutions. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, risk models, stress test scenarios, regulatory requirements data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze and assess financial risks, conduct stress tests, and develop mitigation strategies outputs = model ( ** inputs ) # Decode and provide actionable insights for improved risk management and financial stability ... 68. Financial Market Anomaly Detection and Prediction: Transformers can analyze historical market data and identify unusual patterns and deviations that may be indicative of market anomalies. This information can be used to predict market crashes, identify opportunities for arbitrage, and develop risk management strategies. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Historical market data, trading patterns, anomaly detection algorithms data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and predict market anomalies for informed trading and risk management outputs = model ( ** inputs ) # Decode and provide alerts and insights for proactive action in response to market anomalies ... 69. Financial Data Integration and Interoperability: Transformers can be used to facilitate the integration and interoperability of financial data from diverse sources. This can help break down information silos and enable financial institutions to gain a holistic view of their financial data, leading to improved decision-making and operational efficiency. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data from diverse sources, data integration protocols, interoperability standards data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Integrate and interoperate financial data for comprehensive analysis and insights outputs = model ( ** inputs ) # Decode and deliver unified financial data for enhanced decision-making and efficiency ... 70. Financial Data Augmentation and Generation: Transformers can be used to generate synthetic financial data that can be used for training and evaluating financial models, testing hypotheses, and exploring various scenarios. This can be particularly useful for augmenting limited datasets and improving modelgeneralizability. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Existing financial data, desired data augmentation parameters and specifics data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate synthetic financial data for enhanced model training and evaluation outputs = model ( ** inputs ) # Decode and utilize synthetic data for informed financial ## Additional Examples of Transformer Applications in Financial Analysis: ** 71. Personalized Tax Planning and Optimization : ** Transformers can analyze individual income , expenses , and investments to recommend personalized tax planning strategies and optimize tax liabilities . This can help individuals save money on taxes and ensure compliance with tax regulations . ** Example : ** ``` python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Individual income data, expense data, investment information, tax laws data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized tax planning recommendations and optimize tax liabilities outputs = model ( ** inputs ) # Decode and deliver tailored tax strategies for individuals ... 72. Financial Product Recommendation and Targeting: Transformers can analyze individual financial data and preferences to recommend personalized financial products and services. This can help financial institutions provide relevant offerings to their customers and improve customer satisfaction and retention. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Individual financial data, preferences, product information, target audience data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Recommend personalized financial products and target relevant customer segments outputs = model ( ** inputs ) # Decode and deliver targeted financial product offerings for increased customer satisfaction ... 73. Financial Market Sentiment Analysis and Prediction: Transformers can analyze financial news articles, social media sentiment, and other textual data to gauge overall market sentiment and predict future market movements. This can be valuable information for investors and traders seeking to make informed investment decisions. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Financial news articles, social media data, historical market data, sentiment analysis algorithms data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze market sentiment and predict future market movements outputs = model ( ** inputs ) # Decode and provide insights for sentiment-driven investment strategies ... 74. Financial Regulatory Compliance Automation and Reporting: Transformers can automate the process of generating regulatory reports and ensuring compliance with financial regulations. This can save financial institutions time and resources while reducing the risk of regulatory violations. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, regulatory requirements, reporting templates, compliance standards data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and automate regulatory reports for improved compliance and efficiency outputs = model ( ** inputs ) # Decode and deliver accurate and compliant financial reports to regulatory authorities ... 75. Financial Data Democratization and Accessibility: Transformers can be used to create user-friendly interfaces and tools that allow individuals with varying levels of financial expertise to access and analyze financial data. This can promote financial literacy and empower individuals to make informed financial decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, user interface design, accessibility features, financial literacy resources data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Develop user-friendly financial analysis tools for broader accessibility and empowerment outputs = model ( ** inputs ) # Decode and deliver financial information in an accessible and understandable way ... These diverse examples showcase the immense potential of transformers in revolutionizing the financial landscape. As research and development continue to progress, we can expect even more innovative applications to emerge,","title":"Fourier Transform in Price"},{"location":"finance/use_of_ft/#fourier-transform-in-price","text":"","title":"Fourier Transform in Price"},{"location":"finance/use_of_ft/#transformers-for-financial-analysis-exploring-the-potential","text":"This document summarizes our conversation on the potential of transformers in revolutionizing the field of financial analysis. We discussed various applications and explored how these powerful language models can be used to gain insights, improve decision-making, and automate tasks across the financial landscape. Table of Contents: Introduction Overview of Transformers Applications in Financial Analysis Portfolio Optimization Financial Stress Prediction Market Microstructure Analysis Financial Crisis Prediction Explainability and Interpretability Fraud Investigation and Risk Management Market Anomaly Detection Data Integration and Interoperability Data Augmentation and Generation Additional Applications Model Examples and Resources Dockerfile and Project Files for Testing Sample Test Code Downloadable Markdown File Introduction: Transformers are a class of deep learning models that have achieved remarkable results in various natural language processing tasks. Their ability to process and understand complex language makes them valuable tools for analyzing financial data and extracting valuable insights. Overview of Transformers: Transformers are based on the concept of \"attention,\" which allows them to focus on relevant parts of the input data. This enables them to learn complex relationships between words and sentences, leading to superior performance in tasks like sentiment analysis, question answering, and text summarization. Applications in Financial Analysis: Transformers can be applied to a wide range of tasks in financial analysis, including: Portfolio Optimization: Personalized investment portfolios tailored to individual risk profiles and financial goals. Financial Stress Prediction: Early warnings and interventions for individuals at risk of financial hardship. Market Microstructure Analysis: Identifying trading opportunities and developing algorithmic trading strategies. Financial Crisis Prediction: Proactive preparation and mitigation strategies for potential economic downturns. Explainability and Interpretability: Building trust and confidence in AI-driven financial applications. Fraud Investigation and Risk Management: Identifying suspicious activities and preventing financial losses. Market Anomaly Detection: Predicting market crashes and identifying opportunities for arbitrage. Data Integration and Interoperability: Combining information from diverse sources for comprehensive analysis. Data Augmentation and Generation: Enhancing model training and evaluation by creating synthetic data. Additional Applications: Personalized Tax Planning and Optimization: Recommending customized tax strategies for individuals. Financial Product Recommendation and Targeting: Providing relevant offerings to customers based on their preferences. Financial Market Sentiment Analysis and Prediction: Making informed investment decisions based on market sentiment. Financial Regulatory Compliance Automation and Reporting: Generating regulatory reports and ensuring compliance. Financial Data Democratization and Accessibility: Empowering individuals to make informed financial decisions. Model Examples and Resources: This document lists various pre-trained transformer models suitable for financial tasks, along with information about their functionalities and access points: Model Name Description Where to Get Details ProsusAI/finbert Finetuned for financial text data. Hugging Face Sentiment analysis, entity recognition, classification. t5-base Text-to-text transfer transformer Hugging Face Summarization, question answering, translation. nlpaueb/bert-base-ner Finetuned for named entity recognition Hugging Face Identifying and classifying entities in financial text. allenai/longformer-base-4096 Long sequences of text Hugging Face Analyzing financial documents and reports. bigscience/bloom-176b Large language model Hugging Face Complex tasks like creative text formats, translation, and informative answers. Dockerfile and Project Files for Testing: A Dockerfile and project files are provided to set up a testing environment for exploring these transformers. The files are downloadable and include instructions for building and running the container. Sample Test Code: Example test scripts demonstrate how to perform simple tests with the FinBert model. These examples can be adapted to test other transformers based on their functionalities. Downloadable Markdown File: This markdown file, containing the entire conversation, is downloadable for future reference and sharing. Conclusion: Transformers hold tremendous potential for transforming the way we analyze and operate within the financial world. Their ability to process large amounts of data, extract insights, and automate tasks can lead to improved decision-making, increased efficiency, and broader access to financial services. As research and development continue to advance, we can expect even more innovative applications to emerge, further shaping the future of finance.","title":"Transformers for Financial Analysis: Exploring the Potential"},{"location":"finance/use_of_ft/#dockerfile-for-transformer-testing-environment","text":"This Dockerfile creates an environment suitable for testing various transformers listed above. Text Only FROM python:3.9 WORKDIR /app # Install basic dependencies RUN apt-get update && apt-get install -y curl unzip # Install PyTorch RUN pip install torch torchvision torchaudio # Install Transformers library RUN pip install transformers # Download and install additional dependencies for specific transformers RUN pip install -r requirements.txt # Copy project files COPY . . # Run your test scripts CMD [\"python\", \"run_tests.py\"]","title":"Dockerfile for Transformer Testing Environment"},{"location":"finance/use_of_ft/#project-file-requirementstxt","text":"This file lists additional dependencies needed for specific transformers mentioned in previous examples. Text Only # ProsusAI/finbert transformers[tokenizers]==4.21.0 torch-optimizer==0.9.0 # allenai/longformer-base-4096 tqdm==4.64.0 # bigscience/bloom-176b datasets==2.0.0","title":"Project File (requirements.txt)"},{"location":"finance/use_of_ft/#usage","text":"Build the Docker image: Text Only docker build -t transformers-testing . Run the Docker container: Text Only docker run -it transformers-testing This will place you inside the container with all necessary dependencies installed. You can now run your tests or experiment with different transformers. To stop the container: Text Only docker stop <container_id> To remove the container: Text Only docker rm <container_id> Note: Remember to replace <container_id> with the actual ID of your container.","title":"Usage"},{"location":"finance/use_of_ft/#run_testspy","text":"Python import unittest # Import specific tests for each transformer from tests.test_finbert import TestFinBert from tests.test_t5_base import TestT5Base from tests.test_bert_base_ner import TestBertBaseNER from tests.test_longformer_base_4096 import TestLongformerBase4096 from tests.test_bloom_176b import TestBloom176B # Create a test suite suite = unittest . TestSuite () # Add tests for each transformer suite . addTest ( unittest . makeSuite ( TestFinBert )) suite . addTest ( unittest . makeSuite ( TestT5Base )) suite . addTest ( unittest . makeSuite ( TestBertBaseNER )) suite . addTest ( unittest . makeSuite ( TestLongformerBase4096 )) suite . addTest ( unittest . makeSuite ( TestBloom176B )) # Run the test suite runner = unittest . TextTestRunner () runner . run ( suite )","title":"run_tests.py"},{"location":"finance/use_of_ft/#test-code-for-individual-transformers","text":"This example shows test code for the FinBert model. Similar test scripts should be created for other transformers mentioned earlier. Python # tests/test_finbert.py from transformers import AutoTokenizer , AutoModelForSequenceClassification # Define the FinBert model and tokenizer model_name = \"ProsusAI/finbert\" tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForSequenceClassification . from_pretrained ( model_name ) class TestFinBert ( unittest . TestCase ): def test_sentiment_analysis ( self ): # Define test sentence sentence = \"The company's financial performance is strong and the future looks bright.\" # Encode the sentence inputs = tokenizer ( sentence , return_tensors = \"pt\" ) # Predict the sentiment outputs = model ( ** inputs ) predicted_sentiment = outputs . logits . argmax ( - 1 ) . item () # Assert that the predicted sentiment is positive self . assertEqual ( predicted_sentiment , 1 ) # Add further tests for other FinBert functionalities # ... Note: Replace the FinBert tests with specific test cases for other transformers mentioned earlier. Remember to adapt the code based on the functionalities and tasks each transformer is designed for.","title":"Test Code for Individual Transformers"},{"location":"finance/use_of_ft/#transformer-models-for-financial-analysis","text":"Transformer models, a type of deep learning architecture, have gained immense popularity in natural language processing (NLP) tasks due to their remarkable ability to capture long-range dependencies and context in text data. These same capabilities can be leveraged to enhance financial analysis in several ways: 1. Sentiment Analysis: Analyzing financial news articles, social media posts, and other textual data to gauge market sentiment and identify potential trends. Identifying bullish or bearish sentiment surrounding specific companies or assets based on their textual mentions. Quantifying the sentiment of earnings reports and conference calls to understand investor sentiment towards a company. 2. Financial Text Summarization: Automatically generating concise summaries of financial reports, research papers, and other lengthy documents. Extracting key insights and actionable information from complex financial documents for faster and more efficient analysis. Providing a quick overview of financial news and events for busy professionals. 3. Financial Text Generation: Generating synthetic financial reports, market commentaries, and other relevant text based on historical data and trends. Creating automated reports and updates for financial institutions and investment firms. Personalizing financial news and information for individual investors based on their preferences and risk tolerance. 4. Question Answering Systems: Developing financial chatbots and virtual assistants that can answer questions about specific companies, markets, and financial concepts. Providing on-demand access to financial information and insights for investors and analysts. Enhancing customer service and support for financial institutions through interactive question-answering systems. 5. Entity Recognition and Relationship Extraction: Identifying and extracting key entities like companies, individuals, and locations from financial documents. Uncovering relationships between entities, such as mergers and acquisitions, partnerships, and investments. Creating knowledge graphs and network visualizations to provide a deeper understanding of financial relationships. Benefits of Using Transformer Models for Financial Analysis: High Accuracy: Transformer models achieve state-of-the-art performance in various NLP tasks, leading to more reliable and accurate analyses. Scalability: These models can handle large amounts of financial data, making them suitable for analyzing complex datasets. Adaptability: Transformers can be adapted to various financial analysis tasks by fine-tuning them on specific data and objectives. Automation: Automating tedious tasks like document summarization and sentiment analysis can save time and resources for financial professionals. Personalized Insights: Transformer-based systems can personalize financial information and recommendations based on individual needs and preferences. Challenges and Considerations: Data Requirements: Training accurate transformer models requires a large amount of labeled financial data, which can be expensive and time-consuming to acquire. Interpretability: The complex nature of transformer models can make it difficult to understand their decision-making process and interpret their results. Ethical Concerns: Bias and fairness issues in training data can lead to biased predictions and discriminatory outcomes. Model Explainability: Providing explanations for model predictions is crucial for building trust and understanding the rationale behind financial decisions. Overall, transformer models offer a powerful toolset for enhancing financial analysis by providing deeper insights, automating tasks, and personalizing information for diverse users. However, addressing data challenges, ensuring interpretability, and mitigating ethical concerns are crucial for responsible and successful implementation. Here are some resources to explore further: Financial Transformers Library: https://huggingface.co/models?other=finance Paper: Transformer-based Financial News Sentiment Analysis: https://arxiv.org/abs/2308.07935 Blog: Applying Transformers in Finance: https://www.mathworks.com/solutions/finance-and-risk-management/machine-learning.html By leveraging the capabilities of transformer models, financial analysts and institutions can unlock new opportunities for extracting valuable insights from vast amounts of financial data, leading to better decision-making and improved performance.","title":"Transformer Models for Financial Analysis"},{"location":"finance/use_of_ft/#python-examples-for-transformer-applications-in-financial-analysis","text":"Here are some examples showcasing the application of transformers for various financial analysis tasks: 1. Sentiment Analysis: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Example text text = \"Apple stock price is expected to rise sharply in the coming months.\" # Tokenize and encode the text inputs = tokenizer ( text , return_tensors = \"pt\" ) # Predict sentiment (positive, negative, neutral) outputs = model ( ** inputs ) predicted_sentiment = torch . nn . functional . softmax ( outputs . logits , dim =- 1 ) print ( f \"Predicted sentiment: { predicted_sentiment } \" ) This code snippet demonstrates sentiment analysis on a financial news article using the pre-trained FinBERT model. 2. Financial Text Summarization: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Example financial report text = \"The company's revenue increased by 20% year-over-year, driven by strong demand for its flagship product... The company expects to continue its growth trajectory in the next fiscal year...\" # Tokenize and encode the text inputs = tokenizer ( text , return_tensors = \"pt\" ) # Generate summary outputs = model . generate ( ** inputs ) # Decode and print the summary summary = tokenizer . decode ( outputs [ 0 ]) print ( f \"Summary: { summary } \" ) This code snippet shows how to summarize a financial report using the T5 model, which is pre-trained on a massive dataset of text and code. 3. Entity Recognition and Relationship Extraction: Python from transformers import AutoTokenizer , AutoModelForTokenClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Example text text = \"Apple announced a partnership with Google to develop new AI technologies.\" # Tokenize and encode the text inputs = tokenizer ( text , return_tensors = \"pt\" ) # Predict entity types (e.g., company, product, location) outputs = model ( ** inputs ) # Decode and extract entities and relationships predictions = torch . argmax ( outputs . logits , dim =- 1 ) entities = tokenizer . convert_ids_to_tokens ( predictions . tolist ()[ 0 ]) print ( f \"Entities: { entities } \" ) This code snippet demonstrates how to identify and extract entities and their relationships from financial news using a pre-trained NER model. 4. Question Answering System: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Example question and financial document question = \"What is the company's revenue in the last quarter?\" document = \"The company's revenue for the last quarter was $10 billion,...\" # Tokenize and encode the question and document inputs = tokenizer ( question , document , return_tensors = \"pt\" ) # Predict answer to the question outputs = model ( ** inputs ) # Decode and display the answer start_index = torch . argmax ( outputs . start_logits , dim =- 1 ) end_index = torch . argmax ( outputs . end_logits , dim =- 1 ) answer = tokenizer . decode ( inputs . input_ids [ 0 ][ start_index : end_index + 1 ]) print ( f \"Answer: { answer } \" ) This code snippet showcases a question-answering system built on the Longformer model, capable of answering questions about financial documents. These are just a few examples; many other applications for transformer models exist in the realm of financial analysis. As the technology evolves and more data becomes available, we can expect even more powerful and innovative applications emerging in the future.","title":"Python Examples for Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#additional-examples-of-transformer-applications-in-financial-analysis","text":"5. Portfolio Optimization: Transformers can be used to analyze large datasets of financial instruments and identify optimal portfolio allocations. This can be done by learning the relationships between different assets and predicting their future performance. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Example financial data (historical prices, correlations, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict optimal portfolio allocation outputs = model ( ** inputs ) predicted_allocation = outputs . logits # Process and interpret the predicted allocation ... 6. Credit Risk Assessment: Transformers can be used to analyze loan applications and assess the risk of borrowers defaulting on their loans. This can be done by analyzing textual data in the loan application, such as income statements and credit reports. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Example loan application data (text and financial information) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict credit risk score outputs = model ( ** inputs ) # Decode and interpret the predicted score ... 7. Fraud Detection: Transformers can be used to detect fraudulent transactions in financial datasets. This can be done by analyzing patterns in transaction data and identifying anomalies. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Example transaction data (amount, timestamp, account information, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict fraudulent transactions outputs = model ( ** inputs ) # Decode and identify suspicious transactions ... 8. Regulatory Compliance: Transformers can be used to analyze financial documents and ensure compliance with regulations. This can be done by identifying and extracting relevant information from documents and checking for compliance with specific rules. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Example financial document (regulatory report, contract, etc.) document = ... # Tokenize and encode the document inputs = tokenizer ( document , return_tensors = \"pt\" ) # Identify and extract relevant information outputs = model ( ** inputs ) # Decode and compare extracted information with regulatory requirements ... These are just a few examples of the many potential applications of transformer models in financial analysis. As the technology continues to evolve, we can expect to see even more innovative and powerful applications emerge in the future.","title":"Additional Examples of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#additional-examples-of-transformer-applications-in-financial-analysis_1","text":"9. High-Frequency Trading: Transformers can be used to analyze real-time market data and identify short-term trading opportunities. This can be done by learning patterns in order flow and trading activity, predicting price movements, and executing trades quickly. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Real-time market data feed (order book, trades, quotes, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict short-term price movements outputs = model ( ** inputs ) # Decode and interpret the predicted price movements ... 10. Market Anomaly Detection: Transformers can be used to identify unusual patterns in market data that may indicate potential anomalies or market manipulation. This can be done by analyzing large datasets of financial data and identifying deviations from historical norms. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Historical and real-time market data (prices, volumes, news, etc.) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict potential anomalies in market activity outputs = model ( ** inputs ) # Decode and identify potential anomalies for investigation ... 11. Algorithmic Trading: Transformers can be used to develop sophisticated algorithms for trading financial instruments. This can be done by combining the predictive power of transformers with other machine learning techniques to make data-driven trading decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Historical and real-time financial data, news, sentiment analysis data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate trading signals outputs = model ( ** inputs ) # Process and interpret the predicted signals ... 12. Investment Research: Transformers can be used to analyze vast amounts of textual data and identify potential investment opportunities. This can be done by analyzing company reports, news articles, and research papers to identify promising companies or trends. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Textual data (company reports, news articles, research papers) data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and extract investment insights outputs = model ( ** inputs ) # Decode and analyze the extracted insights ... These additional examples demonstrate the diverse and powerful applications of transformers in financial analysis. As the field continues to evolve, we can expect even more innovative and impactful applications to emerge in the future.","title":"Additional Examples of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#other-examples-of-transformer-applications-in-financial-analysis","text":"13. Risk Management: Transformers can be used to assess and manage various financial risks, including market risk, credit risk, and operational risk. This can be done by analyzing historical data, identifying potential risks, and developing mitigation strategies. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Historical financial data, news, regulatory updates, etc. data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict and analyze potential financial risks outputs = model ( ** inputs ) # Process and interpret the predicted risks ... 14. Algorithmic Insurance: Transformers can be used to develop algorithms for underwriting insurance policies, pricing risks, and managing claims. This can be done by analyzing vast amounts of data about policyholders, claims history, and risk factors. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Policyholder data, claims history, risk factors, etc. data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict insurance premiums and assess risk factors outputs = model ( ** inputs ) # Process and interpret the predicted premiums and risks ... 15. Financial Education and Literacy: Transformers can be used to develop personalized financial education resources and tools. This can be done by analyzing user data, identifying their financial goals and needs, and providing tailored learning materials and recommendations. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # User data, financial goals, questions, etc. data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial education content outputs = model ( ** inputs ) # Decode and deliver personalized learning materials and recommendations ... These additional examples illustrate the potential of transformers to revolutionize various aspects of the financial industry. By harnessing the power of these models, financial institutions and individuals can gain deeper insights, make better decisions, and navigate the complex financial landscape with greater confidence. It's important to note that the effectiveness of these applications depends on several factors, including data quality, model selection, and careful implementation. However, the potential benefits of using transformers in financial analysis are vast and far-reaching, shaping the future of how we interact with and manage our finances.","title":"Other Examples of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#further-examples-of-transformer-applications-in-financial-analysis","text":"16. Regulatory Technology (RegTech): Transformers can analyze vast amounts of legal and regulatory documents to identify compliance risks and automate tasks such as reporting and document review. This can help financial institutions comply with regulations more efficiently and cost-effectively. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Regulatory documents, financial data, legal reports data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify potential regulatory violations and risks outputs = model ( ** inputs ) # Decode and analyze the extracted information ... 17. Cybercrime Detection and Prevention: Transformers can be used to analyze financial transactions and identify patterns indicative of suspicious activity or fraud. This can help financial institutions detect and prevent cybercrime attempts and protect their customers' assets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Transaction data, account information, user behavior patterns data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify anomalies and potential cybercrime activity outputs = model ( ** inputs ) # Decode and investigate suspicious transactions ... 18. Financial Chatbots and Virtual Assistants: Transformers can power financial chatbots and virtual assistants that can answer users' questions about financial products and services, provide personalized recommendations, and assist with managing their finances. This can improve customer service and accessibility to financial information, especially for digitally-savvy users. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # User queries, historical financial data, product information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Answer user questions and provide financial guidance outputs = model ( ** inputs ) # Decode and deliver personalized financial insights and assistance ... 19. Democratizing financial data and analysis: Transformers can help make financial data and analysis more accessible to everyone, not just professional investors. By simplifying complex information and providing personalized insights, transformers can empower individuals to make informed financial decisions and participate in the financial markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial news articles, market data, user preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate summaries and explanations of financial news, trends, and concepts outputs = model ( ** inputs ) # Decode and deliver simplified financial information tailored to user needs ... These additional examples showcase the expansive potential of transformers in shaping the future of financial technology and democratizing access to financial knowledge and tools. As research and development continue, we can expect to see even more innovative and impactful applications emerge, transforming the way we interact with and manage our finances. It's important to remember that responsible development and ethical considerations are crucial for ensuring that the benefits of these powerful technologies are broadly shared and utilized for good.","title":"Further Examples of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#additional-transformer-applications-in-financial-analysis","text":"20. Algorithmic Trading for Alternative Assets: Transformers can analyze and predict price movements of non-traditional assets like cryptocurrencies, commodities, and real estate. This can help investors make informed decisions and capitalize on emerging opportunities in these asset classes. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Market data, news articles, social media posts about alternative assets data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict price movements and identify trading opportunities outputs = model ( ** inputs ) # Process and interpret the predicted movements and risks ... 21. Personalized Financial Planning: Transformers can analyze users' financial data, goals, and risk tolerance to create personalized financial plans. This can help individuals achieve their financial goals by providing tailored recommendations and guidance. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # User data, financial goals, risk tolerance, financial history data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial plans and recommendations outputs = model ( ** inputs ) # Decode and deliver tailored financial strategies for individuals ... 22. Financial Market Forecasting: Transformers can analyze historical financial data and identify patterns to forecast future market trends. This can help investors and financial institutions make informed decisions about their investments and asset allocation strategies. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Historical market data, economic indicators, news articles data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict future market trends and identify potential risks and opportunities outputs = model ( ** inputs ) # Decode and analyze the predicted trends and market movements ... 23. Financial Stress Detection and Intervention: Transformers can analyze social media data and other forms of digital communication to identify individuals experiencing financial stress. This can help provide early intervention and support services to individuals facing financial difficulties. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Social media posts, financial conversations, user behavior data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify individuals exhibiting signs of financial stress outputs = model ( ** inputs ) # Decode and utilize insights to provide targeted financial support and advice ... 24. ESG Investing and Impact Analysis: Transformers can analyze vast amounts of social, environmental, and governance (ESG) data to assess the sustainability and ethical impact of investments. This can help investors make informed decisions that align with their values and contribute to a more sustainable future. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # ESG data, company reports, news articles, regulatory documents data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze ESG performance and impact of companies and investments outputs = model ( ** inputs ) # Decode and provide insights for making informed ethical investments ... These diverse examples demonstrate the expansive potential of transformers in revolutionizing how we interact with and analyze complex financial information. By unlocking deeper insights and automating tasks, transformers can empower individuals, financial institutions, and regulators to make better decisions, promote financial inclusion, and ensure a more sustainable and responsible financial system. As research and development in this field continue, we can expect even","title":"Additional Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#more-transformer-applications-in-financial-analysis","text":"25. Financial Fraud Detection: Transformers can analyze financial transactions and identify patterns that indicate fraudulent activity. This can help financial institutions detect and prevent fraud attempts, saving them substantial financial losses. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Financial transactions, user profiles, historical fraud data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and predict fraudulent transactions and anomalies outputs = model ( ** inputs ) # Decode and analyze suspicious transactions for further investigation ... 26. Tax Optimization: Transformers can analyze tax laws and regulations, identify deductions and credits, and suggest optimal tax-saving strategies. This can help individuals and businesses minimize their tax liabilities and maximize their tax benefits. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Tax codes, regulations, financial data, user income and expenses data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized tax optimization strategies and recommendations outputs = model ( ** inputs ) # Decode and assist users in optimizing their tax liabilities ... 27. Market Abuse Detection: Transformers can analyze large datasets of market data and identify patterns that indicate market manipulation or insider trading. This can help regulators detect and prevent market abuse, ensuring fair and efficient markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Market data, trading orders, social media chatter, news articles data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and predict potential market abuse activities outputs = model ( ** inputs ) # Decode and investigate suspicious activities for regulatory action ... 28. Asset Valuation and Pricing: Transformers can analyze financial data and company fundamentals to generate valuations and price estimates for various assets, including stocks, bonds, and real estate. This can help investors make informed decisions and ensure they are paying fair prices for assets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, company reports, market data, industry benchmarks data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate asset valuations and price estimates outputs = model ( ** inputs ) # Decode and deliver insights for informed investment decisions ... 29. Financial News Recommendation: Transformers can personalize financial news feeds based on individual user preferences and investment interests. This provides users with relevant and timely information, allowing them to stay informed and make informed decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # User preferences, investment interests, past news consumption data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Recommend personalized financial news articles and updates outputs = model ( ** inputs ) # Decode and deliver tailored news feeds for informed investment decisions ... These additional examples highlight the vast potential of transformers in shaping the future of financial analysis and decision-making. By leveraging their ability to process and analyze complex information, transformers can empower individuals, financial institutions, and regulators to navigate the ever-evolving financial landscape with greater accuracy, efficiency, and ethical responsibility. As research and development continue to advance, we can expect transformers to become increasingly sophisticated and integrated into the core fabric of financial systems, revolutionizing the way","title":"More Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#expanding-the-scope-of-transformer-applications-in-financial-analysis","text":"30. Generating Financial Reports: Transformers can be used to automatically generate financial reports, such as income statements and balance sheets. This can save companies time and resources, and ensure accuracy and consistency in reporting. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, historical reports, industry benchmarks data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate financial reports outputs = model ( ** inputs ) # Decode and format the generated reports ... 31. Customer Financial Health Assessment: Transformers can analyze customer data, such as income, expenses, and credit history, to assess their financial health and creditworthiness. This can help financial institutions make informed lending decisions and offer personalized financial products. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Customer data, financial transactions, credit history data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict customer financial health and creditworthiness outputs = model ( ** inputs ) # Decode and analyze the predicted financial health for personalized offers ... 32. Financial Education and Literacy Programs: Transformers can be used to develop personalized financial education and literacy programs that cater to individual needs and learning styles. This can help individuals make informed financial decisions and achieve their financial goals. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # User financial knowledge level, learning style, financial goals data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial education content and learning paths outputs = model ( ** inputs ) # Decode and deliver tailored learning materials for improved financial literacy ... 33. Financial Time Series Forecasting: Transformers can analyze historical financial time series data to predict future trends and market movements. This can help investors and financial institutions make informed decisions about asset allocation and risk management. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Financial time series data, historical trends, economic indicators data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict future financial time series trends and market movements outputs = model ( ** inputs ) # Decode and analyze the predicted trends for informed financial decisions ... 34. Financial Data Visualization and Storytelling: Transformers can be used to generate insightful visualizations and narratives based on complex financial data. This can help users understand complex financial concepts and make informed decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, market trends, user preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate visualizations and narratives explaining financial data and trends outputs = model ( ** inputs ) # Decode and present financial insights in an engaging and informative way ... These examples showcase the diverse and innovative ways transformers can be employed in the financial domain. As technology advancements continue, we can anticipate even more sophisticated applications, leading to a future where transformers are seamlessly integrated into the financial ecosystem, empowering individuals and institutions to make informed decisions, achieve financial goals, and navigate the ever-evolving financial landscape with greater confidence and efficiency.","title":"Expanding the Scope of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#additional-potential-applications-of-transformers-in-financial-analysis","text":"35. Decentralized Finance (DeFi) Analysis: Transformers can analyze on-chain data and smart contracts to assess the risks and opportunities associated with DeFi protocols and investments. This can help investors make informed decisions about participating in the DeFi ecosystem. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # On-chain data, smart contracts, DeFi protocol information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze smart contracts and assess DeFi protocol risks and opportunities outputs = model ( ** inputs ) # Decode and present insights for informed DeFi investment decisions ... 36. Algorithmic Portfolio Management with Reinforcement Learning: Transformers can be combined with reinforcement learning algorithms to create intelligent agents that manage investment portfolios. These agents can learn from market data and adapt their strategies over time to maximize returns and minimize risk. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Market data, historical portfolio performance, risk tolerance data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and optimize portfolio strategies using reinforcement learning outputs = model ( ** inputs ) # Decode and implement the optimized portfolio strategies ... 37. Financial Risk Modeling and Stress Testing: Transformers can be used to analyze vast amounts of financial data and model complex financial risks. This can help financial institutions stress test their portfolios and develop risk mitigation strategies. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, economic indicators, historical stress events data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and analyze financial risk models and stress test results outputs = model ( ** inputs ) # Decode and utilize insights to develop effective risk mitigation strategies ... 38. Anticipating Financial Crises: Transformers can analyze large datasets of financial and social media data to identify early warning signs of financial crises. This can help policymakers and financial institutions prepare for and mitigate the impact of crises. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, social media trends, economic indicators data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify early warning signs of potential financial crises outputs = model ( ** inputs ) # Decode and analyze the predicted risks to inform preventive measures ... 39. Financial Data Cleaning and Preprocessing: Transformers can be used to automate the cleaning and preprocessing of financial data, which is a crucial step in data analysis and modeling. This can save time and resources for financial analysts and researchers. Example: Python from transformers import AutoTokenizer , AutoModelForTokenClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Raw financial data, financial terms and entities data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and correct errors in the financial data outputs = model ( ** inputs ) # Decode and deliver clean and preprocessed financial data for analysis ... 40. Optimizing Financial Fraud Detection Systems: Transformers can be used to improve the accuracy and efficiency of financial fraud detection systems. This can help protect financial institutions and individuals from financial losses. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained ## Further Expanding the Scope of Transformers in Financial Analysis: ** 41. Personalized Debt Management and Debt Relief Options : ** Transformers can analyze individual financial data and recommend personalized debt management strategies and potential debt relief options . This can help individuals struggling with debt develop effective plans to manage their finances and achieve financial freedom . ** Example : ** ``` python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Individual financial data, debt obligations, income and expenses data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze debt situation and recommend personalized management strategies outputs = model ( ** inputs ) # Decode and recommend personalized debt relief options and solutions ... 42. Financial Regulatory Compliance and Reporting: Transformers can be used to automate financial regulatory compliance and reporting processes. This can save financial institutions time and resources and reduce the risk of regulatory violations. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, regulatory requirements, reporting templates data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and complete regulatory reports automatically outputs = model ( ** inputs ) # Decode and deliver compliant and accurate financial reports ... 43. Financial Customer Service and Chatbots: Transformers can power financial chatbots that can answer customer questions about financial products and services, troubleshoot issues, and provide personalized financial advice. This can improve customer service and reduce costs for financial institutions. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Customer queries, financial product information, FAQs data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Train and deploy financial chatbots for customer service and support outputs = model ( ** inputs ) # Decode and deliver accurate and personalized responses to customer inquiries ... 44. Building Financial Data Marketplaces: Transformers can facilitate the creation of efficient and secure financial data marketplaces where data providers and consumers can interact and exchange data. This can democratize access to financial data and accelerate financial innovation. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data provider information, data consumer requests, security protocols data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Facilitate secure and efficient financial data exchange and transactions outputs = model ( ** inputs ) # Decode and connect data providers and consumers for mutual benefit ... 45. Financial Education for Underserved Communities: Transformers can be used to develop culturally relevant and accessible financial education programs for underserved communities. This can help improve financial literacy and empower individuals to make informed financial decisions. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Financial information, cultural context, language preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate culturally relevant and accessible financial education materials outputs = model ( ** inputs ) # Decode and deliver personalized financial education in culturally appropriate ways ... These examples showcase the vast potential of transformers in shaping the future of financial services and empowering individuals to make informed financial decisions. As research and development continue, we can expect to see even more innovative and impactful applications emerge, fostering a more inclusive and equitable financial system for all. It's important to remember that the success of these applications hinges on responsible development, ethical considerations","title":"Additional Potential Applications of Transformers in Financial Analysis:"},{"location":"finance/use_of_ft/#additional-examples-of-transformer-applications-in-financial-analysis_2","text":"46. Crypto-asset Analysis: Transformers can analyze cryptocurrency data, including blockchain transactions, social media sentiment, and news articles, to identify trends, predict price movements, and evaluate the feasibility and potential of new projects. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-large-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-large-ner\" ) # Cryptocurrency data, social media trends, news articles, project information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze cryptocurrency data and identify potential investment opportunities outputs = model ( ** inputs ) # Decode and extract insights for informed cryptocurrency investment decisions ... 47. Financial Market Simulation and Optimization: Transformers can be used to create realistic simulations of financial markets, allowing financial institutions to test and optimize their trading strategies and risk management models. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"ProsusAI/finbert\" ) # Historical market data, economic indicators, trading strategies data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and analyze simulations of different market scenarios outputs = model ( ** inputs ) # Decode and optimize trading strategies based on simulated market outcomes ... 48. Insurance Risk Assessment and Pricing: Transformers can analyze individual and historical data to assess insurance risks and personalize insurance premiums. This can lead to fairer and more efficient insurance markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Individual data, insurance history, medical records, risk factors data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Assess individual insurance risks and generate personalized premium quotes outputs = model ( ** inputs ) # Decode and deliver individualized insurance plans and pricing ... 49. Financial Data Summarization and Reporting: Transformers can automatically summarize and generate reports based on large datasets of financial data, reducing the time and resources required for financial analysts and auditors. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, reports, audit requirements data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate concise and informative financial summaries and reports outputs = model ( ** inputs ) # Decode and deliver summaries and reports for efficient analysis and decision-making ... 50. Financial Contract Review and Negotiation: Transformers can analyze and extract key information from financial contracts, helping lawyers and financial professionals understand the terms and negotiate more effectively. Example: Python from transformers import AutoTokenizer , AutoModelForTokenClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial contracts, legal terms, negotiation history data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify key clauses, extract relevant information, and facilitate contract analysis outputs = model ( ** inputs ) # Decode and assist with contract review and negotiation processes ... These examples demonstrate the vast potential of transformers in revolutionizing the financial industry across various sectors. By automating tasks, analyzing complex data, and generating insights, transformers can empower financial institutions, individuals, and regulators to make informed decisions, optimize processes, and navigate the ever-evolving financial landscape with greater efficiency and effectiveness. It's crucial to remember that the responsible development and ethical implementation of these applications are essential for ensuring that the benefits of transformers are distributed equitably and contribute to a more sustainable and inclusive financial ecosystem for","title":"Additional Examples of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#further-expanding-the-scope-of-transformer-applications-in-financial-analysis","text":"51. Real Estate Market Prediction: Transformers can analyze real estate data, including market trends, property listings, and economic indicators, to predict future market movements and identify potential investment opportunities. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Real estate data, market trends, economic indicators, property details data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict future real estate market trends and identify investment opportunities outputs = model ( ** inputs ) # Decode and provide insights for informed real estate investment decisions ... 52. Personal Finance Planning and Budgeting: Transformers can analyze individual financial data, spending habits, and goals to create personalized financial plans and budgets. This can help individuals manage their finances more effectively and achieve their financial goals. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Individual income, expenses, financial goals, financial data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized financial plans and budgeting recommendations outputs = model ( ** inputs ) # Decode and deliver tailored financial planning insights and strategies ... 53. Financial Inclusion and Access to Credit: Transformers can be used to develop alternative credit scoring models that can assess the creditworthiness of individuals who lack traditional credit history. This can help expand financial inclusion and provide access to credit for underserved communities. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Individual financial data, alternative data sources, social media activity data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Develop alternative credit scoring models for financial inclusion outputs = model ( ** inputs ) # Decode and use alternative credit scores for fairer and accessible credit decisions ... 54. Financial Communication and Content Creation: Transformers can be used to generate clear, concise, and engaging financial content, such as news articles, blog posts, and investment reports. This can make financial information more accessible to a wider audience. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, market trends, target audience preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate engaging and informative financial communication content outputs = model ( ** inputs ) # Decode and deliver tailored financial content to various audiences ... 55. Cybersecurity and Fraud Detection in Financial Services: Transformers can analyze financial transactions and other data sources to detect fraudulent activities and protect financial institutions and individuals from cybercrime. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial transactions, user behavior data, cybersecurity threats data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Detect and prevent cybersecurity threats and fraudulent activities outputs = model ( ** inputs ) # Decode and analyze suspicious activities for further investigation and mitigation ... These examples showcase the transformative potential of transformers in shaping the future of financial services and promoting a more inclusive, accessible, and secure financial ecosystem for all. As research and development progress, we can expect even more innovative applications to emerge, further transforming the financial landscape and empowering individuals and institutions to navigate the financial world with greater clarity, confidence, and efficiency.","title":"Further Expanding the Scope of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#additional-applications-of-transformers-in-financial-analysis","text":"56. Sustainability and Impact Investing: Transformers can analyze companies' environmental, social, and governance (ESG) data to identify sustainable investments and assess the impact of investments on society and the environment. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Company ESG data, sustainability reports, investment goals data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify sustainable investment opportunities and assess their social impact outputs = model ( ** inputs ) # Decode and provide insights for informed sustainable investment decisions ... 57. Financial Education and Gamification: Transformers can be used to develop interactive and engaging games that teach users about financial concepts and make financial education more accessible and enjoyable. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial concepts, game mechanics, user preferences data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate interactive financial education games for engaging learning outputs = model ( ** inputs ) # Decode and deliver personalized and gamified financial education experiences ... 58. Financial Regulatory Compliance and Automation: Transformers can automate the process of compliance with financial regulations, reducing the burden on financial institutions and improving accuracy and efficiency. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Regulatory rules and requirements, financial data, compliance reports data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Automate regulatory compliance processes and identify potential violations outputs = model ( ** inputs ) # Decode and generate reports and take corrective actions for compliance ... 59. Financial Data Privacy and Security: Transformers can be used to develop new methods for protecting financial data privacy and security, such as text anonymization and homomorphic encryption. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, privacy requirements, security protocols data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate anonymized financial data for improved data privacy outputs = model ( ** inputs ) # Decode and deliver secure and privacy-preserving financial data analysis ... 60. Financial Market Regulation and Supervision: Transformers can be used by regulators to analyze financial data and identify potential market manipulation or systemic risk, ensuring the stability and fairness of financial markets. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial market data, regulations, market surveillance data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Detect and analyze potential market manipulation and systemic risks outputs = model ( ** inputs ) # Decode and inform regulatory actions for a stable and fair financial market ... These additional examples showcase the vast potential of transformers in shaping the future of financial analysis and beyond. As research and development continue to advance, we can expect to see even more innovative and impactful applications emerge, leading to a more inclusive, efficient, and sustainable financial ecosystem for all.","title":"Additional Applications of Transformers in Financial Analysis:"},{"location":"finance/use_of_ft/#further-expanding-the-scope-of-transformer-applications-in-financial-analysis_1","text":"61. Personalized Investment Portfolio Optimization: Transformers can be used to tailor investment portfolios to individual investor risk profiles and financial goals. They can analyze historical market data, current market trends, and individual preferences to recommend personalized investment strategies and asset allocations. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Investor risk profile, financial goals, market data, asset information data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized investment portfolio recommendations outputs = model ( ** inputs ) # Decode and deliver tailored investment strategies for optimal returns ... 62. Financial Stress Prediction and Early Warning Systems: Transformers can analyze individual economic and financial data to predict financial stress and identify individuals at risk of financial hardship. This information can be used to develop early warning systems and provide timely interventions to prevent financial distress. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Individual financial data, economic indicators, credit history, employment data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict individual financial stress and identify risk factors outputs = model ( ** inputs ) # Decode and provide early warnings and interventions for financial risk mitigation ... 63. Financial Market Microstructure Analysis: Transformers can analyze high-frequency trading data and order book snapshots to gain insights into market microstructure, including order flow, liquidity, and trading patterns. This information can be used to develop algorithmic trading strategies and improve market efficiency. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # High-frequency trading data, order book snapshots, market microstructure features data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze market microstructure and identify trading opportunities outputs = model ( ** inputs ) # Decode and generate insights for informed algorithmic trading strategies ... 64. Financial Crisis Prediction and Mitigation: Transformers can analyze large datasets of historical financial data, news articles, and social media sentiment to predict financial crises and identify potential risk factors. This information can be used to develop mitigation strategies and prepare for potential economic downturns. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForQuestionAnswering . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, news articles, social media sentiment, historical crises data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Predict financial crisis risks and identify early warning signs outputs = model ( ** inputs ) # Decode and inform crisis mitigation strategies and preparedness measures ... 65. Financial Data Explainability and Interpretability: Transformers can be used to explain the rationale behind their predictions and recommendations, making them more transparent and interpretable for financial analysts and investors. This can help build trust in AI-driven financial applications and enable better decision-making. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, model predictions, desired explanation level data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate explanations for model predictions and recommendations outputs = model ( ** inputs ) # Decode and provide clear and understandable explanations for improved trust and decision-making ... These additional examples showcase the diverse and innovative ways transformers can be employed in the financial domain. As","title":"Further Expanding the Scope of Transformer Applications in Financial Analysis:"},{"location":"finance/use_of_ft/#further-expanding-the-scope-of-transformer-applications-in-financial-analysis_2","text":"66. Financial Forensics and Fraud Investigation: Transformers can analyze financial transactions and communication data to identify patterns and anomalies that may indicate fraudulent activities. This can assist financial institutions in investigating fraud, recovering stolen assets, and preventing future incidents. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Financial transactions, communication data, fraud indicators, historical case data data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and investigate potential financial fraud and suspicious activities outputs = model ( ** inputs ) # Decode and provide insights for effective fraud mitigation and investigation ... 67. Financial Risk Management and Stress Testing: Transformers can analyze complex financial data and models to assess risk exposures and vulnerabilities within financial institutions. This information can be used to develop risk management strategies, conduct stress tests, and ensure the financial stability of institutions. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Financial data, risk models, stress test scenarios, regulatory requirements data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze and assess financial risks, conduct stress tests, and develop mitigation strategies outputs = model ( ** inputs ) # Decode and provide actionable insights for improved risk management and financial stability ... 68. Financial Market Anomaly Detection and Prediction: Transformers can analyze historical market data and identify unusual patterns and deviations that may be indicative of market anomalies. This information can be used to predict market crashes, identify opportunities for arbitrage, and develop risk management strategies. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Historical market data, trading patterns, anomaly detection algorithms data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Identify and predict market anomalies for informed trading and risk management outputs = model ( ** inputs ) # Decode and provide alerts and insights for proactive action in response to market anomalies ... 69. Financial Data Integration and Interoperability: Transformers can be used to facilitate the integration and interoperability of financial data from diverse sources. This can help break down information silos and enable financial institutions to gain a holistic view of their financial data, leading to improved decision-making and operational efficiency. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data from diverse sources, data integration protocols, interoperability standards data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Integrate and interoperate financial data for comprehensive analysis and insights outputs = model ( ** inputs ) # Decode and deliver unified financial data for enhanced decision-making and efficiency ... 70. Financial Data Augmentation and Generation: Transformers can be used to generate synthetic financial data that can be used for training and evaluating financial models, testing hypotheses, and exploring various scenarios. This can be particularly useful for augmenting limited datasets and improving modelgeneralizability. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Existing financial data, desired data augmentation parameters and specifics data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate synthetic financial data for enhanced model training and evaluation outputs = model ( ** inputs ) # Decode and utilize synthetic data for informed financial ## Additional Examples of Transformer Applications in Financial Analysis: ** 71. Personalized Tax Planning and Optimization : ** Transformers can analyze individual income , expenses , and investments to recommend personalized tax planning strategies and optimize tax liabilities . This can help individuals save money on taxes and ensure compliance with tax regulations . ** Example : ** ``` python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"t5-base\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"t5-base\" ) # Individual income data, expense data, investment information, tax laws data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate personalized tax planning recommendations and optimize tax liabilities outputs = model ( ** inputs ) # Decode and deliver tailored tax strategies for individuals ... 72. Financial Product Recommendation and Targeting: Transformers can analyze individual financial data and preferences to recommend personalized financial products and services. This can help financial institutions provide relevant offerings to their customers and improve customer satisfaction and retention. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"ProsusAI/finbert\" ) model = AutoModelForSequenceClassification . from_pretrained ( \"ProsusAI/finbert\" ) # Individual financial data, preferences, product information, target audience data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Recommend personalized financial products and target relevant customer segments outputs = model ( ** inputs ) # Decode and deliver targeted financial product offerings for increased customer satisfaction ... 73. Financial Market Sentiment Analysis and Prediction: Transformers can analyze financial news articles, social media sentiment, and other textual data to gauge overall market sentiment and predict future market movements. This can be valuable information for investors and traders seeking to make informed investment decisions. Example: Python from transformers import AutoTokenizer , AutoModelForQuestionAnswering # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"nlpaueb/bert-base-ner\" ) model = AutoModelForTokenClassification . from_pretrained ( \"nlpaueb/bert-base-ner\" ) # Financial news articles, social media data, historical market data, sentiment analysis algorithms data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Analyze market sentiment and predict future market movements outputs = model ( ** inputs ) # Decode and provide insights for sentiment-driven investment strategies ... 74. Financial Regulatory Compliance Automation and Reporting: Transformers can automate the process of generating regulatory reports and ensuring compliance with financial regulations. This can save financial institutions time and resources while reducing the risk of regulatory violations. Example: Python from transformers import AutoTokenizer , AutoModelForSeq2SeqLM # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"allenai/longformer-base-4096\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"allenai/longformer-base-4096\" ) # Financial data, regulatory requirements, reporting templates, compliance standards data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Generate and automate regulatory reports for improved compliance and efficiency outputs = model ( ** inputs ) # Decode and deliver accurate and compliant financial reports to regulatory authorities ... 75. Financial Data Democratization and Accessibility: Transformers can be used to create user-friendly interfaces and tools that allow individuals with varying levels of financial expertise to access and analyze financial data. This can promote financial literacy and empower individuals to make informed financial decisions. Example: Python from transformers import AutoTokenizer , AutoModelForSequenceClassification # Load pre-trained model and tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bigscience/bloom-176b\" ) model = AutoModelForSeq2SeqLM . from_pretrained ( \"bigscience/bloom-176b\" ) # Financial data, user interface design, accessibility features, financial literacy resources data = ... # Tokenize and encode the data inputs = tokenizer ( data , return_tensors = \"pt\" ) # Develop user-friendly financial analysis tools for broader accessibility and empowerment outputs = model ( ** inputs ) # Decode and deliver financial information in an accessible and understandable way ... These diverse examples showcase the immense potential of transformers in revolutionizing the financial landscape. As research and development continue to progress, we can expect even more innovative applications to emerge,","title":"Further Expanding the Scope of Transformer Applications in Financial Analysis:"},{"location":"getting-started/","text":"Hello from Getting Started \u00b6 Flow diagram ``` mermaid flowchart TD; id1[\"Data class\"] id2[\"Fetching\"] id3[\"Pandas objects\"] id4[\"Data instances\"] id5[\"Alignment\"] id6[\"Merging\"] id7[\"Data instance\"] id8[\"Updating\"] id9[\"Querying\"] id10[\"Resampling\"] id11[\"Transforming\"] id1 --> id2; id2 --> id5; id3 --> id5; id4 --> id5; id5 --> id6; id6 -->|\"creates new\"| id7; id7 --> id8; id7 --> id10; id10 -->|\"creates new\"| id7; id7 --> id11; id11 -->|\"creates new\"| id7; id8 --> id5; id7 --> id9; ``` flowchart TD; id1[\"Data class\"] id2[\"Fetching\"] id3[\"Pandas objects\"] id4[\"Data instances\"] id5[\"Alignment\"] id6[\"Merging\"] id7[\"Data instance\"] id8[\"Updating\"] id9[\"Querying\"] id10[\"Resampling\"] id11[\"Transforming\"] id1 --> id2; id2 --> id5; id3 --> id5; id4 --> id5; id5 --> id6; id6 -->|\"creates new\"| id7; id7 --> id8; id7 --> id10; id10 -->|\"creates new\"| id7; id7 --> id11; id11 -->|\"creates new\"| id7; id8 --> id5; id7 --> id9; classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } Using entity-relationship diagrams \u00b6 An [entity-relationship diagram] is composed of entity types and specifies relationships that exist between entities. It describes inter-related things in a specific domain of knowledge: Entity-relationship diagram ``` mermaid erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ``` erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses","title":"Hello from Getting Started"},{"location":"getting-started/#hello-from-getting-started","text":"Flow diagram ``` mermaid flowchart TD; id1[\"Data class\"] id2[\"Fetching\"] id3[\"Pandas objects\"] id4[\"Data instances\"] id5[\"Alignment\"] id6[\"Merging\"] id7[\"Data instance\"] id8[\"Updating\"] id9[\"Querying\"] id10[\"Resampling\"] id11[\"Transforming\"] id1 --> id2; id2 --> id5; id3 --> id5; id4 --> id5; id5 --> id6; id6 -->|\"creates new\"| id7; id7 --> id8; id7 --> id10; id10 -->|\"creates new\"| id7; id7 --> id11; id11 -->|\"creates new\"| id7; id8 --> id5; id7 --> id9; ``` flowchart TD; id1[\"Data class\"] id2[\"Fetching\"] id3[\"Pandas objects\"] id4[\"Data instances\"] id5[\"Alignment\"] id6[\"Merging\"] id7[\"Data instance\"] id8[\"Updating\"] id9[\"Querying\"] id10[\"Resampling\"] id11[\"Transforming\"] id1 --> id2; id2 --> id5; id3 --> id5; id4 --> id5; id5 --> id6; id6 -->|\"creates new\"| id7; id7 --> id8; id7 --> id10; id10 -->|\"creates new\"| id7; id7 --> id11; id11 -->|\"creates new\"| id7; id8 --> id5; id7 --> id9; classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() }","title":"Hello from Getting Started"},{"location":"getting-started/#using-entity-relationship-diagrams","text":"An [entity-relationship diagram] is composed of entity types and specifies relationships that exist between entities. It describes inter-related things in a specific domain of knowledge: Entity-relationship diagram ``` mermaid erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ``` erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses","title":"Using entity-relationship diagrams"},{"location":"home/","text":"Introduction \u00b6 List of my notes. Deply Angular app to github.io pages","title":"Introduction"},{"location":"home/#introduction","text":"List of my notes. Deply Angular app to github.io pages","title":"Introduction"},{"location":"home/azure-keyboard-shortcuts/","text":"Azure Keyboard Shortcuts in Azure Portal \u00b6 Azure had keyboard shortcut to navigate around different areas. Click mouse around the Blade title G + D Go to Dashboard G + A Go to All Resources G + N Create new Resource G + / Cursor to Search bar G + R Go to Resource groups For all keyboard shortcuts check out Azure Portal Keyboard shortcuts in Help section","title":"Azure Keyboard Shortcuts in Azure Portal"},{"location":"home/azure-keyboard-shortcuts/#azure-keyboard-shortcuts-in-azure-portal","text":"Azure had keyboard shortcut to navigate around different areas. Click mouse around the Blade title G + D Go to Dashboard G + A Go to All Resources G + N Create new Resource G + / Cursor to Search bar G + R Go to Resource groups For all keyboard shortcuts check out Azure Portal Keyboard shortcuts in Help section","title":"Azure Keyboard Shortcuts in Azure Portal"},{"location":"home/azure-resource-graph-explorer/","text":"Write query using Azure Resource Graph Explorer \u00b6 Navigate to Azure Resource Graph Explorer and type Kusto query SQL summarize Count = count () by type | order by Count desc The result has chart option which can be pinned to Dashboard We can further apply further on Dashboard if needed.","title":"Write query using Azure Resource Graph Explorer"},{"location":"home/azure-resource-graph-explorer/#write-query-using-azure-resource-graph-explorer","text":"Navigate to Azure Resource Graph Explorer and type Kusto query SQL summarize Count = count () by type | order by Count desc The result has chart option which can be pinned to Dashboard We can further apply further on Dashboard if needed.","title":"Write query using Azure Resource Graph Explorer"},{"location":"home/azure-service-fabric/","text":"Introduction to Azure Service Fabric \u00b6 Breaking monolithic application into micro services is a lot easier to swallow with Service Fabric and it helps to break up the applications to make better use compute on machines in the cluster and still possible to leverage docker. Concepts and Architecture \u00b6 Understanding Micro Services High Level View Design Recommendations Pattern & Practices Why Micros Services Microservices architecture on Azure Service Fabric Service Fabric Overview \u00b6 What is Service Fabric Overview Deploying an Application Lift and Shift Existing .net Applications Create and Manage a Cluster How to Guides Securing the Cluster Building a web front end Getting started with service Fabric and .net core Hosting .net core service on Service Fabric Code Samples \u00b6 Service fabric dotnet quick start doc Service fabric dotnet quick start repo Service Fabric Service Fabric Watchdog Service Web Reference Getting Started Resources \u00b6 Office documentation Service fabric Actors Pluralsight course Fabric Partitioning Interception & Custom Headers Github docs repo","title":"Introduction to Azure Service Fabric"},{"location":"home/azure-service-fabric/#introduction-to-azure-service-fabric","text":"Breaking monolithic application into micro services is a lot easier to swallow with Service Fabric and it helps to break up the applications to make better use compute on machines in the cluster and still possible to leverage docker.","title":"Introduction to Azure Service Fabric"},{"location":"home/azure-service-fabric/#concepts-and-architecture","text":"Understanding Micro Services High Level View Design Recommendations Pattern & Practices Why Micros Services Microservices architecture on Azure Service Fabric","title":"Concepts and Architecture"},{"location":"home/azure-service-fabric/#service-fabric-overview","text":"What is Service Fabric Overview Deploying an Application Lift and Shift Existing .net Applications Create and Manage a Cluster How to Guides Securing the Cluster Building a web front end Getting started with service Fabric and .net core Hosting .net core service on Service Fabric","title":"Service Fabric Overview"},{"location":"home/azure-service-fabric/#code-samples","text":"Service fabric dotnet quick start doc Service fabric dotnet quick start repo Service Fabric Service Fabric Watchdog Service Web Reference Getting Started","title":"Code Samples"},{"location":"home/azure-service-fabric/#resources","text":"Office documentation Service fabric Actors Pluralsight course Fabric Partitioning Interception & Custom Headers Github docs repo","title":"Resources"},{"location":"home/deploy-angular-app-to-github-io-pages/","text":"Deploy Angular app to github.io pages \u00b6 Create application using angular-cli ...... Bash npm install -g @angular/cli npm new <appname> Install angular-cli-ghpages Bash npm install -g angular-cli-ghpages Build Bash ng build --prod --base-href = \"https://<username>.github.io/<reponame>/\" Deploy Bash npx angular-cli-ghpages --dir = dist/apps/<appname> Change repo setting to use gh-pages:","title":"Deploy Angular app to github.io pages"},{"location":"home/deploy-angular-app-to-github-io-pages/#deploy-angular-app-to-githubio-pages","text":"Create application using angular-cli ...... Bash npm install -g @angular/cli npm new <appname> Install angular-cli-ghpages Bash npm install -g angular-cli-ghpages Build Bash ng build --prod --base-href = \"https://<username>.github.io/<reponame>/\" Deploy Bash npx angular-cli-ghpages --dir = dist/apps/<appname> Change repo setting to use gh-pages:","title":"Deploy Angular app to github.io pages"},{"location":"home/dynamic-365-hcm-kusto/","text":"Troubleshoot using Kusto \u00b6 Kusto Extra MetaData is stored in property bag CustomDimentions, ClientEvents, CustomEvent and TraceEvent SQL ClientEvents | where Timestamp > todatetime ( \"2019-04-01 20:31:02Z\" ) and Timestamp < todatetime ( \"2019-04-01 20:51:02Z\" ) | where Name == \"GenericErrorDialog\" | extend cd = parsejson ( CustomDimentions ) | where cd . errorCode contains \"GenericGetCompanyInfo\" or cd . errorMessage contains \"Undefined\" | where cd . rootActivity == \"\" SQL TraceEvent | where ActivityType contains \"HcmAttEOL\" SQL ClientEvents | where RootActivityId == \"\" // If there is no output then we can get it from TraceEvent SQL TraceEvent | where RootActivityId = \"\" SQL TraceEvent | where SessionId == \"\" | project TIMESTAMP , RootActivityId , ActivityType , ActivityVector , TraceLevel , TraceMessage SQL TraceEvent | where RootActivityId == \"\" | project TIMESTAMP , Application , ActivityType , ActivityVector , TraceLevel , TraceMessage With Local timestamp and sorting \ud83d\udc4d SQL TraceEvent | where TraceLevel == \"Error\" | where ActivityType == \"HcmASHandleChangeMessage\" | extend LocalTime = TIMESTAMP - 7 h // UTC -> PST | project format_datetime ( LocalTime , 'yy-MM-dd [hh:mm:ss tt]' ), format_datetime ( TIMESTAMP , 'yy-MM-dd [hh:mm:ss tt]' ), Tenant , ActivityType , ActivityVector , TraceLevel , TraceMessage | sort by TIMESTAMP ActivityVector provide the sequence of execution for Async calls so it's helpful on building the call stack during investigation Tip Text Only :bulb: Notes ```markdown * Leverage TraceEvent not just ClientEvents using raid/session id etc * Leverage activity vector, activity type, application, service * Use custom dimensions when needed * Use status app for environment details, tenant details, user information * Use tsg/wiki - all possible exception are there so search for it * xrm exceptions - there is an amazing wiki - use that (xrm is on top of CDS) * Play with product to understand how something should work * Big picture - don't focus on just one issue what is reported - offer example * repro - last thing to do ``` Tip Text Only Apps \ud83d\udca1 ```markdown * Go1Adapter.sln * XRMSolutions.sln * TalentEngagementServiceApp.sln (Attract Service) * SchedulingServiceApp.sln (Schedular for scheduling interviews and stuff) * OfferManagementService.sln * JobPostingServiceApp.sln * OnboardingServiceApp.sln * ProvisioningServiceApp.sln * FlightingServiceApp.sln * All services are in D365 ``` Onboarding \u00b6 https://dynamics.wiki/index.php/HCM_Engineering https://dynamics.wiki/index.php/Security_Groups_and_memberships#XRM_environments https://msazure.visualstudio.com/D365 Resources/links \u00b6 https://attract.talent.dev.dynamics.com https://dataexplorer.azure.com/ * https://kusto.azurewebsites.net/docs/ Apps \u00b6 Onboard Offer Attract Tools \u00b6 Repo Data Explorer web Kusto Environments \u00b6 Connection: https://dynamicshcmint.kusto.windows.net Connection: https://dynamicshcmint.kusto.windows.net Documents \u00b6 Dynamics365HCM HCM Arch TODO \u00b6 Service Fabric and service app Kusto Environment \u00b6 ICM Query \u00b6 ICM link Attract \u00b6","title":"Troubleshoot using Kusto"},{"location":"home/dynamic-365-hcm-kusto/#troubleshoot-using-kusto","text":"Kusto Extra MetaData is stored in property bag CustomDimentions, ClientEvents, CustomEvent and TraceEvent SQL ClientEvents | where Timestamp > todatetime ( \"2019-04-01 20:31:02Z\" ) and Timestamp < todatetime ( \"2019-04-01 20:51:02Z\" ) | where Name == \"GenericErrorDialog\" | extend cd = parsejson ( CustomDimentions ) | where cd . errorCode contains \"GenericGetCompanyInfo\" or cd . errorMessage contains \"Undefined\" | where cd . rootActivity == \"\" SQL TraceEvent | where ActivityType contains \"HcmAttEOL\" SQL ClientEvents | where RootActivityId == \"\" // If there is no output then we can get it from TraceEvent SQL TraceEvent | where RootActivityId = \"\" SQL TraceEvent | where SessionId == \"\" | project TIMESTAMP , RootActivityId , ActivityType , ActivityVector , TraceLevel , TraceMessage SQL TraceEvent | where RootActivityId == \"\" | project TIMESTAMP , Application , ActivityType , ActivityVector , TraceLevel , TraceMessage With Local timestamp and sorting \ud83d\udc4d SQL TraceEvent | where TraceLevel == \"Error\" | where ActivityType == \"HcmASHandleChangeMessage\" | extend LocalTime = TIMESTAMP - 7 h // UTC -> PST | project format_datetime ( LocalTime , 'yy-MM-dd [hh:mm:ss tt]' ), format_datetime ( TIMESTAMP , 'yy-MM-dd [hh:mm:ss tt]' ), Tenant , ActivityType , ActivityVector , TraceLevel , TraceMessage | sort by TIMESTAMP ActivityVector provide the sequence of execution for Async calls so it's helpful on building the call stack during investigation Tip Text Only :bulb: Notes ```markdown * Leverage TraceEvent not just ClientEvents using raid/session id etc * Leverage activity vector, activity type, application, service * Use custom dimensions when needed * Use status app for environment details, tenant details, user information * Use tsg/wiki - all possible exception are there so search for it * xrm exceptions - there is an amazing wiki - use that (xrm is on top of CDS) * Play with product to understand how something should work * Big picture - don't focus on just one issue what is reported - offer example * repro - last thing to do ``` Tip Text Only Apps \ud83d\udca1 ```markdown * Go1Adapter.sln * XRMSolutions.sln * TalentEngagementServiceApp.sln (Attract Service) * SchedulingServiceApp.sln (Schedular for scheduling interviews and stuff) * OfferManagementService.sln * JobPostingServiceApp.sln * OnboardingServiceApp.sln * ProvisioningServiceApp.sln * FlightingServiceApp.sln * All services are in D365 ```","title":"Troubleshoot using Kusto"},{"location":"home/dynamic-365-hcm-kusto/#onboarding","text":"https://dynamics.wiki/index.php/HCM_Engineering https://dynamics.wiki/index.php/Security_Groups_and_memberships#XRM_environments https://msazure.visualstudio.com/D365","title":"Onboarding"},{"location":"home/dynamic-365-hcm-kusto/#resourceslinks","text":"https://attract.talent.dev.dynamics.com https://dataexplorer.azure.com/ * https://kusto.azurewebsites.net/docs/","title":"Resources/links"},{"location":"home/dynamic-365-hcm-kusto/#apps","text":"Onboard Offer Attract","title":"Apps"},{"location":"home/dynamic-365-hcm-kusto/#tools","text":"Repo Data Explorer web","title":"Tools"},{"location":"home/dynamic-365-hcm-kusto/#kusto-environments","text":"Connection: https://dynamicshcmint.kusto.windows.net Connection: https://dynamicshcmint.kusto.windows.net","title":"Kusto Environments"},{"location":"home/dynamic-365-hcm-kusto/#documents","text":"Dynamics365HCM HCM Arch","title":"Documents"},{"location":"home/dynamic-365-hcm-kusto/#todo","text":"Service Fabric and service app","title":"TODO"},{"location":"home/dynamic-365-hcm-kusto/#kusto-environment","text":"","title":"Kusto Environment"},{"location":"home/dynamic-365-hcm-kusto/#icm-query","text":"ICM link","title":"ICM Query"},{"location":"home/dynamic-365-hcm-kusto/#attract","text":"","title":"Attract"},{"location":"home/kusto-overview/","text":"Kusto Query Language (KQL) \u00b6 Official documentation KQL Documentation Course Pluralsight course what is KQL and where is it used? KQL is an open source language created by Microsoft to query big data sets stored in the Azure cloud. These queries can also be used in alerting rules. Some examples of services/products hosted in Azure that make use of KQL are: * Azure Data Explorer * Log Analytics * Sentinel (this is Microsoft\u2019s cloud SIEM solution that makes use of a Log Analytics workspace as its backend) * Microsoft Defender ATP Sample or Demo queries can be tested at DemoLogs Generic \u00b6 A string literal with a backslash requires escaping by a backslash: \"a string literal with a \\\\ needs to be escaped\" The same can be achieved using a verbatim string literal by putting the @ sign in front: @\"a verbatim string literal with a \\\\ that does not need to be escaped\" More info on escaping string data types can be found here . Add comments to your query with a double forward slash: // This is a comment The where operator and the pipe ( | ) delimiter are essential in writing KQL queries. where is used to filter rows from a table. In this example we filter on events from a source, the table SecurityEvent, where the column Computer is equal to \"ContosoAppSrv1\", and count the number of results: Bash SecurityEvent | where Computer == \"ContosoAppSrv1\" | count The pipe is used to separate data transformation operators. Such as: where Computer == \"ContosoAppSrv1\". The result can be piped to a new operator. For example, to count the number for rows: | count Only include events from the last 24 hours using the ago() function: Bash T | where TimeGenerated > ago ( 24h ) For performance reasons always use time filters first in your query. The ago() function supports multiple types of timespans. More info can be found here . For example: \u2022 1d 1 day \u2022 10m 10 minutes \u2022 30s 30 seconds Select and customize the columns from the resulting table of your query with the project operator. \u2022 Specify the columns to include : Bash T | project TimeGenerated, EventID, Account, Computer, LogonType \u2022 Rename columns . In this example we renamed the column Account to UserName: Bash T | project TimeGenerated, EventID, UserName = Account, Computer, LogonType \u2022 Remove columns with project-away: Bash T | project-away EventSourceName, Task, Level Add calculated columns to the result using the extend operator: Bash T | extend EventAge = now () -TimeGenerated Count the number of records using the count operator: Bash T | count Match based on conditions using logical operators . For example: \u2022 T | where EventID == 4624 and LogonType == 3 \u2022 T | where EventID == 4624 or EventID == 4625 \u2022 T | where (EventID == 4624 and LogonType == 3) or EventID == 4625 Aggerate results from your query with the summarize operator: \u2022 Aggregate on multiple columns: Bash T | summarize by Computer, Account \u2022 Aggregate on multiple columns and return the count of the group: Bash T | summarize count () by Computer, Account Besides count() many more very useful aggregation functions exist. An overview can be found here . Sort the rows of the result using the sort operator: Bash T | where EventID == 4624 | summarize count () by AuthenticationPackageName | sort by count_ By default, rows are sorted in descending order. Sorting in ascending order is also possible: \u2022 sort by count_ asc \u2022 Descending order: desc Concatenate values . The result will be a string data type: Bash T | project example = strcat ( EventID, \" - \" , Channel ) A variable number of values can be passed through the strcat function. If values are not a string, they will be forcibly converted to a string. Numerical search \u00b6 Search for a specific value : T | where EventID == 4688 \u2022 Not equal to: != All of the numerical operators can be found here . Search for a value less or greater than : T | where EventID == 4688 | summarize count() by Process | where count_ < 5 \u2022 Greater: > \u2022 Less or Equal: <= \u2022 Greater or Equal: >= Match on multiple numeric values : Bash T | where EventID in ( 4624 , 4625 ) Extract values \u00b6 Extract values from a string or JSON data . For example, extract the \u201cprocess name\u201d using a regular expression (if you are less familiar with regular expressions have a look at the split and parse function): Bash SecurityAlert | extend _ProcessName = extract ( '\"process name\": \"(.*)\"' , 1 , ExtendedProperties ) Because the column ExtendedProperties contains JSON data you can also use the function extractjson(): Bash SecurityAlert | extend _ProcessName = extractjson ( \" $ .process name\" , ExtendedProperties ) If you need to extract multiple elements from JSON data, stored as a string, you can use the function parse_json(). Use the dot notation if the data is of the type dictionary or a list of dictionaries in an array. One way to find out is through the gettype() function. To play with data stored as a dictionary have a look at the help cluster in the Azure Data Explorer (table: StormEvents, column: StormSummary). String search \u00b6 Search across all tables and columns : search \" KEYWORD \" \u2022 Keep in mind that this is a performance intensive operation. Search for a specific value : T | where ProcessName == @\"C:\\Windows\\System32\\regsvr32.exe\" A not equal to match is done by adding an exclamation mark as prefix: \u2022 Equal to: == \u2022 Not equal to: != This is also supported in a similar way for other string operators A case insensitive match can be achieved using a tilde: \u2022 Case sensitive: == \u2022 Case insensitive: =~ \u2022 Case insensitive and not equal to: !~ This is also supported in a similar way for other string operators . Match on values that contain a specific string : Bash T | where CommandLine contains \"guest\" Because has is more performant, it\u2019s advised to use has over contains when searching for full keywords. The following expression yields to true: \u2022 \"North America\" has \"america\" contains and has are case insensitive by default. A case sensitive match can be achieved by adding the suffix _cs: contains_cs / has_cs Match on values starting with or ending with a specific string : Bash T | where Computer startswith \"contoso\" \u2022 Ending with a specific string: endswith startswith and endswith are case insensitive by default. A case sensitive match can be achieved by adding the suffix _cs: startswith_cs / endswith_cs Match on multiple string values : T | where Computer in (\"ContosoAppSrv1\", \"ContosoSQLSrv1\") \u2022 Not equal to: !in \u2022 Case insensitive: in~ \u2022 Case insensitive and not equal to: !in~ Match based on a regular expression : T | where Computer matches regex \"^Contoso.*\" \u2022 KQL uses the re2 library and also complies with that syntax. Troubleshooting your regex can be done on regex101.com. Select the regex Flavor \u201cGolang\u201d which also makes use of re2. A not equal to match can be done using the not() function: Bash T | where not ( Computer matches regex \"^Contoso.*\" ) A case insensitive match can be achieved by providing the i flag: Bash T | where Computer matches regex \"(?i)^contoso.*\" Named expressions and user-defined functions \u00b6 Use the let statement to bind names to expressions . See below two examples of a named expression. Of course, much more complex expression can be created. Such as complete queries that can be nested inside another query (i.e. sub-query). For sub-queries consider the use of the materialize() function when the sub-query is called multiple times. Take into account the semicolon at the end of the let statement: Bash let _SearchWindow = ago ( 24h ) ; T | where TimeGenerated > _SearchWindow let _computers = dynamic ([ \"ContosoAppSrv1\" , \"ContosoSQLSrv1\" ]) ; T | where Computer in ( _computers ) The let statement can be used in many other useful ways. Such as to create user-defined functions . More info on the let statement can be found here . Visualizations \u00b6 The render operator can be used to create visualizations. Besides the below example, more types of visualizations are possible. More info can be found here . (Pie charts are not the most telling graphics, but the support for the render operator is limited on the demo environment.) Bash SecurityAlert | summarize Count = count () by Alert = strcat ( DisplayName, \" (\" , toupper ( AlertSeverity ) , \")\" ) | sort by Count | render piechart Join tables \u00b6 KQL has the ability to join tables. In this example, we join some of the events in the SecurityAlert table with process creation events (event ID 4688) from the SecurityEvent table. More information on joining tables can be found here . This query serves purely as an example to explain the join operator because all process data is contained within the column Entities of the SecurityAlert table Bash SecurityAlert | extend _ProcessId = extractjson ( \" $ .process id\" , ExtendedProperties ) , _ProcessCommandLine = tolower ( extractjson ( \" $ .command line\" , ExtendedProperties )) , _HostName = tolower ( extractjson ( \" $ [0].HostName\" , Entities )) | join kind = inner ( SecurityEvent | where EventID == 4688 | extend _HostName = tolower ( Computer ) | extend _ProcessCommandLine = tolower ( CommandLine ) ) on $left ._ProcessId == $right .NewProcessId, _HostName, _ProcessCommandLine","title":"Kusto Query Language (KQL)"},{"location":"home/kusto-overview/#kusto-query-language-kql","text":"Official documentation KQL Documentation Course Pluralsight course what is KQL and where is it used? KQL is an open source language created by Microsoft to query big data sets stored in the Azure cloud. These queries can also be used in alerting rules. Some examples of services/products hosted in Azure that make use of KQL are: * Azure Data Explorer * Log Analytics * Sentinel (this is Microsoft\u2019s cloud SIEM solution that makes use of a Log Analytics workspace as its backend) * Microsoft Defender ATP Sample or Demo queries can be tested at DemoLogs","title":"Kusto Query Language (KQL)"},{"location":"home/kusto-overview/#generic","text":"A string literal with a backslash requires escaping by a backslash: \"a string literal with a \\\\ needs to be escaped\" The same can be achieved using a verbatim string literal by putting the @ sign in front: @\"a verbatim string literal with a \\\\ that does not need to be escaped\" More info on escaping string data types can be found here . Add comments to your query with a double forward slash: // This is a comment The where operator and the pipe ( | ) delimiter are essential in writing KQL queries. where is used to filter rows from a table. In this example we filter on events from a source, the table SecurityEvent, where the column Computer is equal to \"ContosoAppSrv1\", and count the number of results: Bash SecurityEvent | where Computer == \"ContosoAppSrv1\" | count The pipe is used to separate data transformation operators. Such as: where Computer == \"ContosoAppSrv1\". The result can be piped to a new operator. For example, to count the number for rows: | count Only include events from the last 24 hours using the ago() function: Bash T | where TimeGenerated > ago ( 24h ) For performance reasons always use time filters first in your query. The ago() function supports multiple types of timespans. More info can be found here . For example: \u2022 1d 1 day \u2022 10m 10 minutes \u2022 30s 30 seconds Select and customize the columns from the resulting table of your query with the project operator. \u2022 Specify the columns to include : Bash T | project TimeGenerated, EventID, Account, Computer, LogonType \u2022 Rename columns . In this example we renamed the column Account to UserName: Bash T | project TimeGenerated, EventID, UserName = Account, Computer, LogonType \u2022 Remove columns with project-away: Bash T | project-away EventSourceName, Task, Level Add calculated columns to the result using the extend operator: Bash T | extend EventAge = now () -TimeGenerated Count the number of records using the count operator: Bash T | count Match based on conditions using logical operators . For example: \u2022 T | where EventID == 4624 and LogonType == 3 \u2022 T | where EventID == 4624 or EventID == 4625 \u2022 T | where (EventID == 4624 and LogonType == 3) or EventID == 4625 Aggerate results from your query with the summarize operator: \u2022 Aggregate on multiple columns: Bash T | summarize by Computer, Account \u2022 Aggregate on multiple columns and return the count of the group: Bash T | summarize count () by Computer, Account Besides count() many more very useful aggregation functions exist. An overview can be found here . Sort the rows of the result using the sort operator: Bash T | where EventID == 4624 | summarize count () by AuthenticationPackageName | sort by count_ By default, rows are sorted in descending order. Sorting in ascending order is also possible: \u2022 sort by count_ asc \u2022 Descending order: desc Concatenate values . The result will be a string data type: Bash T | project example = strcat ( EventID, \" - \" , Channel ) A variable number of values can be passed through the strcat function. If values are not a string, they will be forcibly converted to a string.","title":"Generic"},{"location":"home/kusto-overview/#numerical-search","text":"Search for a specific value : T | where EventID == 4688 \u2022 Not equal to: != All of the numerical operators can be found here . Search for a value less or greater than : T | where EventID == 4688 | summarize count() by Process | where count_ < 5 \u2022 Greater: > \u2022 Less or Equal: <= \u2022 Greater or Equal: >= Match on multiple numeric values : Bash T | where EventID in ( 4624 , 4625 )","title":"Numerical search"},{"location":"home/kusto-overview/#extract-values","text":"Extract values from a string or JSON data . For example, extract the \u201cprocess name\u201d using a regular expression (if you are less familiar with regular expressions have a look at the split and parse function): Bash SecurityAlert | extend _ProcessName = extract ( '\"process name\": \"(.*)\"' , 1 , ExtendedProperties ) Because the column ExtendedProperties contains JSON data you can also use the function extractjson(): Bash SecurityAlert | extend _ProcessName = extractjson ( \" $ .process name\" , ExtendedProperties ) If you need to extract multiple elements from JSON data, stored as a string, you can use the function parse_json(). Use the dot notation if the data is of the type dictionary or a list of dictionaries in an array. One way to find out is through the gettype() function. To play with data stored as a dictionary have a look at the help cluster in the Azure Data Explorer (table: StormEvents, column: StormSummary).","title":"Extract values"},{"location":"home/kusto-overview/#string-search","text":"Search across all tables and columns : search \" KEYWORD \" \u2022 Keep in mind that this is a performance intensive operation. Search for a specific value : T | where ProcessName == @\"C:\\Windows\\System32\\regsvr32.exe\" A not equal to match is done by adding an exclamation mark as prefix: \u2022 Equal to: == \u2022 Not equal to: != This is also supported in a similar way for other string operators A case insensitive match can be achieved using a tilde: \u2022 Case sensitive: == \u2022 Case insensitive: =~ \u2022 Case insensitive and not equal to: !~ This is also supported in a similar way for other string operators . Match on values that contain a specific string : Bash T | where CommandLine contains \"guest\" Because has is more performant, it\u2019s advised to use has over contains when searching for full keywords. The following expression yields to true: \u2022 \"North America\" has \"america\" contains and has are case insensitive by default. A case sensitive match can be achieved by adding the suffix _cs: contains_cs / has_cs Match on values starting with or ending with a specific string : Bash T | where Computer startswith \"contoso\" \u2022 Ending with a specific string: endswith startswith and endswith are case insensitive by default. A case sensitive match can be achieved by adding the suffix _cs: startswith_cs / endswith_cs Match on multiple string values : T | where Computer in (\"ContosoAppSrv1\", \"ContosoSQLSrv1\") \u2022 Not equal to: !in \u2022 Case insensitive: in~ \u2022 Case insensitive and not equal to: !in~ Match based on a regular expression : T | where Computer matches regex \"^Contoso.*\" \u2022 KQL uses the re2 library and also complies with that syntax. Troubleshooting your regex can be done on regex101.com. Select the regex Flavor \u201cGolang\u201d which also makes use of re2. A not equal to match can be done using the not() function: Bash T | where not ( Computer matches regex \"^Contoso.*\" ) A case insensitive match can be achieved by providing the i flag: Bash T | where Computer matches regex \"(?i)^contoso.*\"","title":"String search"},{"location":"home/kusto-overview/#named-expressions-and-user-defined-functions","text":"Use the let statement to bind names to expressions . See below two examples of a named expression. Of course, much more complex expression can be created. Such as complete queries that can be nested inside another query (i.e. sub-query). For sub-queries consider the use of the materialize() function when the sub-query is called multiple times. Take into account the semicolon at the end of the let statement: Bash let _SearchWindow = ago ( 24h ) ; T | where TimeGenerated > _SearchWindow let _computers = dynamic ([ \"ContosoAppSrv1\" , \"ContosoSQLSrv1\" ]) ; T | where Computer in ( _computers ) The let statement can be used in many other useful ways. Such as to create user-defined functions . More info on the let statement can be found here .","title":"Named expressions and user-defined functions"},{"location":"home/kusto-overview/#visualizations","text":"The render operator can be used to create visualizations. Besides the below example, more types of visualizations are possible. More info can be found here . (Pie charts are not the most telling graphics, but the support for the render operator is limited on the demo environment.) Bash SecurityAlert | summarize Count = count () by Alert = strcat ( DisplayName, \" (\" , toupper ( AlertSeverity ) , \")\" ) | sort by Count | render piechart","title":"Visualizations"},{"location":"home/kusto-overview/#join-tables","text":"KQL has the ability to join tables. In this example, we join some of the events in the SecurityAlert table with process creation events (event ID 4688) from the SecurityEvent table. More information on joining tables can be found here . This query serves purely as an example to explain the join operator because all process data is contained within the column Entities of the SecurityAlert table Bash SecurityAlert | extend _ProcessId = extractjson ( \" $ .process id\" , ExtendedProperties ) , _ProcessCommandLine = tolower ( extractjson ( \" $ .command line\" , ExtendedProperties )) , _HostName = tolower ( extractjson ( \" $ [0].HostName\" , Entities )) | join kind = inner ( SecurityEvent | where EventID == 4688 | extend _HostName = tolower ( Computer ) | extend _ProcessCommandLine = tolower ( CommandLine ) ) on $left ._ProcessId == $right .NewProcessId, _HostName, _ProcessCommandLine","title":"Join tables"},{"location":"home/starting-azure-service-fabric/","text":"Starting Azure Service Fabric \u00b6 Options for building Microservices in Azure: \u00b6 Azure Kubernetes Service - Container Orchestrator Azure Service Fabric - Microservice framework and orchestrator that solves many of problems Service Communication Service discovery Telemetry Provision and upgrade Testing locally Manage downtimes Scaling in and out Azure Functions Programming Models provided by Service Fabric \u00b6 Reliable services stateless (similar to console app) stateful Reliable actors - Virtual actor design pattern built on top of stateful reliable services framework to handle massive amount of client request with enormous computing power Guest executables - Wrap any existing application to run on Service Fabric Containers Installing Service Fabric \u00b6 Service Fabric is best in cloud environment but can be installed in On Premise as well as in Developer workstation and the is no difference the underlying Service Fabric. OneBox - Azure Service Fabric Cluster that can be deployed to a single dev machine. Tip Text Only Tools: ```markdown * Visual Studio 2019 (Community Version would work as well) * Service Fabric tooling (it's a part of Visual studio components) * Service Fabric SDK (can be installed as isolated package or use Web Platform Installer) ``` Creating Service Fabric Services \u00b6 Service Fabric State \u00b6 The minimum set of replica to achieve data consistency is called quorum. The size is usually 3 nodes. Service state consist of local storage to save persist state therefore is very fast. graph LR w(write) -.-> a(Service - primary) w(write) -.-> b(Replica 1) w(write) -.-> c(Replica 2) subgraph quorum a(Service - primary) & b(Replica 1) & c(Replica 2) end Let's create application with these services We will create services * Web Server (API) - It's a stateless service and only act as a facade. * Product Catalog - It's a stateful service. graph LR a(Web Server API) --> b(Product Catalog Service) & c(Checkout Service)--> d(User Service Actors) Both of these services are reliable services as they runs in the background. Reliable service has access to ASF API which is all about microservies, scaling, health report and many more. It has various communication model as http, ftp, websocket etc. It's all about low latency and high speed. It also has access to reliable storage. All these features comes with simple programming model. Creating Service Application \u00b6 When we create a new service C# using System ; using System.Diagnostics ; using System.Threading ; using System.Threading.Tasks ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { internal static class Program { /// <summary> /// This is the entry point of the service host process. /// </summary> private static void Main () { try { // The ServiceManifest.XML file defines one or more service type names. // Registering a service maps a service type name to a .NET type. // When Service Fabric creates an instance of this service type, // an instance of the class is created in this host process. ServiceRuntime . RegisterServiceAsync ( \"ECommerce.ProductCatalogType\" , context => new ProductCatalog ( context )). GetAwaiter (). GetResult (); ServiceEventSource . Current . ServiceTypeRegistered ( Process . GetCurrentProcess (). Id , typeof ( ProductCatalog ). Name ); // Prevents this host process from terminating so services keep running. Thread . Sleep ( Timeout . Infinite ); } catch ( Exception e ) { ServiceEventSource . Current . ServiceHostInitializationFailed ( e . ToString ()); throw ; } } } } Let's checkout what happens in the code graph TB a(Register the Reliable Service) --> b(Log Reliable Service has started) --> c(Sleep forever) There is another class get create which ASF creates the instance during run time and this is the entry point for the service. ProductCatalog.cs C# using System ; using System.Collections.Generic ; using System.Fabric ; using System.Linq ; using System.Threading ; using System.Threading.Tasks ; using Microsoft.ServiceFabric.Data.Collections ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { /// <summary> /// An instance of this class is created for each service replica by the Service Fabric runtime. /// </summary> internal sealed class ProductCatalog : StatefulService { public ProductCatalog ( StatefulServiceContext context ) : base ( context ) { } /// <summary> /// Optional override to create listeners (e.g., HTTP, Service Remoting, WCF, etc.) for this service replica to handle client or user requests. /// </summary> /// <remarks> /// For more information on service communication, see https://aka.ms/servicefabricservicecommunication /// </remarks> /// <returns>A collection of listeners.</returns> protected override IEnumerable < ServiceReplicaListener > CreateServiceReplicaListeners () { return new ServiceReplicaListener [ 0 ]; } /// <summary> /// This is the main entry point for your service replica. /// This method executes when this replica of your service becomes primary and has write status. /// </summary> /// <param name=\"cancellationToken\">Canceled when Service Fabric needs to shut down this service replica.</param> protected override async Task RunAsync ( CancellationToken cancellationToken ) { // TODO: Replace the following sample code with your own logic // or remove this RunAsync override if it's not needed in your service. var myDictionary = await this . StateManager . GetOrAddAsync < IReliableDictionary < string , long >> ( \"myDictionary\" ); while ( true ) { cancellationToken . ThrowIfCancellationRequested (); using ( var tx = this . StateManager . CreateTransaction ()) { var result = await myDictionary . TryGetValueAsync ( tx , \"Counter\" ); ServiceEventSource . Current . ServiceMessage ( this . Context , \"Current Counter Value: {0}\" , result . HasValue ? result . Value . ToString () : \"Value does not exist.\" ); await myDictionary . AddOrUpdateAsync ( tx , \"Counter\" , 0 , ( key , value ) => ++ value ); // If an exception is thrown before calling CommitAsync, the transaction aborts, all changes are // discarded, and nothing is saved to the secondary replicas. await tx . CommitAsync (); } await Task . Delay ( TimeSpan . FromSeconds ( 1 ), cancellationToken ); } } } } Service Lifecycle \u00b6 Startup sequenceDiagram participant ASF_Runtime participant Service_Instance loop ASF_Runtime->>ASF_Runtime: create service instance end ASF_Runtime->>Service_Instance: ask to create listeners Service_Instance-->>ASF_Runtime: listeners[] loop for each listener ASF_Runtime->Service_Instance: open listener end ASF_Runtime->>Service_Instance: RunAsync(cancellationToken) Shutdown sequenceDiagram participant ASF_Runtime participant Service_Instance loop ASF_Runtime->>ASF_Runtime: cancel cancellation token passed in RunAsync end loop for each listener ASF_Runtime->Service_Instance: close end loop ASF_Runtime->>ASF_Runtime: destroy class instance end Tip Text Only It's very important to **always respond to cancellationToken event as soon as possible** if we run RunAsync all the time in the background Creating Product Catalog Service \u00b6 Product Entity classDiagram class Product Product : +ProductId GUID Product : +Name string Product : +Description string Product : +Price double Product : +Availability int Now lets add IProuductRepostory and Product in library project and reference it from ProductCatalog Service project. IProductRepository C# using System ; using System.Collections.Generic ; using System.Text ; using System.Threading.Tasks ; namespace ECommerce.ProductCatalog.Model { public interface IProductRepository { Task < IEnumerable < Product >> GetProducts (); Task AddProudct ( Product product ); } } Product C# using System ; namespace ECommerce.ProductCatalog.Model { public class Product { public Guid Id { get ; set ; } public string Name { get ; set ; } public string Description { get ; set ; } public double Price { get ; set ; } public int Availability { get ; set ; } } } Let's create ServiceFabricProductRepository nad update ProductCatalog in ProductCatalog project. ServiceFabricProductRepository C# using System ; using System.Collections.Generic ; using System.Text ; using System.Threading ; using System.Threading.Tasks ; using ECommerce.ProductCatalog.Model ; using Microsoft.ServiceFabric.Data ; using Microsoft.ServiceFabric.Data.Collections ; namespace ECommerce.ProductCatalog { class ServcieFabricProductRepostiory : IProductRepository { private readonly IReliableStateManager _stateManager ; public ServcieFabricProductRepostiory ( IReliableStateManager stateManager ) { _stateManager = stateManager ; } public async Task AddProudct ( Product product ) { var products = await _stateManager . GetOrAddAsync < IReliableDictionary < Guid , Product >> ( \"product\" ); using ( ITransaction tx = _stateManager . CreateTransaction ()) { await products . AddOrUpdateAsync ( tx , product . Id , product , ( id , value ) => product ); await tx . CommitAsync (); } } public async Task < IEnumerable < Product >> GetProducts () { var products = await _stateManager . GetOrAddAsync < IReliableDictionary < Guid , Product >> ( \"product\" ); var result = new List < Product > (); using ( ITransaction tx = _stateManager . CreateTransaction ()) { var allProducts = await products . CreateEnumerableAsync ( tx , EnumerationMode . Unordered ); using ( var enumerator = allProducts . GetAsyncEnumerator ()) { while ( await enumerator . MoveNextAsync ( CancellationToken . None )) { result . Add ( enumerator . Current . Value ); } } } return result ; } } } ProductCatalog C# using System ; using System.Collections.Generic ; using System.Fabric ; using System.Linq ; using System.Threading ; using System.Threading.Tasks ; using ECommerce.ProductCatalog.Model ; using Microsoft.ServiceFabric.Data.Collections ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { /// <summary> /// An instance of this class is created for each service replica by the Service Fabric runtime. /// </summary> internal sealed class ProductCatalog : StatefulService { private ServcieFabricProductRepostiory _repo ; public ProductCatalog ( StatefulServiceContext context ) : base ( context ) { } /// <summary> /// Optional override to create listeners (e.g., HTTP, Service Remoting, WCF, etc.) for this service replica to handle client or user requests. /// </summary> /// <remarks> /// For more information on service communication, see https://aka.ms/servicefabricservicecommunication /// </remarks> /// <returns>A collection of listeners.</returns> protected override IEnumerable < ServiceReplicaListener > CreateServiceReplicaListeners () { return new ServiceReplicaListener [ 0 ]; } /// <summary> /// This is the main entry point for your service replica. /// This method executes when this replica of your service becomes primary and has write status. /// </summary> /// <param name=\"cancellationToken\">Canceled when Service Fabric needs to shut down this service replica.</param> protected override async Task RunAsync ( CancellationToken cancellationToken ) { var product1 = new Product { Id = Guid . NewGuid (), Name = \"Dell Monitor\" , Description = \" Computer Monitor\" , Price = 500 , Availability = 100 }; var product2 = new Product { Id = Guid . NewGuid (), Name = \"Keyboard\" , Description = \" Computer Accesories\" , Price = 510 , Availability = 110 }; var product3 = new Product { Id = Guid . NewGuid (), Name = \"Mouse\" , Description = \" Computer Accesories\" , Price = 520 , Availability = 120 }; _repo = new ServcieFabricProductRepostiory ( this . StateManager ); await _repo . AddProudct ( product1 ); await _repo . AddProudct ( product2 ); await _repo . AddProudct ( product3 ); var all = await _repo . GetProducts (); } } } At this point we should be able to run the application and check there are 3 products in all variable which is await of GetProducts() This is how the solution should look like: :::warning Visual Studio need to run in elevated mode as Admin to allow access network level resources ::: Creating a Web API \u00b6 To add new Stateless ASP .net Core Web API, right click on ECommerce project and add New Service Fabric Service and choose, stateless asp.net core API. Program.cs is console application very similar to previous one which resisters the service type. API C# using System.Collections.Generic ; using System.Fabric ; using System.IO ; using Microsoft.AspNetCore.Hosting ; using Microsoft.Extensions.DependencyInjection ; using Microsoft.ServiceFabric.Services.Communication.AspNetCore ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.API { /// <summary> /// The FabricRuntime creates an instance of this class for each service type instance. /// </summary> internal sealed class API : StatelessService { public API ( StatelessServiceContext context ) : base ( context ) { } /// <summary> /// Optional override to create listeners (like tcp, http) for this service instance. /// </summary> /// <returns>The collection of listeners.</returns> protected override IEnumerable < ServiceInstanceListener > CreateServiceInstanceListeners () { return new ServiceInstanceListener [] { new ServiceInstanceListener ( serviceContext => new KestrelCommunicationListener ( serviceContext , \"ServiceEndpoint\" , ( url , listener ) => { ServiceEventSource . Current . ServiceMessage ( serviceContext , $\"Starting Kestrel on {url}\" ); return new WebHostBuilder () . UseKestrel () . ConfigureServices ( services => services . AddSingleton < StatelessServiceContext > ( serviceContext )) . UseContentRoot ( Directory . GetCurrentDirectory ()) . UseStartup < Startup > () . UseServiceFabricIntegration ( listener , ServiceFabricIntegrationOptions . None ) . UseUrls ( url ) . Build (); })) }; } } } KestrelCommunicationListener is standard fabric listener which bootstrap asp.net core runtime and configure it to run inside service fabric environment. Let's add Product Api Model C# using System ; using Newtonsoft.Json ; namespace ECommerce.API.Model { public class ApiProduct { [JsonProperty(\"id\")] public Guid Id { get ; set ; } [JsonProperty(\"name\")] public string Name { get ; set ; } [JsonProperty(\"description\")] public string Description { get ; set ; } [JsonProperty(\"price\")] public double Price { get ; set ; } [JsonProperty(\"isAvailable\")] public int IsAvailable { get ; set ; } } } Here is new project structure: Communicating between two services \u00b6 Let's say there are 3 nodes in a cluster graph LR subgraph node3 a(Service API - active) & b(Catalog Service - passive) end subgraph node2 c(Service API - active) & d(Catalog Service - active) end subgraph node1 e(Service API - active) & f(Catalog Service - passive) end Since API are stateless they are active in all nodes but Catalog Service is stateful so Service Fabric makes one active and other two passive in case of fail over. Tip Text Only Never assume a service is running in a fixed location. Services in Service Fabric can move around and change roles all the time during the lifetime of the application and even between requests. Therefore, before making the request we should always query the service location from Service Fabric runtime. Service Fabric is protocol agnostic. Out of the box there are three protocols: * WCF * HTTP * Service Remoting (Default protocol for reliable communication) Benefits of using Service Remoting: * Automatic service address resolution * Establishing connection * Retries * Error handling * Strong typed * Fast For the purpose of remoting we are adding Service interface in the Model IProductCatalogService C# using System.Threading.Tasks ; using Microsoft.ServiceFabric.Services.Remoting ; namespace ECommerce.ProductCatalog.Model { public interface IProductCatalogService : IService { Task < Product [] > GetALlProductsAsync (); Task AddProductAsync ( Product product ); } } We need to install the remoting service library. And implement IProductCatalogService in ProductCatalog C# using System ; using System.Collections.Generic ; using System.Fabric ; using System.Linq ; using System.Threading ; using System.Threading.Tasks ; using ECommerce.ProductCatalog.Model ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Remoting.V2.FabricTransport.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { /// <summary> /// An instance of this class is created for each service replica by the Service Fabric runtime. /// </summary> internal sealed class ProductCatalog : StatefulService , IProductCatalogService { private ServiceFabricProductRepository _repo ; public ProductCatalog ( StatefulServiceContext context ) : base ( context ) { } public async Task AddProductAsync ( Product product ) { await _repo . AddProduct ( product ); } public async Task < Product [] > GetALlProductsAsync () { return ( await _repo . GetProducts ()). ToArray (); } /// <summary> /// Optional override to create listeners (e.g., HTTP, Service Remoting, WCF, etc.) for this service replica to handle client or user requests. /// </summary> /// <remarks> /// For more information on service communication, see https://aka.ms/servicefabricservicecommunication /// </remarks> /// <returns>A collection of listeners.</returns> protected override IEnumerable < ServiceReplicaListener > CreateServiceReplicaListeners () { return new [] { new ServiceReplicaListener ( context => new FabricTransportServiceRemotingListener ( context , this )) }; } /// <summary> /// This is the main entry point for your service replica. /// This method executes when this replica of your service becomes primary and has write status. /// </summary> /// <param name=\"cancellationToken\">Canceled when Service Fabric needs to shut down this service replica.</param> protected override async Task RunAsync ( CancellationToken cancellationToken ) { var product1 = new Product { Id = Guid . NewGuid (), Name = \"Dell Monitor\" , Description = \" Computer Monitor\" , Price = 500 , Availability = 100 }; var product2 = new Product { Id = Guid . NewGuid (), Name = \"Keyboard\" , Description = \" Computer Accessories\" , Price = 510 , Availability = 110 }; var product3 = new Product { Id = Guid . NewGuid (), Name = \"Mouse\" , Description = \" Computer Accessories\" , Price = 520 , Availability = 120 }; _repo = new ServiceFabricProductRepository ( this . StateManager ); await _repo . AddProduct ( product1 ); await _repo . AddProduct ( product2 ); await _repo . AddProduct ( product3 ); var all = await _repo . GetProducts (); } } } Tip Text Only The return type must be array as the service remoting doesn't understand IEnumerable and it needs to transform over the networks it should be simple type. Finally it's time to add proxy in API to connect to ProductCatalogService. ProductController C# using System ; using System.Collections.Generic ; using System.Linq ; using System.Threading.Tasks ; using ECommerce.API.Model ; using ECommerce.ProductCatalog.Model ; using Microsoft.AspNetCore.Mvc ; using Microsoft.ServiceFabric.Services.Client ; using Microsoft.ServiceFabric.Services.Remoting.Client ; using Microsoft.ServiceFabric.Services.Remoting.V2.FabricTransport.Client ; namespace ECommerce.API.Controllers { [ApiController] [Route(\"[controller] \")] public class ProductsController : ControllerBase { private readonly IProductCatalogService _service ; public ProductsController () { var proxyFactory = new ServiceProxyFactory ( c => new FabricTransportServiceRemotingClientFactory ()); _service = proxyFactory . CreateServiceProxy < IProductCatalogService > ( new Uri ( \"fabric:/ECommerce/ECommerce.ProductCatalog\" ), new ServicePartitionKey ( 0 )); } [HttpGet] public async Task < IEnumerable < ApiProduct >> GetAsync () { var allProducts = await _service . GetAllProductsAsync (); return allProducts . Select ( p => new ApiProduct { Id = p . Id , Name = p . Name , Description = p . Description , Price = p . Price , IsAvailable = p . Availability > 0 }); } [HttpPost] public async Task PostAsync ([ FromBody ] ApiProduct product ) { var newProduct = new Product () { Id = Guid . NewGuid (), Name = product . Name , Description = product . Description , Price = product . Price , Availability = 100 }; await _service . AddProductAsync ( newProduct ); } } } Launch the application and test it with Postman for both get and post. Tip Text Only URI Format for proxy: fabric/ApplicationName/ServiceName Eg: fabric:/ECommerce/ProductCatalog End to End flow \u00b6 Here is end to end flow: API: Stateless service ProductsController (Product_API) ProductCatalog: StatefulService, IProductCatalogService sequenceDiagram participant ProductsController participant ServiceProxyFactory participant ProductCatalog participant SFProductRepository participant IReliableStateManager ProductsController->>ServiceProxyFactory: CreateServiceProxy ServiceProxyFactory-->>ProductsController: _service ProductsController->>ProductCatalog :GetAllProductsAsync() loop new ServiceReplicaListener of ctx ProductCatalog->>ProductCatalog: new FabricTransportServiceRemotingListener end ProductCatalog->>SFProductRepository: GetProducts() SFProductRepository->>IReliableStateManager: products.CreateEnumerableAsync SFProductRepository-->>ProductCatalog: result ProductCatalog-->>ProductsController: result.ToArray() Exploring Actor Model Support \u00b6 Managing State \u00b6 Getting Ready for Deployment \u00b6 Test \u00b6","title":"Starting Azure Service Fabric "},{"location":"home/starting-azure-service-fabric/#starting-azure-service-fabric","text":"","title":"Starting Azure Service Fabric"},{"location":"home/starting-azure-service-fabric/#options-for-building-microservices-in-azure","text":"Azure Kubernetes Service - Container Orchestrator Azure Service Fabric - Microservice framework and orchestrator that solves many of problems Service Communication Service discovery Telemetry Provision and upgrade Testing locally Manage downtimes Scaling in and out Azure Functions","title":"Options for building Microservices in Azure:"},{"location":"home/starting-azure-service-fabric/#programming-models-provided-by-service-fabric","text":"Reliable services stateless (similar to console app) stateful Reliable actors - Virtual actor design pattern built on top of stateful reliable services framework to handle massive amount of client request with enormous computing power Guest executables - Wrap any existing application to run on Service Fabric Containers","title":"Programming Models provided by Service Fabric"},{"location":"home/starting-azure-service-fabric/#installing-service-fabric","text":"Service Fabric is best in cloud environment but can be installed in On Premise as well as in Developer workstation and the is no difference the underlying Service Fabric. OneBox - Azure Service Fabric Cluster that can be deployed to a single dev machine. Tip Text Only Tools: ```markdown * Visual Studio 2019 (Community Version would work as well) * Service Fabric tooling (it's a part of Visual studio components) * Service Fabric SDK (can be installed as isolated package or use Web Platform Installer) ```","title":"Installing Service Fabric"},{"location":"home/starting-azure-service-fabric/#creating-service-fabric-services","text":"","title":"Creating Service Fabric Services"},{"location":"home/starting-azure-service-fabric/#service-fabric-state","text":"The minimum set of replica to achieve data consistency is called quorum. The size is usually 3 nodes. Service state consist of local storage to save persist state therefore is very fast. graph LR w(write) -.-> a(Service - primary) w(write) -.-> b(Replica 1) w(write) -.-> c(Replica 2) subgraph quorum a(Service - primary) & b(Replica 1) & c(Replica 2) end Let's create application with these services We will create services * Web Server (API) - It's a stateless service and only act as a facade. * Product Catalog - It's a stateful service. graph LR a(Web Server API) --> b(Product Catalog Service) & c(Checkout Service)--> d(User Service Actors) Both of these services are reliable services as they runs in the background. Reliable service has access to ASF API which is all about microservies, scaling, health report and many more. It has various communication model as http, ftp, websocket etc. It's all about low latency and high speed. It also has access to reliable storage. All these features comes with simple programming model.","title":"Service Fabric State"},{"location":"home/starting-azure-service-fabric/#creating-service-application","text":"When we create a new service C# using System ; using System.Diagnostics ; using System.Threading ; using System.Threading.Tasks ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { internal static class Program { /// <summary> /// This is the entry point of the service host process. /// </summary> private static void Main () { try { // The ServiceManifest.XML file defines one or more service type names. // Registering a service maps a service type name to a .NET type. // When Service Fabric creates an instance of this service type, // an instance of the class is created in this host process. ServiceRuntime . RegisterServiceAsync ( \"ECommerce.ProductCatalogType\" , context => new ProductCatalog ( context )). GetAwaiter (). GetResult (); ServiceEventSource . Current . ServiceTypeRegistered ( Process . GetCurrentProcess (). Id , typeof ( ProductCatalog ). Name ); // Prevents this host process from terminating so services keep running. Thread . Sleep ( Timeout . Infinite ); } catch ( Exception e ) { ServiceEventSource . Current . ServiceHostInitializationFailed ( e . ToString ()); throw ; } } } } Let's checkout what happens in the code graph TB a(Register the Reliable Service) --> b(Log Reliable Service has started) --> c(Sleep forever) There is another class get create which ASF creates the instance during run time and this is the entry point for the service. ProductCatalog.cs C# using System ; using System.Collections.Generic ; using System.Fabric ; using System.Linq ; using System.Threading ; using System.Threading.Tasks ; using Microsoft.ServiceFabric.Data.Collections ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { /// <summary> /// An instance of this class is created for each service replica by the Service Fabric runtime. /// </summary> internal sealed class ProductCatalog : StatefulService { public ProductCatalog ( StatefulServiceContext context ) : base ( context ) { } /// <summary> /// Optional override to create listeners (e.g., HTTP, Service Remoting, WCF, etc.) for this service replica to handle client or user requests. /// </summary> /// <remarks> /// For more information on service communication, see https://aka.ms/servicefabricservicecommunication /// </remarks> /// <returns>A collection of listeners.</returns> protected override IEnumerable < ServiceReplicaListener > CreateServiceReplicaListeners () { return new ServiceReplicaListener [ 0 ]; } /// <summary> /// This is the main entry point for your service replica. /// This method executes when this replica of your service becomes primary and has write status. /// </summary> /// <param name=\"cancellationToken\">Canceled when Service Fabric needs to shut down this service replica.</param> protected override async Task RunAsync ( CancellationToken cancellationToken ) { // TODO: Replace the following sample code with your own logic // or remove this RunAsync override if it's not needed in your service. var myDictionary = await this . StateManager . GetOrAddAsync < IReliableDictionary < string , long >> ( \"myDictionary\" ); while ( true ) { cancellationToken . ThrowIfCancellationRequested (); using ( var tx = this . StateManager . CreateTransaction ()) { var result = await myDictionary . TryGetValueAsync ( tx , \"Counter\" ); ServiceEventSource . Current . ServiceMessage ( this . Context , \"Current Counter Value: {0}\" , result . HasValue ? result . Value . ToString () : \"Value does not exist.\" ); await myDictionary . AddOrUpdateAsync ( tx , \"Counter\" , 0 , ( key , value ) => ++ value ); // If an exception is thrown before calling CommitAsync, the transaction aborts, all changes are // discarded, and nothing is saved to the secondary replicas. await tx . CommitAsync (); } await Task . Delay ( TimeSpan . FromSeconds ( 1 ), cancellationToken ); } } } }","title":"Creating Service Application"},{"location":"home/starting-azure-service-fabric/#service-lifecycle","text":"Startup sequenceDiagram participant ASF_Runtime participant Service_Instance loop ASF_Runtime->>ASF_Runtime: create service instance end ASF_Runtime->>Service_Instance: ask to create listeners Service_Instance-->>ASF_Runtime: listeners[] loop for each listener ASF_Runtime->Service_Instance: open listener end ASF_Runtime->>Service_Instance: RunAsync(cancellationToken) Shutdown sequenceDiagram participant ASF_Runtime participant Service_Instance loop ASF_Runtime->>ASF_Runtime: cancel cancellation token passed in RunAsync end loop for each listener ASF_Runtime->Service_Instance: close end loop ASF_Runtime->>ASF_Runtime: destroy class instance end Tip Text Only It's very important to **always respond to cancellationToken event as soon as possible** if we run RunAsync all the time in the background","title":"Service Lifecycle"},{"location":"home/starting-azure-service-fabric/#creating-product-catalog-service","text":"Product Entity classDiagram class Product Product : +ProductId GUID Product : +Name string Product : +Description string Product : +Price double Product : +Availability int Now lets add IProuductRepostory and Product in library project and reference it from ProductCatalog Service project. IProductRepository C# using System ; using System.Collections.Generic ; using System.Text ; using System.Threading.Tasks ; namespace ECommerce.ProductCatalog.Model { public interface IProductRepository { Task < IEnumerable < Product >> GetProducts (); Task AddProudct ( Product product ); } } Product C# using System ; namespace ECommerce.ProductCatalog.Model { public class Product { public Guid Id { get ; set ; } public string Name { get ; set ; } public string Description { get ; set ; } public double Price { get ; set ; } public int Availability { get ; set ; } } } Let's create ServiceFabricProductRepository nad update ProductCatalog in ProductCatalog project. ServiceFabricProductRepository C# using System ; using System.Collections.Generic ; using System.Text ; using System.Threading ; using System.Threading.Tasks ; using ECommerce.ProductCatalog.Model ; using Microsoft.ServiceFabric.Data ; using Microsoft.ServiceFabric.Data.Collections ; namespace ECommerce.ProductCatalog { class ServcieFabricProductRepostiory : IProductRepository { private readonly IReliableStateManager _stateManager ; public ServcieFabricProductRepostiory ( IReliableStateManager stateManager ) { _stateManager = stateManager ; } public async Task AddProudct ( Product product ) { var products = await _stateManager . GetOrAddAsync < IReliableDictionary < Guid , Product >> ( \"product\" ); using ( ITransaction tx = _stateManager . CreateTransaction ()) { await products . AddOrUpdateAsync ( tx , product . Id , product , ( id , value ) => product ); await tx . CommitAsync (); } } public async Task < IEnumerable < Product >> GetProducts () { var products = await _stateManager . GetOrAddAsync < IReliableDictionary < Guid , Product >> ( \"product\" ); var result = new List < Product > (); using ( ITransaction tx = _stateManager . CreateTransaction ()) { var allProducts = await products . CreateEnumerableAsync ( tx , EnumerationMode . Unordered ); using ( var enumerator = allProducts . GetAsyncEnumerator ()) { while ( await enumerator . MoveNextAsync ( CancellationToken . None )) { result . Add ( enumerator . Current . Value ); } } } return result ; } } } ProductCatalog C# using System ; using System.Collections.Generic ; using System.Fabric ; using System.Linq ; using System.Threading ; using System.Threading.Tasks ; using ECommerce.ProductCatalog.Model ; using Microsoft.ServiceFabric.Data.Collections ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { /// <summary> /// An instance of this class is created for each service replica by the Service Fabric runtime. /// </summary> internal sealed class ProductCatalog : StatefulService { private ServcieFabricProductRepostiory _repo ; public ProductCatalog ( StatefulServiceContext context ) : base ( context ) { } /// <summary> /// Optional override to create listeners (e.g., HTTP, Service Remoting, WCF, etc.) for this service replica to handle client or user requests. /// </summary> /// <remarks> /// For more information on service communication, see https://aka.ms/servicefabricservicecommunication /// </remarks> /// <returns>A collection of listeners.</returns> protected override IEnumerable < ServiceReplicaListener > CreateServiceReplicaListeners () { return new ServiceReplicaListener [ 0 ]; } /// <summary> /// This is the main entry point for your service replica. /// This method executes when this replica of your service becomes primary and has write status. /// </summary> /// <param name=\"cancellationToken\">Canceled when Service Fabric needs to shut down this service replica.</param> protected override async Task RunAsync ( CancellationToken cancellationToken ) { var product1 = new Product { Id = Guid . NewGuid (), Name = \"Dell Monitor\" , Description = \" Computer Monitor\" , Price = 500 , Availability = 100 }; var product2 = new Product { Id = Guid . NewGuid (), Name = \"Keyboard\" , Description = \" Computer Accesories\" , Price = 510 , Availability = 110 }; var product3 = new Product { Id = Guid . NewGuid (), Name = \"Mouse\" , Description = \" Computer Accesories\" , Price = 520 , Availability = 120 }; _repo = new ServcieFabricProductRepostiory ( this . StateManager ); await _repo . AddProudct ( product1 ); await _repo . AddProudct ( product2 ); await _repo . AddProudct ( product3 ); var all = await _repo . GetProducts (); } } } At this point we should be able to run the application and check there are 3 products in all variable which is await of GetProducts() This is how the solution should look like: :::warning Visual Studio need to run in elevated mode as Admin to allow access network level resources :::","title":"Creating Product Catalog Service"},{"location":"home/starting-azure-service-fabric/#creating-a-web-api","text":"To add new Stateless ASP .net Core Web API, right click on ECommerce project and add New Service Fabric Service and choose, stateless asp.net core API. Program.cs is console application very similar to previous one which resisters the service type. API C# using System.Collections.Generic ; using System.Fabric ; using System.IO ; using Microsoft.AspNetCore.Hosting ; using Microsoft.Extensions.DependencyInjection ; using Microsoft.ServiceFabric.Services.Communication.AspNetCore ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.API { /// <summary> /// The FabricRuntime creates an instance of this class for each service type instance. /// </summary> internal sealed class API : StatelessService { public API ( StatelessServiceContext context ) : base ( context ) { } /// <summary> /// Optional override to create listeners (like tcp, http) for this service instance. /// </summary> /// <returns>The collection of listeners.</returns> protected override IEnumerable < ServiceInstanceListener > CreateServiceInstanceListeners () { return new ServiceInstanceListener [] { new ServiceInstanceListener ( serviceContext => new KestrelCommunicationListener ( serviceContext , \"ServiceEndpoint\" , ( url , listener ) => { ServiceEventSource . Current . ServiceMessage ( serviceContext , $\"Starting Kestrel on {url}\" ); return new WebHostBuilder () . UseKestrel () . ConfigureServices ( services => services . AddSingleton < StatelessServiceContext > ( serviceContext )) . UseContentRoot ( Directory . GetCurrentDirectory ()) . UseStartup < Startup > () . UseServiceFabricIntegration ( listener , ServiceFabricIntegrationOptions . None ) . UseUrls ( url ) . Build (); })) }; } } } KestrelCommunicationListener is standard fabric listener which bootstrap asp.net core runtime and configure it to run inside service fabric environment. Let's add Product Api Model C# using System ; using Newtonsoft.Json ; namespace ECommerce.API.Model { public class ApiProduct { [JsonProperty(\"id\")] public Guid Id { get ; set ; } [JsonProperty(\"name\")] public string Name { get ; set ; } [JsonProperty(\"description\")] public string Description { get ; set ; } [JsonProperty(\"price\")] public double Price { get ; set ; } [JsonProperty(\"isAvailable\")] public int IsAvailable { get ; set ; } } } Here is new project structure:","title":"Creating a Web API"},{"location":"home/starting-azure-service-fabric/#communicating-between-two-services","text":"Let's say there are 3 nodes in a cluster graph LR subgraph node3 a(Service API - active) & b(Catalog Service - passive) end subgraph node2 c(Service API - active) & d(Catalog Service - active) end subgraph node1 e(Service API - active) & f(Catalog Service - passive) end Since API are stateless they are active in all nodes but Catalog Service is stateful so Service Fabric makes one active and other two passive in case of fail over. Tip Text Only Never assume a service is running in a fixed location. Services in Service Fabric can move around and change roles all the time during the lifetime of the application and even between requests. Therefore, before making the request we should always query the service location from Service Fabric runtime. Service Fabric is protocol agnostic. Out of the box there are three protocols: * WCF * HTTP * Service Remoting (Default protocol for reliable communication) Benefits of using Service Remoting: * Automatic service address resolution * Establishing connection * Retries * Error handling * Strong typed * Fast For the purpose of remoting we are adding Service interface in the Model IProductCatalogService C# using System.Threading.Tasks ; using Microsoft.ServiceFabric.Services.Remoting ; namespace ECommerce.ProductCatalog.Model { public interface IProductCatalogService : IService { Task < Product [] > GetALlProductsAsync (); Task AddProductAsync ( Product product ); } } We need to install the remoting service library. And implement IProductCatalogService in ProductCatalog C# using System ; using System.Collections.Generic ; using System.Fabric ; using System.Linq ; using System.Threading ; using System.Threading.Tasks ; using ECommerce.ProductCatalog.Model ; using Microsoft.ServiceFabric.Services.Communication.Runtime ; using Microsoft.ServiceFabric.Services.Remoting.V2.FabricTransport.Runtime ; using Microsoft.ServiceFabric.Services.Runtime ; namespace ECommerce.ProductCatalog { /// <summary> /// An instance of this class is created for each service replica by the Service Fabric runtime. /// </summary> internal sealed class ProductCatalog : StatefulService , IProductCatalogService { private ServiceFabricProductRepository _repo ; public ProductCatalog ( StatefulServiceContext context ) : base ( context ) { } public async Task AddProductAsync ( Product product ) { await _repo . AddProduct ( product ); } public async Task < Product [] > GetALlProductsAsync () { return ( await _repo . GetProducts ()). ToArray (); } /// <summary> /// Optional override to create listeners (e.g., HTTP, Service Remoting, WCF, etc.) for this service replica to handle client or user requests. /// </summary> /// <remarks> /// For more information on service communication, see https://aka.ms/servicefabricservicecommunication /// </remarks> /// <returns>A collection of listeners.</returns> protected override IEnumerable < ServiceReplicaListener > CreateServiceReplicaListeners () { return new [] { new ServiceReplicaListener ( context => new FabricTransportServiceRemotingListener ( context , this )) }; } /// <summary> /// This is the main entry point for your service replica. /// This method executes when this replica of your service becomes primary and has write status. /// </summary> /// <param name=\"cancellationToken\">Canceled when Service Fabric needs to shut down this service replica.</param> protected override async Task RunAsync ( CancellationToken cancellationToken ) { var product1 = new Product { Id = Guid . NewGuid (), Name = \"Dell Monitor\" , Description = \" Computer Monitor\" , Price = 500 , Availability = 100 }; var product2 = new Product { Id = Guid . NewGuid (), Name = \"Keyboard\" , Description = \" Computer Accessories\" , Price = 510 , Availability = 110 }; var product3 = new Product { Id = Guid . NewGuid (), Name = \"Mouse\" , Description = \" Computer Accessories\" , Price = 520 , Availability = 120 }; _repo = new ServiceFabricProductRepository ( this . StateManager ); await _repo . AddProduct ( product1 ); await _repo . AddProduct ( product2 ); await _repo . AddProduct ( product3 ); var all = await _repo . GetProducts (); } } } Tip Text Only The return type must be array as the service remoting doesn't understand IEnumerable and it needs to transform over the networks it should be simple type. Finally it's time to add proxy in API to connect to ProductCatalogService. ProductController C# using System ; using System.Collections.Generic ; using System.Linq ; using System.Threading.Tasks ; using ECommerce.API.Model ; using ECommerce.ProductCatalog.Model ; using Microsoft.AspNetCore.Mvc ; using Microsoft.ServiceFabric.Services.Client ; using Microsoft.ServiceFabric.Services.Remoting.Client ; using Microsoft.ServiceFabric.Services.Remoting.V2.FabricTransport.Client ; namespace ECommerce.API.Controllers { [ApiController] [Route(\"[controller] \")] public class ProductsController : ControllerBase { private readonly IProductCatalogService _service ; public ProductsController () { var proxyFactory = new ServiceProxyFactory ( c => new FabricTransportServiceRemotingClientFactory ()); _service = proxyFactory . CreateServiceProxy < IProductCatalogService > ( new Uri ( \"fabric:/ECommerce/ECommerce.ProductCatalog\" ), new ServicePartitionKey ( 0 )); } [HttpGet] public async Task < IEnumerable < ApiProduct >> GetAsync () { var allProducts = await _service . GetAllProductsAsync (); return allProducts . Select ( p => new ApiProduct { Id = p . Id , Name = p . Name , Description = p . Description , Price = p . Price , IsAvailable = p . Availability > 0 }); } [HttpPost] public async Task PostAsync ([ FromBody ] ApiProduct product ) { var newProduct = new Product () { Id = Guid . NewGuid (), Name = product . Name , Description = product . Description , Price = product . Price , Availability = 100 }; await _service . AddProductAsync ( newProduct ); } } } Launch the application and test it with Postman for both get and post. Tip Text Only URI Format for proxy: fabric/ApplicationName/ServiceName Eg: fabric:/ECommerce/ProductCatalog","title":"Communicating between two services"},{"location":"home/starting-azure-service-fabric/#end-to-end-flow","text":"Here is end to end flow: API: Stateless service ProductsController (Product_API) ProductCatalog: StatefulService, IProductCatalogService sequenceDiagram participant ProductsController participant ServiceProxyFactory participant ProductCatalog participant SFProductRepository participant IReliableStateManager ProductsController->>ServiceProxyFactory: CreateServiceProxy ServiceProxyFactory-->>ProductsController: _service ProductsController->>ProductCatalog :GetAllProductsAsync() loop new ServiceReplicaListener of ctx ProductCatalog->>ProductCatalog: new FabricTransportServiceRemotingListener end ProductCatalog->>SFProductRepository: GetProducts() SFProductRepository->>IReliableStateManager: products.CreateEnumerableAsync SFProductRepository-->>ProductCatalog: result ProductCatalog-->>ProductsController: result.ToArray()","title":"End to End flow"},{"location":"home/starting-azure-service-fabric/#exploring-actor-model-support","text":"","title":"Exploring Actor Model Support"},{"location":"home/starting-azure-service-fabric/#managing-state","text":"","title":"Managing State"},{"location":"home/starting-azure-service-fabric/#getting-ready-for-deployment","text":"","title":"Getting Ready for Deployment"},{"location":"home/starting-azure-service-fabric/#test","text":"","title":"Test"},{"location":"home/tools-for-daily-use/","text":"List of tools for daily use \u00b6 dexpot autohotkey microsoft-windows-terminal Bash choco install -y 7zip git sourcetree nvm vscode brave foxitreader dexpot autohotkey microsoft-windows-terminal Keys \u00b6 Win + S for Run shell:startup to go to startup folder","title":"List of tools for daily use"},{"location":"home/tools-for-daily-use/#list-of-tools-for-daily-use","text":"dexpot autohotkey microsoft-windows-terminal Bash choco install -y 7zip git sourcetree nvm vscode brave foxitreader dexpot autohotkey microsoft-windows-terminal","title":"List of tools for daily use"},{"location":"home/tools-for-daily-use/#keys","text":"Win + S for Run shell:startup to go to startup folder","title":"Keys"},{"location":"microservices/azure-function-dapr/","text":"Azure function with Dapr to create event-driven, distributed application \u00b6 In this post we will integrate Azure function app to use Dapr system for pubsub messaging. Azure function could be either triggered by Event Hub or HttpTrigger. For simplicity we will test it with HttpTrigger. Project Repo System Diagram \u00b6 Azure function provides easier way to integrate with Event Hub Dapr Subsystem hides complexity to deal with PubSub messaging Getting Ready \u00b6 Getting started with dapr See the dapr Azure Functions quickstart Requirements Dapr installed and configured locally (dapr init) Azure Functions Core Tools - v3 Docker .NET Core SDK - enables building of the extension locally in the project Node 12 for local debugging of the JavaScript app Or .Net Azure Function Note: Run Docker on WSL 2 on Windows for better experience Install Dapr \u00b6 Run in WSL window PowerShell wget -q https :// raw . githubusercontent . com / dapr / cli / master / install / install . sh -O - | / bin / bash Run to verify PowerShell dapr Run init PowerShell dapr init Verify Dapr version PowerShell dapr - -version Verify containers are running PowerShell docker ps Install dotnet sdk 3.1 and azure-functions-core-tools \u00b6 Install dotnet sdk with azure function tools Create Azure function App \u00b6 Create a folder for a project or clone empty project from the git \u00b6 Navigate to the folder from WSL console \u00b6 PowerShell dotnet new sln func init process - -dotnet - -docker dotnet sln azure-function-dapr . sln add ./ process / process . csproj cd process / func new - -name GetMessage - -template \"HTTP Trigger\" - -authlevel \"anonymous\" ps @Dev -WD MINGW64 / c / Projects / azure-function-dapr / process ( main ) $ func extensions install -p Dapr . AzureFunctions . Extension -v 0 . 12 . 0-p review01 dotnet build process . csproj -o bin - -no-incremental To run/debug the function app locally click on \"Initialize Project for Use with VS Code\" Update the connection for the WebStorage and run/debug in VS Code Add Nuget packages \u00b6 Dapr.AzureFunctions.Extension Microsoft.NET.Sdk.Functions Microsoft.Azure.WebJobs.Extensions.EventHubs Create Function GetMessage and add Dapr pubsub \u00b6 ```csharp{19,34} using System; using System.IO; using System.Threading.Tasks; using Microsoft.AspNetCore.Mvc; using Microsoft.Azure.WebJobs; using Microsoft.Azure.WebJobs.Extensions.Http; using Dapr.AzureFunctions.Extension; using Microsoft.AspNetCore.Http; using Microsoft.Extensions.Logging; using Newtonsoft.Json; namespace process { public static class GetMessage { [FunctionName(\"GetMessage\")] public static async Task Run( [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)] HttpRequest req, [DaprPublish(PubSubName = \"pubsub\", Topic = \"alerts\")] IAsyncCollector alertEvent, ILogger log) { log.LogInformation(\"C# HTTP trigger function processed a request.\"); Text Only string name = req.Query[\"name\"]; string requestBody = await new StreamReader(req.Body).ReadToEndAsync(); dynamic data = JsonConvert.DeserializeObject(requestBody); name = name ?? data?.name; string responseMessage = string.IsNullOrEmpty(name) ? \"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\" : $\"Hello, {name}. This HTTP triggered function executed successfully.\"; await alertEvent.AddAsync(new DaprPubSubEvent(JsonConvert.SerializeObject(new { id = \"testobj\", message = \"This is a test alert\" }))); return new OkObjectResult(responseMessage); } } } ``` Dapr config file config.yaml \u00b6 yaml apiVersion: dapr.io/v1alpha1 kind: Configuration metadata: name: daprConfig spec: tracing: samplingRate: '1' zipkin: endpointAddress: 'http://localhost:9411/api/v2/spans' Add messagebus component /components/messagebus.yaml \u00b6 YAML apiVersion : dapr.io/v1alpha1 kind : Component metadata : name : pubsub spec : type : pubsub.redis metadata : - name : redisHost value : localhost:6379 - name : redisPassword secretKeyRef : key : redis-password name : redis Run Azure Function locally \u00b6 PowerShell dapr run -a functionapp -p 3001 - -components-path ./ components / - -config ./ config . yaml -- func host start Trigger Azure function from WSL console \u00b6 PowerShell curl -X POST -H \"Content-Type: application/json\" -d '{ \"name\": \"John Gray\"}' http :// 172 . 21 . 132 . 136 : 7071 / api / GetMessage Validate if the message is in Pubsub \u00b6 PowerShell docker exec -it dapr_redis / bin / sh keys * It should return something like PowerShell 1 ) \"alerts\" Check actual messages PowerShell xread STREAMS alert 0","title":"Azure function with Dapr to create event-driven, distributed application"},{"location":"microservices/azure-function-dapr/#azure-function-with-dapr-to-create-event-driven-distributed-application","text":"In this post we will integrate Azure function app to use Dapr system for pubsub messaging. Azure function could be either triggered by Event Hub or HttpTrigger. For simplicity we will test it with HttpTrigger. Project Repo","title":"Azure function with Dapr to create event-driven, distributed application"},{"location":"microservices/azure-function-dapr/#system-diagram","text":"Azure function provides easier way to integrate with Event Hub Dapr Subsystem hides complexity to deal with PubSub messaging","title":"System Diagram"},{"location":"microservices/azure-function-dapr/#getting-ready","text":"Getting started with dapr See the dapr Azure Functions quickstart Requirements Dapr installed and configured locally (dapr init) Azure Functions Core Tools - v3 Docker .NET Core SDK - enables building of the extension locally in the project Node 12 for local debugging of the JavaScript app Or .Net Azure Function Note: Run Docker on WSL 2 on Windows for better experience","title":"Getting Ready"},{"location":"microservices/azure-function-dapr/#install-dapr","text":"Run in WSL window PowerShell wget -q https :// raw . githubusercontent . com / dapr / cli / master / install / install . sh -O - | / bin / bash Run to verify PowerShell dapr Run init PowerShell dapr init Verify Dapr version PowerShell dapr - -version Verify containers are running PowerShell docker ps","title":"Install Dapr"},{"location":"microservices/azure-function-dapr/#install-dotnet-sdk-31-and-azure-functions-core-tools","text":"Install dotnet sdk with azure function tools","title":"Install dotnet sdk 3.1 and azure-functions-core-tools"},{"location":"microservices/azure-function-dapr/#create-azure-function-app","text":"","title":"Create Azure function App"},{"location":"microservices/azure-function-dapr/#create-a-folder-for-a-project-or-clone-empty-project-from-the-git","text":"","title":"Create a folder for a project or clone empty project from the git"},{"location":"microservices/azure-function-dapr/#navigate-to-the-folder-from-wsl-console","text":"PowerShell dotnet new sln func init process - -dotnet - -docker dotnet sln azure-function-dapr . sln add ./ process / process . csproj cd process / func new - -name GetMessage - -template \"HTTP Trigger\" - -authlevel \"anonymous\" ps @Dev -WD MINGW64 / c / Projects / azure-function-dapr / process ( main ) $ func extensions install -p Dapr . AzureFunctions . Extension -v 0 . 12 . 0-p review01 dotnet build process . csproj -o bin - -no-incremental To run/debug the function app locally click on \"Initialize Project for Use with VS Code\" Update the connection for the WebStorage and run/debug in VS Code","title":"Navigate to the folder from WSL console"},{"location":"microservices/azure-function-dapr/#add-nuget-packages","text":"Dapr.AzureFunctions.Extension Microsoft.NET.Sdk.Functions Microsoft.Azure.WebJobs.Extensions.EventHubs","title":"Add Nuget packages"},{"location":"microservices/azure-function-dapr/#create-function-getmessage-and-add-dapr-pubsub","text":"```csharp{19,34} using System; using System.IO; using System.Threading.Tasks; using Microsoft.AspNetCore.Mvc; using Microsoft.Azure.WebJobs; using Microsoft.Azure.WebJobs.Extensions.Http; using Dapr.AzureFunctions.Extension; using Microsoft.AspNetCore.Http; using Microsoft.Extensions.Logging; using Newtonsoft.Json; namespace process { public static class GetMessage { [FunctionName(\"GetMessage\")] public static async Task Run( [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)] HttpRequest req, [DaprPublish(PubSubName = \"pubsub\", Topic = \"alerts\")] IAsyncCollector alertEvent, ILogger log) { log.LogInformation(\"C# HTTP trigger function processed a request.\"); Text Only string name = req.Query[\"name\"]; string requestBody = await new StreamReader(req.Body).ReadToEndAsync(); dynamic data = JsonConvert.DeserializeObject(requestBody); name = name ?? data?.name; string responseMessage = string.IsNullOrEmpty(name) ? \"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\" : $\"Hello, {name}. This HTTP triggered function executed successfully.\"; await alertEvent.AddAsync(new DaprPubSubEvent(JsonConvert.SerializeObject(new { id = \"testobj\", message = \"This is a test alert\" }))); return new OkObjectResult(responseMessage); } } } ```","title":"Create Function GetMessage and add Dapr pubsub"},{"location":"microservices/azure-function-dapr/#dapr-config-file-configyaml","text":"yaml apiVersion: dapr.io/v1alpha1 kind: Configuration metadata: name: daprConfig spec: tracing: samplingRate: '1' zipkin: endpointAddress: 'http://localhost:9411/api/v2/spans'","title":"Dapr config file config.yaml"},{"location":"microservices/azure-function-dapr/#add-messagebus-component-componentsmessagebusyaml","text":"YAML apiVersion : dapr.io/v1alpha1 kind : Component metadata : name : pubsub spec : type : pubsub.redis metadata : - name : redisHost value : localhost:6379 - name : redisPassword secretKeyRef : key : redis-password name : redis","title":"Add messagebus component /components/messagebus.yaml"},{"location":"microservices/azure-function-dapr/#run-azure-function-locally","text":"PowerShell dapr run -a functionapp -p 3001 - -components-path ./ components / - -config ./ config . yaml -- func host start","title":"Run Azure Function locally"},{"location":"microservices/azure-function-dapr/#trigger-azure-function-from-wsl-console","text":"PowerShell curl -X POST -H \"Content-Type: application/json\" -d '{ \"name\": \"John Gray\"}' http :// 172 . 21 . 132 . 136 : 7071 / api / GetMessage","title":"Trigger Azure function from WSL console"},{"location":"microservices/azure-function-dapr/#validate-if-the-message-is-in-pubsub","text":"PowerShell docker exec -it dapr_redis / bin / sh keys * It should return something like PowerShell 1 ) \"alerts\" Check actual messages PowerShell xread STREAMS alert 0","title":"Validate if the message is in Pubsub"},{"location":"microservices/dapr-resources/","text":"Resources \u00b6 Azure function with Dapr - Youtube Azure function extension for Dapr Dapr system series - youtube","title":"Resources"},{"location":"microservices/dapr-resources/#resources","text":"Azure function with Dapr - Youtube Azure function extension for Dapr Dapr system series - youtube","title":"Resources"},{"location":"microservices/grafana-dashboards/","text":"Grafana Dashboards \u00b6 Stock Analyzer \u00b6 ARK Holding Trends \u00b6 Azure Application Monitoring \u00b6","title":"Grafana Dashboards"},{"location":"microservices/grafana-dashboards/#grafana-dashboards","text":"","title":"Grafana Dashboards"},{"location":"microservices/grafana-dashboards/#stock-analyzer","text":"","title":"Stock Analyzer"},{"location":"microservices/grafana-dashboards/#ark-holding-trends","text":"","title":"ARK Holding Trends"},{"location":"microservices/grafana-dashboards/#azure-application-monitoring","text":"","title":"Azure Application Monitoring"},{"location":"microservices/links/","text":"\ud83d\udd17 Dapr links \u00b6 Azure Sample Dapr","title":"\ud83d\udd17 Dapr links"},{"location":"microservices/links/#dapr-links","text":"Azure Sample Dapr","title":"\ud83d\udd17 Dapr links"},{"location":"python/create-python-package/","text":"\ud83d\udce6 Create Custom Python Package \u00b6 Packing allows us to modularize the code and reused them access the projects. Additionally, it allows us to test them independently. Refer to Getting Started With Testing in Python and Effective Python Testing With Pytest . The general package structure would look like: Text Only project/ \u2502 \u251c\u2500\u2500 src/ \u2502 \u2514\u2500\u2500 auth/ \u2502 | \u251c\u2500\u2500 __init__.py \u2502 | \u251c\u2500\u2500 __main__.py \u2502 | \u251c\u2500\u2500 config.toml \u2502 | \u251c\u2500\u2500 feed.py \u2502 | \u2514\u2500\u2500 viewer.py | |__ utils/ | |__ __init__.py \u2502 \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 test_auth.py \u2502 \u2514\u2500\u2500 test_utils.py \u2502 \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 README.md \u2514\u2500\u2500 pyproject.toml Let's create a package just containing two files inside the app/src folder. Requirements \u00b6 Python installed Init src folder as module with init .py folder in it \u00b6 Create setup.py file \u00b6 setup.py from setuptools import setup , find_packages setup ( #this will be the package name you will see, e.g. the output of 'conda list' in anaconda prompt name = 'custom_package' , #some version number you may wish to add - increment this after every update version = '1.0' , # Use one of the below approach to define package and/or module names: #if there are only handful of modules placed in root directory, and no packages/directories exist then can use below syntax # packages=[''], #have to import modules directly in code after installing this wheel, like import mod2 (respective file name in this case is mod2.py) - no direct use of distribution name while importing #can list down each package names - no need to keep __init__.py under packages / directories # packages=['<list of name of packages>'], #importing is like: from package1 import mod2, or import package1.mod2 as m2 #this approach automatically finds out all directories (packages) - those must contain a file named __init__.py (can be empty) packages = find_packages (), #include/exclude arguments take * as wildcard, . for any sub-package names install_requires = [ \"schedule\" ] ) Install build tools \u00b6 PowerShell python / py -m pip install build twine Build the package \u00b6 Text Only py -m build It would generate .whl and tar.gz file Successfully built custom_package-1.0.tar.gz and custom_package-1.0-py3-none-any.whl Use the package in Synapse workspace \u00b6 The package can be uploaded in Azure synapse workspace. Once uploaded that package can be installed in the selected pool. Therefore, package has to be exclusively added in pool packages. Reference: Create and publish python package References \u00b6 Parsing python config","title":"\ud83d\udce6 Create Custom Python Package"},{"location":"python/create-python-package/#create-custom-python-package","text":"Packing allows us to modularize the code and reused them access the projects. Additionally, it allows us to test them independently. Refer to Getting Started With Testing in Python and Effective Python Testing With Pytest . The general package structure would look like: Text Only project/ \u2502 \u251c\u2500\u2500 src/ \u2502 \u2514\u2500\u2500 auth/ \u2502 | \u251c\u2500\u2500 __init__.py \u2502 | \u251c\u2500\u2500 __main__.py \u2502 | \u251c\u2500\u2500 config.toml \u2502 | \u251c\u2500\u2500 feed.py \u2502 | \u2514\u2500\u2500 viewer.py | |__ utils/ | |__ __init__.py \u2502 \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 test_auth.py \u2502 \u2514\u2500\u2500 test_utils.py \u2502 \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 MANIFEST.in \u251c\u2500\u2500 README.md \u2514\u2500\u2500 pyproject.toml Let's create a package just containing two files inside the app/src folder.","title":"\ud83d\udce6 Create Custom Python Package"},{"location":"python/create-python-package/#requirements","text":"Python installed","title":"Requirements"},{"location":"python/create-python-package/#init-src-folder-as-module-with-initpy-folder-in-it","text":"","title":"Init src folder as module with init.py folder in it"},{"location":"python/create-python-package/#create-setuppy-file","text":"setup.py from setuptools import setup , find_packages setup ( #this will be the package name you will see, e.g. the output of 'conda list' in anaconda prompt name = 'custom_package' , #some version number you may wish to add - increment this after every update version = '1.0' , # Use one of the below approach to define package and/or module names: #if there are only handful of modules placed in root directory, and no packages/directories exist then can use below syntax # packages=[''], #have to import modules directly in code after installing this wheel, like import mod2 (respective file name in this case is mod2.py) - no direct use of distribution name while importing #can list down each package names - no need to keep __init__.py under packages / directories # packages=['<list of name of packages>'], #importing is like: from package1 import mod2, or import package1.mod2 as m2 #this approach automatically finds out all directories (packages) - those must contain a file named __init__.py (can be empty) packages = find_packages (), #include/exclude arguments take * as wildcard, . for any sub-package names install_requires = [ \"schedule\" ] )","title":"Create setup.py file"},{"location":"python/create-python-package/#install-build-tools","text":"PowerShell python / py -m pip install build twine","title":"Install build tools"},{"location":"python/create-python-package/#build-the-package","text":"Text Only py -m build It would generate .whl and tar.gz file Successfully built custom_package-1.0.tar.gz and custom_package-1.0-py3-none-any.whl","title":"Build the package"},{"location":"python/create-python-package/#use-the-package-in-synapse-workspace","text":"The package can be uploaded in Azure synapse workspace. Once uploaded that package can be installed in the selected pool. Therefore, package has to be exclusively added in pool packages. Reference: Create and publish python package","title":"Use the package in Synapse workspace"},{"location":"python/create-python-package/#references","text":"Parsing python config","title":"References"},{"location":"python/links/","text":"\ud83d\udd17 Python links \u00b6 python asyncio test Code for Python for Data Analysis Code for Python for Data Analysis - jupyter Greyhound Analytics Vectorbt \u00b6 Combination parameters Algorithmic trading platform - Make me rich! Bot Orchestrator Freqtrade for Vectorbt volatility estimators for Vectorbt Riskfolio integration Vectorbt Python Finance \u00b6 Python, finance and getting them to play Optopsy - Backtesting and statistics library for option strategies Ichimoku \u00b6 ichimoku-crypto Python/Pandas calculate Ichimoku Ichimoku Clouds ichimoku trading strategy with python ichimoku trading strategy with python \u2013 part 2","title":"\ud83d\udd17 Python links"},{"location":"python/links/#python-links","text":"python asyncio test Code for Python for Data Analysis Code for Python for Data Analysis - jupyter Greyhound Analytics","title":"\ud83d\udd17 Python links"},{"location":"python/links/#vectorbt","text":"Combination parameters Algorithmic trading platform - Make me rich! Bot Orchestrator Freqtrade for Vectorbt volatility estimators for Vectorbt Riskfolio integration Vectorbt","title":"Vectorbt"},{"location":"python/links/#python-finance","text":"Python, finance and getting them to play Optopsy - Backtesting and statistics library for option strategies","title":"Python Finance"},{"location":"python/links/#ichimoku","text":"ichimoku-crypto Python/Pandas calculate Ichimoku Ichimoku Clouds ichimoku trading strategy with python ichimoku trading strategy with python \u2013 part 2","title":"Ichimoku"},{"location":"python/python-basics/","text":"\ud83d\udc0d Python Basics \u00b6 Args and Kwargs \u00b6 Python def sum_of ( * args ): sum = 0 for x in args : sum += x return sum print ( sum_of ( 4 , 5 , 6 )) Python def sum_of ( ** kwargs ): sum = 0 for key , value in kwargs . items (): sum += value return round ( sum ) print ( sum_of ( coffee = 2.99 , cake = 4.55 , juice = 2.99 )) Scope: LEGB - Local, Enclosing, Global, Built-in \u00b6 Tuples are immutalbe while lists are mutable Instantiate a custom Object Python class Recipe (): def __init__ ( self , dish , items , time ) -> None : self . dish = dish self . items = items self . time = time def contents ( self ): print ( f \" { self . dish } { str ( self . items ) } { str ( self . time ) } \" ) pizza = Recipe ( \"Pizza\" , [ \"cheese\" , \"bread\" , \"tomato\" ], 45 ) print ( pizza . contents ()) Inheritance \u00b6 Python class Employees : def __init__ ( self , name , last ) -> None : self . name = name self . last = last class Supervisors ( Employees ): def __init__ ( self , name , last , password ) -> None : super () . __init__ ( name , last ) self . password = password class Chefs ( Employees ): def leave_request ( self , days ): return f \"Return in { days } days\" adrian = Supervisors ( \"Adrian\" , \"A\" , \"apple\" ) emily = Chefs ( \"Emily\" , \"E\" ) juno = Chefs ( \"Juno\" , \"J\" ) print ( adrian . password ) print ( emily . leave_request ( 20 )) print ( emily . name ) print ( juno . name ) Abstract calsses and methods \u00b6 Interoperability Consistency Code duplication Python from abc import ABC , abstractmethod class Employee ( ABC ): @abstractmethod def donate ( self ): pass class Donation ( Employee ): def donate ( self ): a = input ( \"How much would you like to donate: \" ) return a amounts = [] john = Donation () j = john . donate () amounts . append ( j ) peter = Donation () p = peter . donate () amounts . append ( p ) print ( amounts ) Method Resolution Order (MRO) \u00b6 MRO determines the order in which a given method or attribute passed is searched or it's resloution. Method resultion default order is bottom -> top and left -> right Simple Inheritance Multiple Inheritance Multi Level Inheritance Hierarchical Inheritance Hybrid Inheritance Beyond Python3 C3 Linearization algorithm - Adheres to Monotonicity - Follows Inheritance graph - Visits super class after local classes Python class A : num = 5 class B ( A ): num = 9 class C ( B ): pass print ( C . mro ()) print ( help ( C )) Modules \u00b6 Module generally imported in global scope but can be imported only for funciton scope. Scope : Modules create a seperate namespace Reusability : Avoid duplication Simplicity : Helps avoid interdependency among modules Types of modules: - Built-in (Eg, import math) Any file can be module and here is the lookup by interpreter 1. Current Directory path 2. Build-in module directory 3. PYTHONPATH 4. Installation dependent default directory Python import sys locations = sys . path for path in locations : print ( path ) Import module \u00b6 Python import sys sys . path . insert ( 1 , r '/app/yahoo' ) Try to import just function needed rather than entire module. Modules and functions can be imported with alias. Python from math import sqrt root = sqrt ( 9 ) All function from the module can be explicitly imported with * (Eg. from math import *). But it's a bad practice to load everything. Namespacing and scoping \u00b6 LEGB rule: Local -> Enclosed, Global, Built-in It is discouraged to used global variables Global: Access global variable from function Non-Local : Special scope nested funtions defined previously Python def d (): animal = \"elephant\" def e (): nonlocal animal animal = \"giraffe\" print ( f \"insided nested functions: { animal } \" ) print ( f \"before calling functions: { animal } \" ) e () print ( f \"after calling functions: { animal } \" ) animal = \"camel\" d () print ( f \"global animal: { animal } \" ) Reload function \u00b6 reload() - Reloads an imported module in Python. It can be used to detect dynamic change in the module without reruning the application Python import importlib importlib . reload ( \"module_name\" ) Note Text Only ```markdown - Packages: NumPy, pandas, Matplotlib, etc - Modules: os, sys, csv, json, importlib, re, math, intertools ```","title":"\ud83d\udc0d Python Basics"},{"location":"python/python-basics/#python-basics","text":"","title":"\ud83d\udc0d Python Basics"},{"location":"python/python-basics/#args-and-kwargs","text":"Python def sum_of ( * args ): sum = 0 for x in args : sum += x return sum print ( sum_of ( 4 , 5 , 6 )) Python def sum_of ( ** kwargs ): sum = 0 for key , value in kwargs . items (): sum += value return round ( sum ) print ( sum_of ( coffee = 2.99 , cake = 4.55 , juice = 2.99 ))","title":"Args and Kwargs"},{"location":"python/python-basics/#scope-legb-local-enclosing-global-built-in","text":"Tuples are immutalbe while lists are mutable Instantiate a custom Object Python class Recipe (): def __init__ ( self , dish , items , time ) -> None : self . dish = dish self . items = items self . time = time def contents ( self ): print ( f \" { self . dish } { str ( self . items ) } { str ( self . time ) } \" ) pizza = Recipe ( \"Pizza\" , [ \"cheese\" , \"bread\" , \"tomato\" ], 45 ) print ( pizza . contents ())","title":"Scope: LEGB - Local, Enclosing, Global, Built-in"},{"location":"python/python-basics/#inheritance","text":"Python class Employees : def __init__ ( self , name , last ) -> None : self . name = name self . last = last class Supervisors ( Employees ): def __init__ ( self , name , last , password ) -> None : super () . __init__ ( name , last ) self . password = password class Chefs ( Employees ): def leave_request ( self , days ): return f \"Return in { days } days\" adrian = Supervisors ( \"Adrian\" , \"A\" , \"apple\" ) emily = Chefs ( \"Emily\" , \"E\" ) juno = Chefs ( \"Juno\" , \"J\" ) print ( adrian . password ) print ( emily . leave_request ( 20 )) print ( emily . name ) print ( juno . name )","title":"Inheritance"},{"location":"python/python-basics/#abstract-calsses-and-methods","text":"Interoperability Consistency Code duplication Python from abc import ABC , abstractmethod class Employee ( ABC ): @abstractmethod def donate ( self ): pass class Donation ( Employee ): def donate ( self ): a = input ( \"How much would you like to donate: \" ) return a amounts = [] john = Donation () j = john . donate () amounts . append ( j ) peter = Donation () p = peter . donate () amounts . append ( p ) print ( amounts )","title":"Abstract calsses and methods"},{"location":"python/python-basics/#method-resolution-order-mro","text":"MRO determines the order in which a given method or attribute passed is searched or it's resloution. Method resultion default order is bottom -> top and left -> right Simple Inheritance Multiple Inheritance Multi Level Inheritance Hierarchical Inheritance Hybrid Inheritance Beyond Python3 C3 Linearization algorithm - Adheres to Monotonicity - Follows Inheritance graph - Visits super class after local classes Python class A : num = 5 class B ( A ): num = 9 class C ( B ): pass print ( C . mro ()) print ( help ( C ))","title":"Method Resolution Order (MRO)"},{"location":"python/python-basics/#modules","text":"Module generally imported in global scope but can be imported only for funciton scope. Scope : Modules create a seperate namespace Reusability : Avoid duplication Simplicity : Helps avoid interdependency among modules Types of modules: - Built-in (Eg, import math) Any file can be module and here is the lookup by interpreter 1. Current Directory path 2. Build-in module directory 3. PYTHONPATH 4. Installation dependent default directory Python import sys locations = sys . path for path in locations : print ( path )","title":"Modules"},{"location":"python/python-basics/#import-module","text":"Python import sys sys . path . insert ( 1 , r '/app/yahoo' ) Try to import just function needed rather than entire module. Modules and functions can be imported with alias. Python from math import sqrt root = sqrt ( 9 ) All function from the module can be explicitly imported with * (Eg. from math import *). But it's a bad practice to load everything.","title":"Import module"},{"location":"python/python-basics/#namespacing-and-scoping","text":"LEGB rule: Local -> Enclosed, Global, Built-in It is discouraged to used global variables Global: Access global variable from function Non-Local : Special scope nested funtions defined previously Python def d (): animal = \"elephant\" def e (): nonlocal animal animal = \"giraffe\" print ( f \"insided nested functions: { animal } \" ) print ( f \"before calling functions: { animal } \" ) e () print ( f \"after calling functions: { animal } \" ) animal = \"camel\" d () print ( f \"global animal: { animal } \" )","title":"Namespacing and scoping"},{"location":"python/python-basics/#reload-function","text":"reload() - Reloads an imported module in Python. It can be used to detect dynamic change in the module without reruning the application Python import importlib importlib . reload ( \"module_name\" ) Note Text Only ```markdown - Packages: NumPy, pandas, Matplotlib, etc - Modules: os, sys, csv, json, importlib, re, math, intertools ```","title":"Reload function"},{"location":"python/python-courses/","text":"\ud83d\udcd9 Courses and Tutorials \u00b6 Programming in Python - Meta \u00b6 Week 1: Getting started with Python Video: Introduction to the Course Video: How is Python used in the real world? Discussion Prompt: What do you hope to learn? Reading: Course syllabus Reading: How to be successful in this course Video: Introduction to Programming Video: Why Python? Reading: Visual Studio Code Reading: Installing Python paths (Optional for Windows Users) Reading: Installing Python paths (Optional for Mac users) Reading: Required dependencies Video: Environment check for Windows Video: Environment check for Mac Video: Running code - Command line VS IDE Video: Python syntax, spaces matter Reading: Python syntax cheat sheet Reading: Commenting code Video: Variables Video: Basic data types Video: Strings Reading: Basic Data type and Function Cheatsheet Video: Type casting Video: User input, console output Reading: Type casting, a deeper look Practice Quiz: Knowledge check - Welcome to Python Programming Reading: Additional resources Video: Math and logical operators Video: Control flow: If / else, else if Reading: Conditional statements Video: Switch statement Video: Looping constructs Reading: Looping Constructs: Practical Examples Reading: Practicing control flow and loops Video: Nested loops and the effect on algorithmic complexity Reading: Exercise: Use control flow and loops to solve a problem Reading: Use control flow and loops to solve a problem - solution Practice Quiz: Self-review: Use control flow and loops to solve a problem Video: Module summary: Getting started with Python Reading: Additional resources Week 2: Basic Programming with Python Video: Functions Video: Variable scope Reading: Function and variable scope Reading: What are data structures? Video: Lists Video: Tuples Video: Sets Video: Dictionaries Video: kwargs Practice Quiz: Functions, loops and data structures Reading: Choosing and using data structures Reading: Visual Studio Code on Coursera Practice Quiz: Knowledge check: Functions and Data structures Reading: Additional resources Video: What are exceptions Video: Exception handling Reading: Exercise: Exceptions in Python Reading: Exceptions in Python - solution Practice Quiz: Exceptions in Python Video: File handling in Python Video: Creating Files Video: Reading Files Reading: Storing file contents in data structures Practice Quiz: Read in data, store, manipulate and output new data to a file Video: Module summary: Basic Programming with Python Reading: Additional resources Week 3: Programming paradigms Video: What is procedural programming? Video: Algorithms Reading: Writing Algorithms Reading: Exercise: Make a cup of coffee Reading: Make a cup of coffee - solution Practice Quiz: Self-review: Make a cup of coffee Video: Algorithmic complexity Reading: Intro to Big-O notation Practice Quiz: Knowledge check: Procedural Programming Reading: Additional resources Video: What is functional programming? Video: Pure functions Video: Recursion Reading: Recursion example: Tower of Hanoi Video: Reversing a string on Python Video: Map & filter Reading: Comprehensions Practice Quiz: Mapping key values to dictionary data structures Practice Quiz: Knowledge check: Functional Programming Reading: Additional resources Video: Introduction to Object Oriented Programming Reading: OOP Principles Video: Python classes and instances Reading: Exercise: Define a Class Reading: Define a Class - solution Practice Quiz: Self-review: Define a Class Video: Instantiate a custom Object Reading: Exercise: Instantiate a custom Object Reading: Instantiate a custom Object - solution Practice Quiz: Self-review: Instantiate a custom Object Video: Instance methods Video: Parent classes vs. child classes Reading: Inheritance and Multiple Inheritance Reading: Exercise: Classes and object exploration Video: Abstract classes and methods Practice Quiz: Abstract classes and methods Video: Method Resolution Order Reading: Working with Methods: Examples Reading: Exercise: Working with Methods Reading: Working with Methods - solution Practice Quiz: Self-review: Working with Methods Video: Module summary: Programming paradigms Reading: Additional resources Week 4: Modules, packages, libraries and tools Video: What is a module in Python? Video: Accessing modules Video: The import statement Video: Writing import statements Video: Namespacing and scoping Video: reload() function Reading: Module Use-cases Practice Quiz: Knowledge check: Modules Reading: Additional resources Video: Popular packages: NumPy, pandas, Matplotlib, etc Reading: Popular Packages: Examples Video: Data analysis packages Video: Machine learning, deep learning and AI: PyTorch, TensorFlow Reading: Big Data and Analysis with Python Discussion Prompt: What do you consider to be the difference between machine learning and AI? Video: Python web frameworks Practice Quiz: Knowledge check: Popular Packages, Libraries and Frameworks Reading: Additional Resources Video: What is testing? Video: Types of testing Practice Quiz: Testing quiz Video: Test automation packages Video: Writing tests with PyTest Reading: PyTest cheat sheet Video: Test-driven development (TDD) Video: Applying TDD Video: Module summary: Modules, packages, libraries and tools Reading: Additional resources Week 5: End-of-Course Graded Assessment Video: Course Recap: Programming in Python Reading: About the End-of-Course Graded Assessment Discussion Prompt: Discuss challenges that you had with your fellow learners Video: Congratulations, you have completed Programming in Python Discussion Prompt: What did you find most challenging about learning to program in Python? Reading: Next steps after Programming in Python Crash Course on Python - Google \u00b6 Week 1: Hello Python! Video: Specialization Introduction Video: Course Introduction Ungraded Plugin: Take a Minute to Set Yourself up for Success Reading: Welcome to the Course! Reading: Program Surveys Ungraded Plugin: Automation with Python Participant Entry Survey Discussion Prompt: Meet & Greet Video: The Beginning of Your Programming Journey Video: What is programming? Video: What is automation? Video: Getting Computers to Work for You Discussion Prompt: Your Hopes for Automation Practice Quiz: Practice Quiz: Introduction to Programming Video: What is Python? Reading: Python Resources Reading: A Note on Syntax and Code Blocks Video: Why is Python relevant to IT? Video: Other Languages Practice Quiz: Practice Quiz: Introduction to Python Video: Hello, World! Video: Getting Information from the User Video: Python Can Be Your Calculator Reading: First Programming Concepts Cheat Sheet Practice Quiz: Practice Quiz: Hello World Video: First Steps Wrap Up Video: Meet Marga, the Curriculum Developer Discussion Prompt: Your First Programming Code Week 2: Basic Python Syntax Video: Basic Python Syntax introduction Video: Data Types Reading: Data Types Recap Video: Variables Video: Expressions, Numbers, and Type Conversions Reading: Implicit vs Explicit Conversion Practice Quiz: Practice Quiz: Expressions and Variables Video: Defining Functions Reading: Defining Functions Recap Video: Returning Values Reading: Returning Values Using Functions Video: The Principles of Code Reuse Video: Code Style Practice Quiz: Practice Quiz: Functions Video: Comparing Things Reading: Comparison Operators Video: Branching with if Statements Reading: if Statements Recap Video: else Statements Reading: else Statements and the Modulo Operator Video: elif Statements Reading: More Complex Branching with elif Statements Reading: Conditionals Cheat Sheet Practice Quiz: Practice Quiz: Conditionals Video: Basic Syntax Wrap Up Video: In Marga's Words: Why I Like Python Discussion Prompt: Basic Python Syntax Review Week 3: Loops Video: Introduction to Loops Video: What is a while loop? Reading: Anatomy of a While Loop Video: More while Loop Examples Video: Why Initializing Variables Matters Reading: Common Pitfalls With Variable Initialization Video: Infinite Loops and How to Break Them Reading: Infinite loops and Code Blocks Practice Quiz: Practice Quiz: While Loops Video: What is a for loop? Reading: For Loops Recap Video: More for Loop Examples Reading: A Closer Look at the Range() Function Video: Nested for Loops Video: Common Errors in for Loops Reading: Loops Cheat Sheet Practice Quiz: Practice Quiz: For Loops Video: What is recursion? (Optional) Video: Recursion in Action in the IT Context Reading: Additional Recursion Sources Practice Quiz: Practice Quiz: Recursion Video: Loops Wrap Up Video: In Marga's Words: How I Got Into Programming Discussion Prompt: Solving Problems with Loops Week 4: Strings, Lists and Dictionaries Video: Basic Structures Introduction Video: What is a string? Video: The Parts of a String Reading: String Indexing and Slicing Video: Creating New Strings Reading: Basic String Methods Video: More String Methods Reading: Advanced String Methods Video: Formatting Strings Reading: String Formatting Reading: String Reference Cheat Sheet Reading: Formatting Strings Cheat Sheet Practice Quiz: Practice Quiz: Strings Video: What is a list? Reading: Lists Defined Video: Modifying the Contents of a List Reading: Modifying Lists Video: Lists and Tuples Reading: Tuples Video: Iterating over Lists and Tuples Reading: Iterating Over Lists Using Enumerate Video: List Comprehensions Reading: List Comprehensions Reading: Lists and Tuples Operations Cheat Sheet Practice Quiz: Practice Quiz: Lists Video: What is a dictionary? Reading: Dictionaries Defined Video: Iterating over the Contents of a Dictionary Reading: Iterating Over Dictionaries Video: Dictionaries vs. Lists Reading: Dictionary Methods Cheat Sheet Practice Quiz: Practice Quiz: Dictionaries Video: Basic Structures Wrap Up Video: In Marga's Words: My Most Challenging Script Discussion Prompt: Discussion Prompt Week 5: Object Oriented Programming (Optional) Video: OOP Introduction (Optional) Video: What is Object-oriented programming? (Optional) Reading: Object-Oriented Programming Defined Video: Classes and Objects in Python (Optional) Reading: Classes and Objects in Detail Video: Defining New Classes (Optional) Reading: Defining Classes (Optional) Practice Quiz: Practice Quiz: Object-oriented Programming (Optional) Video: Instance Methods (Optional) Reading: What Is a Method? Video: Constructors and Other Special Methods (Optional) Reading: Special Methods Video: Documenting Functions, Classes, and Methods (Optional) Reading: Documenting with Docstrings Reading: Classes and Methods Cheat Sheet (Optional) Video: About Jupyter Notebooks (Optional) Reading: Help with Jupyter Notebooks (Optional) Ungraded Lab: Methods and Classes (Optional) Video: Inheritance (Optional) Reading: Object Inheritance Video: Composition (Optional) Reading: Object Composition Video: Python Modules (Optional) Reading: Augmenting Python with Modules Reading: Supplemental Reading for Code Reuse (Optional) Ungraded Lab: Code Reuse (Optional) Video: OOP Wrap Up (Optional) Video: In Marga's Words: My Favorite Course (Optional) Ungraded Lab: Practice Notebook: Object Oriented Programming (Optional) Discussion Prompt: Object Oriented Programming Assignment (Optional) Week 6: Final Project Video: Final Project Introduction Video: Problem Statement Video: Research Video: Planning Video: Writing the Script Video: Putting It All Together Video: Using Graded Jupyter Notebooks Ungraded Lab: Putting It All Together Video: In Marga's Words: Developing an Inclusive Curriculum Video: Final Project Overview Reading: Final Project Help Ungraded Lab: Final Project: WordCloud Discussion Prompt: Share your word cloud! Video: Congratulations! Discussion Prompt: Your Learner Journey Video: Sneak Peek of the Next Course","title":"\ud83d\udcd9 Courses and Tutorials"},{"location":"python/python-courses/#courses-and-tutorials","text":"","title":"\ud83d\udcd9 Courses and Tutorials"},{"location":"python/python-courses/#programming-in-python-meta","text":"Week 1: Getting started with Python Video: Introduction to the Course Video: How is Python used in the real world? Discussion Prompt: What do you hope to learn? Reading: Course syllabus Reading: How to be successful in this course Video: Introduction to Programming Video: Why Python? Reading: Visual Studio Code Reading: Installing Python paths (Optional for Windows Users) Reading: Installing Python paths (Optional for Mac users) Reading: Required dependencies Video: Environment check for Windows Video: Environment check for Mac Video: Running code - Command line VS IDE Video: Python syntax, spaces matter Reading: Python syntax cheat sheet Reading: Commenting code Video: Variables Video: Basic data types Video: Strings Reading: Basic Data type and Function Cheatsheet Video: Type casting Video: User input, console output Reading: Type casting, a deeper look Practice Quiz: Knowledge check - Welcome to Python Programming Reading: Additional resources Video: Math and logical operators Video: Control flow: If / else, else if Reading: Conditional statements Video: Switch statement Video: Looping constructs Reading: Looping Constructs: Practical Examples Reading: Practicing control flow and loops Video: Nested loops and the effect on algorithmic complexity Reading: Exercise: Use control flow and loops to solve a problem Reading: Use control flow and loops to solve a problem - solution Practice Quiz: Self-review: Use control flow and loops to solve a problem Video: Module summary: Getting started with Python Reading: Additional resources Week 2: Basic Programming with Python Video: Functions Video: Variable scope Reading: Function and variable scope Reading: What are data structures? Video: Lists Video: Tuples Video: Sets Video: Dictionaries Video: kwargs Practice Quiz: Functions, loops and data structures Reading: Choosing and using data structures Reading: Visual Studio Code on Coursera Practice Quiz: Knowledge check: Functions and Data structures Reading: Additional resources Video: What are exceptions Video: Exception handling Reading: Exercise: Exceptions in Python Reading: Exceptions in Python - solution Practice Quiz: Exceptions in Python Video: File handling in Python Video: Creating Files Video: Reading Files Reading: Storing file contents in data structures Practice Quiz: Read in data, store, manipulate and output new data to a file Video: Module summary: Basic Programming with Python Reading: Additional resources Week 3: Programming paradigms Video: What is procedural programming? Video: Algorithms Reading: Writing Algorithms Reading: Exercise: Make a cup of coffee Reading: Make a cup of coffee - solution Practice Quiz: Self-review: Make a cup of coffee Video: Algorithmic complexity Reading: Intro to Big-O notation Practice Quiz: Knowledge check: Procedural Programming Reading: Additional resources Video: What is functional programming? Video: Pure functions Video: Recursion Reading: Recursion example: Tower of Hanoi Video: Reversing a string on Python Video: Map & filter Reading: Comprehensions Practice Quiz: Mapping key values to dictionary data structures Practice Quiz: Knowledge check: Functional Programming Reading: Additional resources Video: Introduction to Object Oriented Programming Reading: OOP Principles Video: Python classes and instances Reading: Exercise: Define a Class Reading: Define a Class - solution Practice Quiz: Self-review: Define a Class Video: Instantiate a custom Object Reading: Exercise: Instantiate a custom Object Reading: Instantiate a custom Object - solution Practice Quiz: Self-review: Instantiate a custom Object Video: Instance methods Video: Parent classes vs. child classes Reading: Inheritance and Multiple Inheritance Reading: Exercise: Classes and object exploration Video: Abstract classes and methods Practice Quiz: Abstract classes and methods Video: Method Resolution Order Reading: Working with Methods: Examples Reading: Exercise: Working with Methods Reading: Working with Methods - solution Practice Quiz: Self-review: Working with Methods Video: Module summary: Programming paradigms Reading: Additional resources Week 4: Modules, packages, libraries and tools Video: What is a module in Python? Video: Accessing modules Video: The import statement Video: Writing import statements Video: Namespacing and scoping Video: reload() function Reading: Module Use-cases Practice Quiz: Knowledge check: Modules Reading: Additional resources Video: Popular packages: NumPy, pandas, Matplotlib, etc Reading: Popular Packages: Examples Video: Data analysis packages Video: Machine learning, deep learning and AI: PyTorch, TensorFlow Reading: Big Data and Analysis with Python Discussion Prompt: What do you consider to be the difference between machine learning and AI? Video: Python web frameworks Practice Quiz: Knowledge check: Popular Packages, Libraries and Frameworks Reading: Additional Resources Video: What is testing? Video: Types of testing Practice Quiz: Testing quiz Video: Test automation packages Video: Writing tests with PyTest Reading: PyTest cheat sheet Video: Test-driven development (TDD) Video: Applying TDD Video: Module summary: Modules, packages, libraries and tools Reading: Additional resources Week 5: End-of-Course Graded Assessment Video: Course Recap: Programming in Python Reading: About the End-of-Course Graded Assessment Discussion Prompt: Discuss challenges that you had with your fellow learners Video: Congratulations, you have completed Programming in Python Discussion Prompt: What did you find most challenging about learning to program in Python? Reading: Next steps after Programming in Python","title":"Programming in Python - Meta"},{"location":"python/python-courses/#crash-course-on-python-google","text":"Week 1: Hello Python! Video: Specialization Introduction Video: Course Introduction Ungraded Plugin: Take a Minute to Set Yourself up for Success Reading: Welcome to the Course! Reading: Program Surveys Ungraded Plugin: Automation with Python Participant Entry Survey Discussion Prompt: Meet & Greet Video: The Beginning of Your Programming Journey Video: What is programming? Video: What is automation? Video: Getting Computers to Work for You Discussion Prompt: Your Hopes for Automation Practice Quiz: Practice Quiz: Introduction to Programming Video: What is Python? Reading: Python Resources Reading: A Note on Syntax and Code Blocks Video: Why is Python relevant to IT? Video: Other Languages Practice Quiz: Practice Quiz: Introduction to Python Video: Hello, World! Video: Getting Information from the User Video: Python Can Be Your Calculator Reading: First Programming Concepts Cheat Sheet Practice Quiz: Practice Quiz: Hello World Video: First Steps Wrap Up Video: Meet Marga, the Curriculum Developer Discussion Prompt: Your First Programming Code Week 2: Basic Python Syntax Video: Basic Python Syntax introduction Video: Data Types Reading: Data Types Recap Video: Variables Video: Expressions, Numbers, and Type Conversions Reading: Implicit vs Explicit Conversion Practice Quiz: Practice Quiz: Expressions and Variables Video: Defining Functions Reading: Defining Functions Recap Video: Returning Values Reading: Returning Values Using Functions Video: The Principles of Code Reuse Video: Code Style Practice Quiz: Practice Quiz: Functions Video: Comparing Things Reading: Comparison Operators Video: Branching with if Statements Reading: if Statements Recap Video: else Statements Reading: else Statements and the Modulo Operator Video: elif Statements Reading: More Complex Branching with elif Statements Reading: Conditionals Cheat Sheet Practice Quiz: Practice Quiz: Conditionals Video: Basic Syntax Wrap Up Video: In Marga's Words: Why I Like Python Discussion Prompt: Basic Python Syntax Review Week 3: Loops Video: Introduction to Loops Video: What is a while loop? Reading: Anatomy of a While Loop Video: More while Loop Examples Video: Why Initializing Variables Matters Reading: Common Pitfalls With Variable Initialization Video: Infinite Loops and How to Break Them Reading: Infinite loops and Code Blocks Practice Quiz: Practice Quiz: While Loops Video: What is a for loop? Reading: For Loops Recap Video: More for Loop Examples Reading: A Closer Look at the Range() Function Video: Nested for Loops Video: Common Errors in for Loops Reading: Loops Cheat Sheet Practice Quiz: Practice Quiz: For Loops Video: What is recursion? (Optional) Video: Recursion in Action in the IT Context Reading: Additional Recursion Sources Practice Quiz: Practice Quiz: Recursion Video: Loops Wrap Up Video: In Marga's Words: How I Got Into Programming Discussion Prompt: Solving Problems with Loops Week 4: Strings, Lists and Dictionaries Video: Basic Structures Introduction Video: What is a string? Video: The Parts of a String Reading: String Indexing and Slicing Video: Creating New Strings Reading: Basic String Methods Video: More String Methods Reading: Advanced String Methods Video: Formatting Strings Reading: String Formatting Reading: String Reference Cheat Sheet Reading: Formatting Strings Cheat Sheet Practice Quiz: Practice Quiz: Strings Video: What is a list? Reading: Lists Defined Video: Modifying the Contents of a List Reading: Modifying Lists Video: Lists and Tuples Reading: Tuples Video: Iterating over Lists and Tuples Reading: Iterating Over Lists Using Enumerate Video: List Comprehensions Reading: List Comprehensions Reading: Lists and Tuples Operations Cheat Sheet Practice Quiz: Practice Quiz: Lists Video: What is a dictionary? Reading: Dictionaries Defined Video: Iterating over the Contents of a Dictionary Reading: Iterating Over Dictionaries Video: Dictionaries vs. Lists Reading: Dictionary Methods Cheat Sheet Practice Quiz: Practice Quiz: Dictionaries Video: Basic Structures Wrap Up Video: In Marga's Words: My Most Challenging Script Discussion Prompt: Discussion Prompt Week 5: Object Oriented Programming (Optional) Video: OOP Introduction (Optional) Video: What is Object-oriented programming? (Optional) Reading: Object-Oriented Programming Defined Video: Classes and Objects in Python (Optional) Reading: Classes and Objects in Detail Video: Defining New Classes (Optional) Reading: Defining Classes (Optional) Practice Quiz: Practice Quiz: Object-oriented Programming (Optional) Video: Instance Methods (Optional) Reading: What Is a Method? Video: Constructors and Other Special Methods (Optional) Reading: Special Methods Video: Documenting Functions, Classes, and Methods (Optional) Reading: Documenting with Docstrings Reading: Classes and Methods Cheat Sheet (Optional) Video: About Jupyter Notebooks (Optional) Reading: Help with Jupyter Notebooks (Optional) Ungraded Lab: Methods and Classes (Optional) Video: Inheritance (Optional) Reading: Object Inheritance Video: Composition (Optional) Reading: Object Composition Video: Python Modules (Optional) Reading: Augmenting Python with Modules Reading: Supplemental Reading for Code Reuse (Optional) Ungraded Lab: Code Reuse (Optional) Video: OOP Wrap Up (Optional) Video: In Marga's Words: My Favorite Course (Optional) Ungraded Lab: Practice Notebook: Object Oriented Programming (Optional) Discussion Prompt: Object Oriented Programming Assignment (Optional) Week 6: Final Project Video: Final Project Introduction Video: Problem Statement Video: Research Video: Planning Video: Writing the Script Video: Putting It All Together Video: Using Graded Jupyter Notebooks Ungraded Lab: Putting It All Together Video: In Marga's Words: Developing an Inclusive Curriculum Video: Final Project Overview Reading: Final Project Help Ungraded Lab: Final Project: WordCloud Discussion Prompt: Share your word cloud! Video: Congratulations! Discussion Prompt: Your Learner Journey Video: Sneak Peek of the Next Course","title":"Crash Course on Python - Google"},{"location":"random/links/","text":"\ud83d\udd17 Random useful links \u00b6 VDI Teams install for blur background Check linux kernel \u00b6 uname -r Docker locks from installation \u00b6 DEBIAN_FRONTEND=noninteractive","title":"\ud83d\udd17 Random useful links"},{"location":"random/links/#random-useful-links","text":"VDI Teams install for blur background","title":"\ud83d\udd17 Random useful links"},{"location":"random/links/#check-linux-kernel","text":"uname -r","title":"Check linux kernel"},{"location":"random/links/#docker-locks-from-installation","text":"DEBIAN_FRONTEND=noninteractive","title":"Docker locks from installation"},{"location":"serverless/links/","text":"\ud83d\udd17 Azure Serverless links \u00b6 Github links \u00b6 Serverless microservices references architecture Modern cloud apps","title":"\ud83d\udd17 Azure Serverless links"},{"location":"serverless/links/#azure-serverless-links","text":"","title":"\ud83d\udd17 Azure Serverless links"},{"location":"serverless/links/#github-links","text":"Serverless microservices references architecture Modern cloud apps","title":"Github links"},{"location":"spark/dotnet-spark-docker-ubuntu/","text":"Running Dotnet Spark applications on Ubuntu Continer \u00b6 Let's create the first console application called HelloSpark \u00b6 Create console Application Bash dotnet new console -o HelloSpark Add nuget package Microsoft.Spark Bash cd ~/Projects/HelloSpark dotnet add package Microsoft.Spark Let open in VS Code and add some code to it Bash code . and add following code ion Program.cs C# using Microsoft.Spark.Sql ; namespace HelloSpark { class Program { static void Main ( string [] args ) { var spark = SparkSession . Builder (). GetOrCreate (); var df = spark . Read (). Json ( \"people.json\" ); df . Show (); } } } Add person.js file JSON { \"name\" : \"Michael\" } { \"name\" : \"Andy\" , \"age\" : 30 } { \"name\" : \"Justin\" , \"age\" : 19 } Change .csproj file to copy json file to output directory ```xml{10-12} Exe netcoreapp3.1 Always Text Only 6. Build the application dotnet build Text Only 7. Run the application in the docker container with network mapped to host ```bash docker run -d --name dotnet-spark --network host -v \"$HOME/Projects/HelloSpark/bin/Debug:/dotnet/Debug\" 3rdman/dotnet-spark:latest Docker containe should be running at this point Other option would be to run with port mapping Bash docker run -d --name dotnet-spark -p 8080 :8080 -p 8081 :8081 -p 5567 :5567 -p 4040 :4040 -v \" $HOME /Projects/HelloSpark/bin/Debug:/dotnet/Debug\" 3rdman/dotnet-spark:latest where port 8080 (spark-master), 8081 (spark-slave) 5567 (backend-debugging) and 4040 (Spark UI) Configure VS Code to debug the application launch.json JSON { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \".NET Core Launch (console)\" , \"type\" : \"coreclr\" , \"request\" : \"launch\" , \"preLaunchTask\" : \"build\" , // If you have changed target frameworks, make sure to update the program path. \"program\" : \"${workspaceFolder}/bin/Debug/netcoreapp3.1/HelloSpark.dll\" , \"args\" : [], \"cwd\" : \"${workspaceFolder}\" , // For more information about the 'console' field, see https://aka.ms/VSCode-CS-LaunchJson-Console \"console\" : \"internalConsole\" , \"stopAtEntry\" : false , \"logging\" : { \"moduleLoad\" : false } }, { \"name\" : \".NET Core Attach\" , \"type\" : \"coreclr\" , \"request\" : \"attach\" , \"processId\" : \"${command:pickProcess}\" } ] } When we hit debug it might say the build task is note avialable, it will ask to create one, choose .Net Core Awesome, we could see now we are able to run and debug the application Tip Aswesome!!! No hassle of configuring all the spark and stuff locally Warning While debugging on VS Code in Ubuntu use only one instance otherwise debugger won't work. Warning TODO: Investigate why the execution is looking for json file at '/dotnet/Debug/netcoreapp3.1/' for other project Project URL Commands to execute after cloning: Bash dotnet restore dotnet build docker run -d --name dotnet-spark-helloudf --network host -v \" $HOME /Projects/HelloUdf/bin/Debug:/dotnet/HelloUdf/Debug\" 3rdman/dotnet-spark:latest docker exec -it dotnet-spark-helloudf /bin/bash Failure Somehow docker is looking for Microsoft.Spark.Worker in host directory java.io.IOException: Cannot run program \"/home/rig/bin/Microsoft.Spark.Worker/Microsoft.Spark.Worker\": error=2, No such file or directory","title":"Running Dotnet Spark applications on Ubuntu Continer"},{"location":"spark/dotnet-spark-docker-ubuntu/#running-dotnet-spark-applications-on-ubuntu-continer","text":"","title":"Running Dotnet Spark applications on Ubuntu Continer"},{"location":"spark/dotnet-spark-docker-ubuntu/#lets-create-the-first-console-application-called-hellospark","text":"Create console Application Bash dotnet new console -o HelloSpark Add nuget package Microsoft.Spark Bash cd ~/Projects/HelloSpark dotnet add package Microsoft.Spark Let open in VS Code and add some code to it Bash code . and add following code ion Program.cs C# using Microsoft.Spark.Sql ; namespace HelloSpark { class Program { static void Main ( string [] args ) { var spark = SparkSession . Builder (). GetOrCreate (); var df = spark . Read (). Json ( \"people.json\" ); df . Show (); } } } Add person.js file JSON { \"name\" : \"Michael\" } { \"name\" : \"Andy\" , \"age\" : 30 } { \"name\" : \"Justin\" , \"age\" : 19 } Change .csproj file to copy json file to output directory ```xml{10-12} Exe netcoreapp3.1 Always Text Only 6. Build the application dotnet build Text Only 7. Run the application in the docker container with network mapped to host ```bash docker run -d --name dotnet-spark --network host -v \"$HOME/Projects/HelloSpark/bin/Debug:/dotnet/Debug\" 3rdman/dotnet-spark:latest Docker containe should be running at this point Other option would be to run with port mapping Bash docker run -d --name dotnet-spark -p 8080 :8080 -p 8081 :8081 -p 5567 :5567 -p 4040 :4040 -v \" $HOME /Projects/HelloSpark/bin/Debug:/dotnet/Debug\" 3rdman/dotnet-spark:latest where port 8080 (spark-master), 8081 (spark-slave) 5567 (backend-debugging) and 4040 (Spark UI) Configure VS Code to debug the application launch.json JSON { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \".NET Core Launch (console)\" , \"type\" : \"coreclr\" , \"request\" : \"launch\" , \"preLaunchTask\" : \"build\" , // If you have changed target frameworks, make sure to update the program path. \"program\" : \"${workspaceFolder}/bin/Debug/netcoreapp3.1/HelloSpark.dll\" , \"args\" : [], \"cwd\" : \"${workspaceFolder}\" , // For more information about the 'console' field, see https://aka.ms/VSCode-CS-LaunchJson-Console \"console\" : \"internalConsole\" , \"stopAtEntry\" : false , \"logging\" : { \"moduleLoad\" : false } }, { \"name\" : \".NET Core Attach\" , \"type\" : \"coreclr\" , \"request\" : \"attach\" , \"processId\" : \"${command:pickProcess}\" } ] } When we hit debug it might say the build task is note avialable, it will ask to create one, choose .Net Core Awesome, we could see now we are able to run and debug the application Tip Aswesome!!! No hassle of configuring all the spark and stuff locally Warning While debugging on VS Code in Ubuntu use only one instance otherwise debugger won't work. Warning TODO: Investigate why the execution is looking for json file at '/dotnet/Debug/netcoreapp3.1/' for other project Project URL Commands to execute after cloning: Bash dotnet restore dotnet build docker run -d --name dotnet-spark-helloudf --network host -v \" $HOME /Projects/HelloUdf/bin/Debug:/dotnet/HelloUdf/Debug\" 3rdman/dotnet-spark:latest docker exec -it dotnet-spark-helloudf /bin/bash Failure Somehow docker is looking for Microsoft.Spark.Worker in host directory java.io.IOException: Cannot run program \"/home/rig/bin/Microsoft.Spark.Worker/Microsoft.Spark.Worker\": error=2, No such file or directory","title":"Let's create the first console application called HelloSpark"},{"location":"spark/dotnet-spark-docker/","text":"Run and Debug Dotnet Spark application on Docker \u00b6 Warning Please go to \"Running Dotnet Spark applications on Ubuntu Continer\" for running dotnet spark applicatoin on Docker Instal Docker Desktop \u00b6 Batchfile choco install docker-desktop Tip Make sure to restart the machine after installation. If there is an access issue add current user to docker group. User Docker container \u00b6 Precompiled Container If you want to custom compile, visit Dotnet Spark Container Script and build.sh Batchfile docker run -d --name dotnet-spark -p 8080:8080 -p 8081:8081 -e SPARK_DEBUG_DISABLED=true 3rdman/dotnet-spark:latest Docker container should be running as Batchfile docker ps Builing and exectuting the example application \u00b6 Go to docker bash Batchfile docker exec -it dotnet-spark /bin/bash Navigate to the project Batchfile cd /dotnet/HelloSpark ls -la Where json file as sample data. Now let's build Batchfile dotnet build Once the build is completed, there will be bin and obj folders and let's navigate to the bin folder now Batchfile cd /dotnet/HelloSpark/bin/Debug/netcoreapp3.1 The sample application expects json file, so let's copy it from the application folder Batchfile cp /dotnet/HelloSpark/people.json . Finally, let's execute the application spark-submit Batchfile spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master spark://$HOSTNAME:$SPARK_MASTER_PORT microsoft-spark-2.4.x-0.12.1.jar dotnet HelloSpark.dll Tip Have a proper version of jar from the bin folder i.e microsoft-spark-2.4.x-0.12.1.jar We can verify the application and workder at http://localhost:8080/ Debug Spark applciation using VS Code \u00b6 Start the container with mounted project volume or folder Batchfile docker run -d --name dotnet-spark -p 8080:8080 -p 8081:8081 -v \"C:\\Projects\\ML\\mySparkApp\\bin\\Debug:/dotnet/Debug\" -e SPARK_DEBUG_DISABLED=true 3rdman/dotnet-spark:latest Windows will have notification to share Debug folder to share, you should accept it. Unfortunately, there is an exception due to a lot more different undefined dyanmic port mapping there This can be resolve in linus using host network as Batchfile docker run -d --name dotnet-spark --network host -v \"$HOME/Projects/ML/mySparkApp/bin/Debug:/dotnet/Debug\" -e SPARK_DEBUG_DISABLED=true 3rdman/dotnet-spark:latest Warning So we might get fix for window from Spark or Docker in near future till then stick with Linux","title":"Run and Debug Dotnet Spark application on Docker"},{"location":"spark/dotnet-spark-docker/#run-and-debug-dotnet-spark-application-on-docker","text":"Warning Please go to \"Running Dotnet Spark applications on Ubuntu Continer\" for running dotnet spark applicatoin on Docker","title":"Run and Debug Dotnet Spark application on Docker"},{"location":"spark/dotnet-spark-docker/#instal-docker-desktop","text":"Batchfile choco install docker-desktop Tip Make sure to restart the machine after installation. If there is an access issue add current user to docker group.","title":"Instal Docker Desktop"},{"location":"spark/dotnet-spark-docker/#user-docker-container","text":"Precompiled Container If you want to custom compile, visit Dotnet Spark Container Script and build.sh Batchfile docker run -d --name dotnet-spark -p 8080:8080 -p 8081:8081 -e SPARK_DEBUG_DISABLED=true 3rdman/dotnet-spark:latest Docker container should be running as Batchfile docker ps","title":"User Docker container"},{"location":"spark/dotnet-spark-docker/#builing-and-exectuting-the-example-application","text":"Go to docker bash Batchfile docker exec -it dotnet-spark /bin/bash Navigate to the project Batchfile cd /dotnet/HelloSpark ls -la Where json file as sample data. Now let's build Batchfile dotnet build Once the build is completed, there will be bin and obj folders and let's navigate to the bin folder now Batchfile cd /dotnet/HelloSpark/bin/Debug/netcoreapp3.1 The sample application expects json file, so let's copy it from the application folder Batchfile cp /dotnet/HelloSpark/people.json . Finally, let's execute the application spark-submit Batchfile spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master spark://$HOSTNAME:$SPARK_MASTER_PORT microsoft-spark-2.4.x-0.12.1.jar dotnet HelloSpark.dll Tip Have a proper version of jar from the bin folder i.e microsoft-spark-2.4.x-0.12.1.jar We can verify the application and workder at http://localhost:8080/","title":"Builing and exectuting the example application"},{"location":"spark/dotnet-spark-docker/#debug-spark-applciation-using-vs-code","text":"Start the container with mounted project volume or folder Batchfile docker run -d --name dotnet-spark -p 8080:8080 -p 8081:8081 -v \"C:\\Projects\\ML\\mySparkApp\\bin\\Debug:/dotnet/Debug\" -e SPARK_DEBUG_DISABLED=true 3rdman/dotnet-spark:latest Windows will have notification to share Debug folder to share, you should accept it. Unfortunately, there is an exception due to a lot more different undefined dyanmic port mapping there This can be resolve in linus using host network as Batchfile docker run -d --name dotnet-spark --network host -v \"$HOME/Projects/ML/mySparkApp/bin/Debug:/dotnet/Debug\" -e SPARK_DEBUG_DISABLED=true 3rdman/dotnet-spark:latest Warning So we might get fix for window from Spark or Docker in near future till then stick with Linux","title":"Debug Spark applciation using VS Code"},{"location":"spark/dotnet-spark-learning/","text":"Spark Machine Learning \u00b6 TALib \u00b6 programgreek - TALib \u00b6 Quantopian Quantopian - Notebook What is Algorithmic Trading & How to Get Started Python for Financial Analysis Useful sites with samples \u00b6 Data-flaire.training Sparkbyexamples programgreek - Python Stock Python Projects \u00b6 https://github.com/moyuanz/DevilYuan https://www.programcreek.com/python/?code=moyuanz%2FDevilYuan%2FDevilYuan-master%2FStock%2FData%2FUtility%2FDyStockDataUtility.py https://github.com/QUANTAXIS/QUANTAXIS https://www.programcreek.com/python/?code=QUANTAXIS%2FQUANTAXIS%2FQUANTAXIS-master%2FQUANTAXIS%2FQAIndicator%2Ftalib_numpy.py https://github.com/bbfamily/abu https://www.programcreek.com/python/?code=bbfamily%2Fabu%2Fabu-master%2Fabupy%2FIndicatorBu%2FABuNDBoll.py API for Sec Data for companies \u00b6 https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany%27&CIK=msft&type=%2710-k%27&owner=exclude&output=json ML Tuning \u00b6 ML Tuning Machine leaning from Disaster and Github code Sample Data \u00b6 Sample Stock Data - Kaggle Daily Financial News - Kaggle NLP \u00b6 Stanford NLP github - Readme Stanford NLP nuget Stanford CoreNLP for .Net Spark Deep learning - github Learning Spark v1 Book code - github Learning Spark v2 Book code - github Spark: The Definitive Guide Book code - github Tutorial \u00b6 Apache Spark Beginner Tutorial - Kaggle Deep learning with Apache Spark Resources \u00b6 Deep MLP github Spark MLP github Price prediction using ML ANd Spark github Stock price prediction github - Good to get library information Probably complicated one * StockInference Spark github Git Users https://legacy.gitbook.com/@jaceklaskowski https://github.com/jaceklaskowski 18 Machine Learning Platforms for Developers - Kaggle EDA, feature engineering and everything - Kaggle Courses \u00b6 Intro to Machine Learning - Kaggle Intermediate Machine Learning - Kaggle Deep Learning - Kaggle Some other info \u00b6 NY Stock Price Prediction RNN LSTM GRU - Kaggle Finance Time series basic with Pandas and Numpy Tools \u00b6 Charting tool - plotly","title":"Spark Machine Learning"},{"location":"spark/dotnet-spark-learning/#spark-machine-learning","text":"","title":"Spark Machine Learning"},{"location":"spark/dotnet-spark-learning/#talib","text":"programgreek - TALib","title":"TALib"},{"location":"spark/dotnet-spark-learning/#_1","text":"Quantopian Quantopian - Notebook What is Algorithmic Trading & How to Get Started Python for Financial Analysis","title":""},{"location":"spark/dotnet-spark-learning/#useful-sites-with-samples","text":"Data-flaire.training Sparkbyexamples programgreek - Python","title":"Useful sites with samples"},{"location":"spark/dotnet-spark-learning/#stock-python-projects","text":"https://github.com/moyuanz/DevilYuan https://www.programcreek.com/python/?code=moyuanz%2FDevilYuan%2FDevilYuan-master%2FStock%2FData%2FUtility%2FDyStockDataUtility.py https://github.com/QUANTAXIS/QUANTAXIS https://www.programcreek.com/python/?code=QUANTAXIS%2FQUANTAXIS%2FQUANTAXIS-master%2FQUANTAXIS%2FQAIndicator%2Ftalib_numpy.py https://github.com/bbfamily/abu https://www.programcreek.com/python/?code=bbfamily%2Fabu%2Fabu-master%2Fabupy%2FIndicatorBu%2FABuNDBoll.py","title":"Stock Python Projects"},{"location":"spark/dotnet-spark-learning/#api-for-sec-data-for-companies","text":"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany%27&CIK=msft&type=%2710-k%27&owner=exclude&output=json","title":"API for Sec Data for companies"},{"location":"spark/dotnet-spark-learning/#ml-tuning","text":"ML Tuning Machine leaning from Disaster and Github code","title":"ML Tuning"},{"location":"spark/dotnet-spark-learning/#sample-data","text":"Sample Stock Data - Kaggle Daily Financial News - Kaggle","title":"Sample Data"},{"location":"spark/dotnet-spark-learning/#nlp","text":"Stanford NLP github - Readme Stanford NLP nuget Stanford CoreNLP for .Net Spark Deep learning - github Learning Spark v1 Book code - github Learning Spark v2 Book code - github Spark: The Definitive Guide Book code - github","title":"NLP"},{"location":"spark/dotnet-spark-learning/#tutorial","text":"Apache Spark Beginner Tutorial - Kaggle Deep learning with Apache Spark","title":"Tutorial"},{"location":"spark/dotnet-spark-learning/#resources","text":"Deep MLP github Spark MLP github Price prediction using ML ANd Spark github Stock price prediction github - Good to get library information Probably complicated one * StockInference Spark github Git Users https://legacy.gitbook.com/@jaceklaskowski https://github.com/jaceklaskowski 18 Machine Learning Platforms for Developers - Kaggle EDA, feature engineering and everything - Kaggle","title":"Resources"},{"location":"spark/dotnet-spark-learning/#courses","text":"Intro to Machine Learning - Kaggle Intermediate Machine Learning - Kaggle Deep Learning - Kaggle","title":"Courses"},{"location":"spark/dotnet-spark-learning/#some-other-info","text":"NY Stock Price Prediction RNN LSTM GRU - Kaggle Finance Time series basic with Pandas and Numpy","title":"Some other info"},{"location":"spark/dotnet-spark-learning/#tools","text":"Charting tool - plotly","title":"Tools"},{"location":"spark/dotnet-spark-ubuntu/","text":"Install Dotnet Spark on Ubuntu \u00b6 Warning Please go to \"Running Dotnet Spark applications on Ubuntu Continer\" for running dotnet spark applicatoin on Docker Install Dotnet core \u00b6 Bash sudo apt-get update ; \\ sudo apt-get install -y apt-transport-https && \\ sudo apt-get update && \\ sudo apt-get install -y aspnetcore-runtime-2.1 For 3.1 Bash sudo apt-get update ; \\ sudo apt-get install -y apt-transport-https && \\ sudo apt-get update && \\ sudo apt-get install -y dotnet-sdk-3.1 Tip If run into problem, 1. The repository ' http://dl.google.com/linux/chrome/deb stable InRelease' is no longer signed. N: Updating from such a repository can't be done securely, and is therefore disabled by default. Solution: Bash wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://dl.yarnpkg.com/debian stable InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 23E7166788B63E1E Solution: Bash wget -qO - https://raw.githubusercontent.com/yarnpkg/releases/gh-pages/debian/pubkey.gpg | sudo apt-key add - ::: Install OpenJDK \u00b6 Bash sudo apt-get install openjdk-8-jre Install Apache Maven 3.6.3+ \u00b6 Bash mkdir -p ~/bin/maven cd ~/bin/maven wget https://www-us.apache.org/dist/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar -xvzf apache-maven-3.6.3-bin.tar.gz ln -s apache-maven-3.6.3 current export M2_HOME = ~/bin/maven/current export PATH = ${ M2_HOME } /bin: ${ PATH } source ~/.bashrc Install Apache Spark 2.4.1 \u00b6 Bash mkdir -p ~/bin/spark-2.4.1-bin-hadoop2.7 wget https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz tar -xvzf spark-2.4.1-bin-hadoop2.7.tar.gz export SPARK_HOME = ~/bin/spark-2.4.1-bin-hadoop2.7 export PATH = \" $SPARK_HOME /bin: $PATH \" source ~/.bashrc Probably tar -xvzf spark-2.4.1-bin-hadoop2.7.tar.gz . to unzip same folder than creating a new one inside Verify all the installed components so far: Bash spark-shell dotnet --version java -version mvn -version Install Microsoft.Spark.Worker \u00b6 Bash mkdir -p ~/bin/Microsoft.Spark.Worker wget https://github.com/dotnet/spark/releases/download/v0.12.1/Microsoft.Spark.Worker.netcoreapp3.1.linux-x64-0.12.1.tar.gz tar -xvzf Microsoft.Spark.Worker.netcoreapp3.1.linux-x64-0.12.1.tar.gz --directory ~/bin/Microsoft.Spark.Worker export DOTNET_WORKER_DIR = ~/bin/Microsoft.Spark.Worker source ~/.bashrc Clone sample application \u00b6 Bash git clone https://github.com/dotnet/spark.git ~/Projects/dotnet.spark Building Spark .NET Scala Extensions Layer \u00b6 When you submit a .NET application, Spark .NET has the necessary logic written in Scala that inform Apache Spark how to handle your requests (e.g., request to create a new Spark Session, request to transfer data from .NET side to JVM side etc.). This logic can be found in the Spark .NET Scala Source Code . Let us now build the Spark .NET Scala extension layer. This is easy to do: Bash cd ~/Projects/dotnet.spark/src/scala mvn clean package Ran into problem: You should see JARs created for the supported Spark versions: * microsoft-spark-2.3.x/target/microsoft-spark-2.3.x- .jar * microsoft-spark-2.4.x/target/microsoft-spark-2.4.x- .jar Building .NET Sample Applications using .NET Core CLI \u00b6 Build the Worker Bash cd ~/Projects/dotnet.spark/src/csharp/Microsoft.Spark.Worker/ dotnet publish -f netcoreapp3.1 -r linux-x64 Build the Samples Bash cd ~/Projects/dotnet.spark/examples/Microsoft.Spark.CSharp.Examples/ dotnet publish -f netcoreapp3.1 -r linux-x64 Warning Let's move on to Docker Continer rather","title":"Install Dotnet Spark on Ubuntu"},{"location":"spark/dotnet-spark-ubuntu/#install-dotnet-spark-on-ubuntu","text":"Warning Please go to \"Running Dotnet Spark applications on Ubuntu Continer\" for running dotnet spark applicatoin on Docker","title":"Install Dotnet Spark on Ubuntu"},{"location":"spark/dotnet-spark-ubuntu/#install-dotnet-core","text":"Bash sudo apt-get update ; \\ sudo apt-get install -y apt-transport-https && \\ sudo apt-get update && \\ sudo apt-get install -y aspnetcore-runtime-2.1 For 3.1 Bash sudo apt-get update ; \\ sudo apt-get install -y apt-transport-https && \\ sudo apt-get update && \\ sudo apt-get install -y dotnet-sdk-3.1 Tip If run into problem, 1. The repository ' http://dl.google.com/linux/chrome/deb stable InRelease' is no longer signed. N: Updating from such a repository can't be done securely, and is therefore disabled by default. Solution: Bash wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://dl.yarnpkg.com/debian stable InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 23E7166788B63E1E Solution: Bash wget -qO - https://raw.githubusercontent.com/yarnpkg/releases/gh-pages/debian/pubkey.gpg | sudo apt-key add - :::","title":"Install Dotnet core"},{"location":"spark/dotnet-spark-ubuntu/#install-openjdk","text":"Bash sudo apt-get install openjdk-8-jre","title":"Install OpenJDK"},{"location":"spark/dotnet-spark-ubuntu/#install-apache-maven-363","text":"Bash mkdir -p ~/bin/maven cd ~/bin/maven wget https://www-us.apache.org/dist/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar -xvzf apache-maven-3.6.3-bin.tar.gz ln -s apache-maven-3.6.3 current export M2_HOME = ~/bin/maven/current export PATH = ${ M2_HOME } /bin: ${ PATH } source ~/.bashrc","title":"Install Apache Maven 3.6.3+"},{"location":"spark/dotnet-spark-ubuntu/#install-apache-spark-241","text":"Bash mkdir -p ~/bin/spark-2.4.1-bin-hadoop2.7 wget https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz tar -xvzf spark-2.4.1-bin-hadoop2.7.tar.gz export SPARK_HOME = ~/bin/spark-2.4.1-bin-hadoop2.7 export PATH = \" $SPARK_HOME /bin: $PATH \" source ~/.bashrc Probably tar -xvzf spark-2.4.1-bin-hadoop2.7.tar.gz . to unzip same folder than creating a new one inside Verify all the installed components so far: Bash spark-shell dotnet --version java -version mvn -version","title":"Install Apache Spark 2.4.1"},{"location":"spark/dotnet-spark-ubuntu/#install-microsoftsparkworker","text":"Bash mkdir -p ~/bin/Microsoft.Spark.Worker wget https://github.com/dotnet/spark/releases/download/v0.12.1/Microsoft.Spark.Worker.netcoreapp3.1.linux-x64-0.12.1.tar.gz tar -xvzf Microsoft.Spark.Worker.netcoreapp3.1.linux-x64-0.12.1.tar.gz --directory ~/bin/Microsoft.Spark.Worker export DOTNET_WORKER_DIR = ~/bin/Microsoft.Spark.Worker source ~/.bashrc","title":"Install Microsoft.Spark.Worker"},{"location":"spark/dotnet-spark-ubuntu/#clone-sample-application","text":"Bash git clone https://github.com/dotnet/spark.git ~/Projects/dotnet.spark","title":"Clone sample application"},{"location":"spark/dotnet-spark-ubuntu/#building-spark-net-scala-extensions-layer","text":"When you submit a .NET application, Spark .NET has the necessary logic written in Scala that inform Apache Spark how to handle your requests (e.g., request to create a new Spark Session, request to transfer data from .NET side to JVM side etc.). This logic can be found in the Spark .NET Scala Source Code . Let us now build the Spark .NET Scala extension layer. This is easy to do: Bash cd ~/Projects/dotnet.spark/src/scala mvn clean package Ran into problem: You should see JARs created for the supported Spark versions: * microsoft-spark-2.3.x/target/microsoft-spark-2.3.x- .jar * microsoft-spark-2.4.x/target/microsoft-spark-2.4.x- .jar","title":"Building Spark .NET Scala Extensions Layer"},{"location":"spark/dotnet-spark-ubuntu/#building-net-sample-applications-using-net-core-cli","text":"Build the Worker Bash cd ~/Projects/dotnet.spark/src/csharp/Microsoft.Spark.Worker/ dotnet publish -f netcoreapp3.1 -r linux-x64 Build the Samples Bash cd ~/Projects/dotnet.spark/examples/Microsoft.Spark.CSharp.Examples/ dotnet publish -f netcoreapp3.1 -r linux-x64 Warning Let's move on to Docker Continer rather","title":"Building .NET Sample Applications using .NET Core CLI"},{"location":"spark/install-docker-ubuntu2004/","text":"Install Docker on Ubuntu 20.04 LTS and Dotnet Spark on Docker \u00b6 Tip Make sure to have Ubuntu 20.04 installed and turn on Ubuntu-20.04 in Docker Settings -> Resources -> WSL Integration Use the apt command to install the docker.io package: Bash sudo apt-get update sudo apt install docker.io Start docker and enable it to start after the system reboot: Bash sudo systemctl enable --now docker Run, if error \"System has not been booted with systemd as init system (PID 1). Can't operate. Failed to connect to bus: Host is down\" Bash sudo dockerd If error \"Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied\" Bash sudo groupadd docker sudo usermod -aG docker ${ user } // where ${ user } is current user \"ps\" su -s ${ USER } // Log out and log back in Restart WSL 2 ubuntu from Powershell, if su -s command doesn't work PowerShell Get-Service LxssManager | Restart-Service Optionally give any user administrative privileges to docker: Bash sudo usermod -aG docker SOMEUSERNAME where SOMEUSERNAME is admin user Check docker version: Bash docker --version Test Docker Bash docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Install Dotnet 3.1 on Ubuntu 20.04 LTS Bash wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-3.1 Just in case if need to clean prior installations Bash sudo dpkg --purge packages-microsoft-prod && sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update Install VS Code from other post And finally, Clone sample application from HelloUdf we should be able to run and debug dotnet spark on docker Bash docker run -d --name dotnet-spark-helloudf --network host -v \" $HOME /Projects/HelloUdf/bin/Debug:/dotnet/Debug\" 3rdman/dotnet-spark:latest Install Docker compose \u00b6 Bash sudo apt install docker-compose Run using docker-compose \u00b6 docker-compose.yml version : '3' services : dotnet-spark-helloudf : container_name : dotnet-spark-helloudf image : 3rdman/dotnet-spark:latest network_mode : host volumes : - $HOME/Projects/HelloUdf/bin/Debug:/dotnet/Debug","title":"Install Docker on Ubuntu 20.04 LTS and Dotnet Spark on Docker"},{"location":"spark/install-docker-ubuntu2004/#install-docker-on-ubuntu-2004-lts-and-dotnet-spark-on-docker","text":"Tip Make sure to have Ubuntu 20.04 installed and turn on Ubuntu-20.04 in Docker Settings -> Resources -> WSL Integration Use the apt command to install the docker.io package: Bash sudo apt-get update sudo apt install docker.io Start docker and enable it to start after the system reboot: Bash sudo systemctl enable --now docker Run, if error \"System has not been booted with systemd as init system (PID 1). Can't operate. Failed to connect to bus: Host is down\" Bash sudo dockerd If error \"Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied\" Bash sudo groupadd docker sudo usermod -aG docker ${ user } // where ${ user } is current user \"ps\" su -s ${ USER } // Log out and log back in Restart WSL 2 ubuntu from Powershell, if su -s command doesn't work PowerShell Get-Service LxssManager | Restart-Service Optionally give any user administrative privileges to docker: Bash sudo usermod -aG docker SOMEUSERNAME where SOMEUSERNAME is admin user Check docker version: Bash docker --version Test Docker Bash docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Install Dotnet 3.1 on Ubuntu 20.04 LTS Bash wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-3.1 Just in case if need to clean prior installations Bash sudo dpkg --purge packages-microsoft-prod && sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update Install VS Code from other post And finally, Clone sample application from HelloUdf we should be able to run and debug dotnet spark on docker Bash docker run -d --name dotnet-spark-helloudf --network host -v \" $HOME /Projects/HelloUdf/bin/Debug:/dotnet/Debug\" 3rdman/dotnet-spark:latest","title":"Install Docker on Ubuntu 20.04 LTS and Dotnet Spark on Docker"},{"location":"spark/install-docker-ubuntu2004/#install-docker-compose","text":"Bash sudo apt install docker-compose","title":"Install Docker compose"},{"location":"spark/install-docker-ubuntu2004/#run-using-docker-compose","text":"docker-compose.yml version : '3' services : dotnet-spark-helloudf : container_name : dotnet-spark-helloudf image : 3rdman/dotnet-spark:latest network_mode : host volumes : - $HOME/Projects/HelloUdf/bin/Debug:/dotnet/Debug","title":"Run using docker-compose"},{"location":"spark/jupyter-pyspark-docker/","text":"Run Jupyter for Pyspark on Docker \u00b6 Install WSL 2 \u00b6 Install Ubuntu 20.01 LTS \u00b6 Install Docker \u00b6 Make sure to have Ubuntu 20.04 installed and turn on Ubuntu-20.04 in Docker Settings -> Resources -> WSL Integration Bash sudo apt-get update sudo apt install docker.io Start docker and enable it to start after the system reboot: Bash sudo systemctl enable --now docker Fork Project \u00b6 https://github.com/padamshrestha/pyspark-practice-notebook Run docker command from WSL command \u00b6 Bash docker run -p 8888 :8888 -p 4040 :4040 -v /mnt/c/Projects/pyspark-practice-notebook/:/home/jovyan/work --name spark jupyter/pyspark-notebook Where Port 8888 for Jupyter editor and 4040 is for Spark Jobs or Bash docker-compose up Open VS Code in container \u00b6 For web version","title":"Run Jupyter for Pyspark on Docker"},{"location":"spark/jupyter-pyspark-docker/#run-jupyter-for-pyspark-on-docker","text":"","title":"Run Jupyter for Pyspark on Docker"},{"location":"spark/jupyter-pyspark-docker/#install-wsl-2","text":"","title":"Install WSL 2"},{"location":"spark/jupyter-pyspark-docker/#install-ubuntu-2001-lts","text":"","title":"Install Ubuntu 20.01 LTS"},{"location":"spark/jupyter-pyspark-docker/#install-docker","text":"Make sure to have Ubuntu 20.04 installed and turn on Ubuntu-20.04 in Docker Settings -> Resources -> WSL Integration Bash sudo apt-get update sudo apt install docker.io Start docker and enable it to start after the system reboot: Bash sudo systemctl enable --now docker","title":"Install Docker"},{"location":"spark/jupyter-pyspark-docker/#fork-project","text":"https://github.com/padamshrestha/pyspark-practice-notebook","title":"Fork Project"},{"location":"spark/jupyter-pyspark-docker/#run-docker-command-from-wsl-command","text":"Bash docker run -p 8888 :8888 -p 4040 :4040 -v /mnt/c/Projects/pyspark-practice-notebook/:/home/jovyan/work --name spark jupyter/pyspark-notebook Where Port 8888 for Jupyter editor and 4040 is for Spark Jobs or Bash docker-compose up","title":"Run docker command from WSL command"},{"location":"spark/jupyter-pyspark-docker/#open-vs-code-in-container","text":"For web version","title":"Open VS Code in container"},{"location":"spark/links/","text":"\ud83d\udd17 Spark links \u00b6 Working with Parquet Databricks tech talks Delta lake - github Delta spark api Delta lake intro Various install Manuals - in chinese How does Delta lake works? Vivek's learning notes Basic PySpark etl for function naming convention Read JSON using PySpark Download Intraday Stock Data with IEX and Parquet Config Documentation from Qubole Tempo - financial services quick start Very nice gitbook from https://github.com/jaceklaskowski - works only with private browser mode How Delta Lake Supercharges Data Lakes from tech-talk Making Apache Spark better with Delta Lake Delta lake sample projects \u00b6 Old PySpark practice notebooks Spark worker number performance Ifood Data Spark Definitive Guide Spark Definitive Guide - data Stock Data Processing in Delta Lake Spark Kafka Spark Delta Nice docker compose for spark Traffic accident classification Nice Datahub custom git with containers for many services Docker with Airflow + Postgres + Spark cluster + JDK (spark-submit support) + Jupyter Notebooks Databricks tempo - value at risk","title":"\ud83d\udd17 Spark links"},{"location":"spark/links/#spark-links","text":"Working with Parquet Databricks tech talks Delta lake - github Delta spark api Delta lake intro Various install Manuals - in chinese How does Delta lake works? Vivek's learning notes Basic PySpark etl for function naming convention Read JSON using PySpark Download Intraday Stock Data with IEX and Parquet Config Documentation from Qubole Tempo - financial services quick start Very nice gitbook from https://github.com/jaceklaskowski - works only with private browser mode How Delta Lake Supercharges Data Lakes from tech-talk Making Apache Spark better with Delta Lake","title":"\ud83d\udd17 Spark links"},{"location":"spark/links/#delta-lake-sample-projects","text":"Old PySpark practice notebooks Spark worker number performance Ifood Data Spark Definitive Guide Spark Definitive Guide - data Stock Data Processing in Delta Lake Spark Kafka Spark Delta Nice docker compose for spark Traffic accident classification Nice Datahub custom git with containers for many services Docker with Airflow + Postgres + Spark cluster + JDK (spark-submit support) + Jupyter Notebooks Databricks tempo - value at risk","title":"Delta lake sample projects"},{"location":"spark/python-learning/","text":"Learning Python \u00b6 Resources \u00b6 From Daniel Bourke Course Blog The Best AI ML and DS Resources End to end Dog Vision Google drive Machine Learning Roadmap 2020","title":"Learning Python"},{"location":"spark/python-learning/#learning-python","text":"","title":"Learning Python"},{"location":"spark/python-learning/#resources","text":"From Daniel Bourke Course Blog The Best AI ML and DS Resources End to end Dog Vision Google drive Machine Learning Roadmap 2020","title":"Resources"},{"location":"synapse/build-dev-pyspark-cluster/","text":"\ud83d\udc33 Build Development Spark cluster - PySpark, Delta Lake \u00b6 Let's build Development cluster where we could run pyspark code in a Spark cluster and load data into delta lake as depicted in the diagram. System Design \u00b6 Container hierarchy \u00b6 We can create the docker images in hierarchy in such a way so we don't have to install same packages everywhere. flowchart TD B[\"\ud83d\udc33 cluster-base\"] B-->C[\"\ud83d\udc33 jupyter\"] B-->D[\"\ud83d\udc33 spark-base\"] B-->E[\"\ud83d\udc33 hive-metastore\"] B-->F[\"\ud83d\udc33 timescaledb\"] D-->G[\"\ud83d\udc33 spark-master\"] D-->H[\"\ud83d\udc33 spark-worker\"] cluster-base.Dockerfile FROM ubuntu:latest # -- Layer: OS + Python 3.11 ARG shared_workspace = /opt/workspace RUN apt-get clean add-apt-repository ppa:deadsnakes/ppa && apt-get update -y && \\ apt-get install -y python3.11 python3-pip curl wget git unzip procps openjdk-8-jdk && \\ ln -s /usr/bin/python3 /usr/bin/python && \\ rm -rf /var/lib/apt/lists/* ENV SHARED_WORKSPACE = ${ shared_workspace } # -- Runtime # Prepare dirs RUN mkdir -p /tmp/logs/ && chmod a+w /tmp/logs/ && mkdir /app && chmod a+rwx /app && mkdir /data && chmod a+rwx /data ENV JAVA_HOME = /usr ENV PATH = $SPARK_HOME : $PATH :/bin: $JAVA_HOME /bin: $JAVA_HOME /jre/bin ENV PYTHONPATH = $SPARK_HOME /python: $PYTHONPATH ENV SHARED_WORKSPACE = /opt/workspace RUN mkdir -p ${ SHARED_WORKSPACE } VOLUME ${SHARED_WORKSPACE} CMD [ \"bash\" ]","title":"\ud83d\udc33 Build Development Spark cluster - PySpark, Delta Lake"},{"location":"synapse/build-dev-pyspark-cluster/#build-development-spark-cluster-pyspark-delta-lake","text":"Let's build Development cluster where we could run pyspark code in a Spark cluster and load data into delta lake as depicted in the diagram.","title":"\ud83d\udc33 Build Development Spark cluster - PySpark, Delta Lake"},{"location":"synapse/build-dev-pyspark-cluster/#system-design","text":"","title":"System Design"},{"location":"synapse/build-dev-pyspark-cluster/#container-hierarchy","text":"We can create the docker images in hierarchy in such a way so we don't have to install same packages everywhere. flowchart TD B[\"\ud83d\udc33 cluster-base\"] B-->C[\"\ud83d\udc33 jupyter\"] B-->D[\"\ud83d\udc33 spark-base\"] B-->E[\"\ud83d\udc33 hive-metastore\"] B-->F[\"\ud83d\udc33 timescaledb\"] D-->G[\"\ud83d\udc33 spark-master\"] D-->H[\"\ud83d\udc33 spark-worker\"] cluster-base.Dockerfile FROM ubuntu:latest # -- Layer: OS + Python 3.11 ARG shared_workspace = /opt/workspace RUN apt-get clean add-apt-repository ppa:deadsnakes/ppa && apt-get update -y && \\ apt-get install -y python3.11 python3-pip curl wget git unzip procps openjdk-8-jdk && \\ ln -s /usr/bin/python3 /usr/bin/python && \\ rm -rf /var/lib/apt/lists/* ENV SHARED_WORKSPACE = ${ shared_workspace } # -- Runtime # Prepare dirs RUN mkdir -p /tmp/logs/ && chmod a+w /tmp/logs/ && mkdir /app && chmod a+rwx /app && mkdir /data && chmod a+rwx /data ENV JAVA_HOME = /usr ENV PATH = $SPARK_HOME : $PATH :/bin: $JAVA_HOME /bin: $JAVA_HOME /jre/bin ENV PYTHONPATH = $SPARK_HOME /python: $PYTHONPATH ENV SHARED_WORKSPACE = /opt/workspace RUN mkdir -p ${ SHARED_WORKSPACE } VOLUME ${SHARED_WORKSPACE} CMD [ \"bash\" ]","title":"Container hierarchy"},{"location":"synapse/build-trade-bot/","text":"Design/Build trade bot \u00b6 Designing a stock and option trade bot that can automate downloading tickers every five minutes, store them in Delta Lake using PySpark, and run various strategies to generate buy and sell signals can be a complex process. Here are the general steps you could take to accomplish this: Gather the necessary data: You will need to acquire historical ticker data for the stocks and options you want to trade. This data can be obtained from financial data providers such as Yahoo Finance or Alpha Vantage. Store the data: Once you have the data, you can store it in Delta Lake. PySpark is a good choice for this because it allows you to process large amounts of data quickly and easily. Develop the trading strategies: You will need to develop trading strategies that use the data to generate buy and sell signals. There are many different strategies that can be used, such as moving averages, momentum indicators, and technical analysis. Implement the bot: You will need to write code to automate the process of downloading the data, storing it in Delta Lake, and running the trading strategies. Test the bot: Before deploying the bot, you will need to test it thoroughly to ensure that it is working correctly and that the strategies are generating accurate buy and sell signals. Deploy the bot: Once you are satisfied that the bot is working correctly, you can deploy it to a trading platform so that it can execute trades automatically. Please note that this is a high-level overview of the process and each step may require significant effort and knowledge. It's important to have a good understanding of trading and market data, as well as experience working with programming languages and big data technologies before attempting to build a trading bot like this. Create delta lake schema to record stock buy and sell \u00b6 JSON { \"type\" : \"record\" , \"name\" : \"stock_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } The above schema defines a record for a stock transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, number of shares, and price per share. This schema can be used with Delta Lake to record stock buy and sell transactions. Also define schema to store buy sell of options \u00b6 JSON { \"type\" : \"record\" , \"name\" : \"options_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"option_type\" , \"type\" : \"string\" }, { \"name\" : \"strike_price\" , \"type\" : \"double\" }, { \"name\" : \"expiration_date\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } This schema defines a record for an options transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, option type (call or put), strike price, expiration date, number of shares, and price per share. This schema can be used with Delta Lake to record buy and sell transactions for options. Create pyspark schema from previous schemas and provide code to read write and delete records \u00b6 Here is the code to create PySpark schemas from the previous JSON schemas, and to read, write, and delete records using the PySpark Delta API: Python from pyspark.sql.types import * # Create PySpark schema for stock transactions stock_schema = StructType . fromJson ( json . loads ( \"\"\"{ \"type\": \"record\", \"name\": \"stock_transaction\", \"fields\": [ {\"name\": \"transaction_id\", \"type\": \"string\"}, {\"name\": \"timestamp\", \"type\": \"string\"}, {\"name\": \"transaction_type\", \"type\": \"string\"}, {\"name\": \"stock_symbol\", \"type\": \"string\"}, {\"name\": \"shares\", \"type\": \"int\"}, {\"name\": \"price\", \"type\": \"double\"} ] }\"\"\" )) # Create PySpark schema for options transactions options_schema = StructType . fromJson ( json . loads ( \"\"\"{ \"type\": \"record\", \"name\": \"options_transaction\", \"fields\": [ {\"name\": \"transaction_id\", \"type\": \"string\"}, {\"name\": \"timestamp\", \"type\": \"string\"}, {\"name\": \"transaction_type\", \"type\": \"string\"}, {\"name\": \"stock_symbol\", \"type\": \"string\"}, {\"name\": \"option_type\", \"type\": \"string\"}, {\"name\": \"strike_price\", \"type\": \"double\"}, {\"name\": \"expiration_date\", \"type\": \"string\"}, {\"name\": \"shares\", \"type\": \"int\"}, {\"name\": \"price\", \"type\": \"double\"} ] }\"\"\" )) # Create a Delta Lake table for stock transactions ( spark . read . format ( \"json\" ) . schema ( stock_schema ) . load ( \"path/to/stock_transactions\" ) . write . format ( \"delta\" ) . save ( \"path/to/stock_transactions_table\" )) # Create a Delta Lake table for options transactions ( spark . read . format ( \"json\" ) . schema ( options_schema ) . load ( \"path/to/options_transactions\" ) . write . format ( \"delta\" ) . save ( \"path/to/options_transactions_table\" )) # Read records from stock transactions table stock_transactions_df = spark . read . format ( \"delta\" ) . load ( \"path/to/stock_transactions_table\" ) # Read records from options transactions table options_transactions_df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_transactions_table\" ) # Write new records to stock transactions table new_stock_transactions_df = spark . createDataFrame ([ ( \"1\" , \"2022-01-01\" , \"buy\" , \"AAPL\" , 100 , 200.0 ), ( \"2\" , \"2022-01-02\" , \"sell\" , \"GOOG\" , 50 , 1500.0 ) ], stock_schema ) new_stock_transactions_df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/stock_transactions_table\" ) # Delete specific records from options transactions table options_ OR Create pyspark schema from previous schemas and provide code to read write and delete records \u00b6 Here is the PySpark code to create the schema for the stock_transaction record: Python from pyspark.sql.types import * stock_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) And here is the PySpark code to create the schema for the options_transaction record: Python options_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"option_type\" , StringType ()), StructField ( \"strike_price\" , DoubleType ()), StructField ( \"expiration_date\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) To read a Delta Lake table into a PySpark DataFrame, you can use the read method: df = spark.read.format(\"delta\").load(\"path/to/delta/table\") To write a DataFrame to a Delta Lake table, you can use the write method: df.write.format(\"delta\").save(\"path/to/delta/table\") To delete a Delta Lake table, you can use the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table \") It is important to note that the above code will delete the entire table. If you would like to delete specific records, you can use a filter in the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table WHERE stock_symbol='AAPL'\") Generate fast-api apis to interact with add, update delete process from pyspark \u00b6 Here is an example of how you could use the FastAPI library to create a REST API that interacts with a Delta Lake table using PySpark: Python from fastapi import FastAPI , HTTPException from pyspark.sql import SparkSession app = FastAPI () # Create a SparkSession spark = SparkSession . builder . appName ( \"DeltaLakeAPI\" ) . getOrCreate () @app . post ( \"/stock\" ) async def add_stock_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], stock_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/stock/ {transaction_id} \" ) async def update_stock_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/stock/ {transaction_id} \" ) async def delete_stock_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } This is an example to handle the stock transactions. You can use the same approach to handle option transaction as well. In the above example, the add_stock_transaction endpoint accepts a JSON object containing the transaction data, creates a DataFrame from that data, and writes it to the Delta Lake table. The update_stock_transaction endpoint accepts a transaction ID and a JSON object containing the updated transaction data, reads the Delta Lake table into a DataFrame, filters the DataFrame for the specified transaction, updates the values in the DataFrame with the incoming data, and overwrites the Delta Lake table with the updated DataFrame. The delete_stock_transaction endpoint accepts a transaction ID and deletes the specified transaction from the Delta Lake table. It is important to note that the above code is an example and may require additional error handling and validation. Also, you should update the path of delta table in the code to use your own delta table. Generate similar api for options schema \u00b6 Here's an example of how you could use the same approach to create REST APIs for the options_transaction schema: Python @app . post ( \"/options\" ) async def add_options_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], options_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/options/ {transaction_id} \" ) async def update_options_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/options/ {transaction_id} \" ) async def delete_options_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/options_delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } The above code is similar to the previous code for handling stock transactions, but it uses the options_transaction schema and different endpoint paths and file paths. As with the previous example, you should update the path of delta table in the code to use your own delta table. Provide api for profit and loss for both stocks and options \u00b6 Here's an example of how you could create a REST API endpoint for calculating profit and loss for both stock and options transactions using PySpark: Python @app . get ( \"/profit_loss/ {stock_symbol} \" ) async def get_profit_loss ( stock_symbol : str ): # Read the stock transaction table into a DataFrame stock_df = spark . read . format ( \"delta\" ) . load ( \"path/to/stock_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Read the options transaction table into a DataFrame options_df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Calculate the profit and loss for the stock transactions stock_profit_loss = stock_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) stock_profit_loss = stock_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] # Calculate the profit and loss for the options transactions options_profit_loss = options_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) options_profit_loss = options_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] return { \"stock_profit_loss\" : stock_profit_loss , \"options_profit_loss\" : options_profit_loss } In the above code, the get_profit_loss endpoint accepts a stock symbol, reads the stock transaction table and options transaction table into DataFrames, filters the DataFrames for transactions with the specified stock symbol, calculates the profit and loss for the stock and options transactions using the formula (price - cost) * shares, and returns the calculated values in a JSON response. In the above code, cost is not present in the schema, you need to add that accordingly. Also, you should update the path of delta table in the code to use your own delta table. It is important to note that the above code is an example and may require additional error handling and validation. Theory \u00b6 what are the best strategies for volatile market \u00b6 There are a variety of different strategies that traders can use in volatile markets, depending on their goals, risk tolerance, and overall trading style. Some common strategies used in volatile markets include: Position trading: This strategy involves taking a long-term view of the market and holding positions for weeks or months. It can be useful for traders who are comfortable with larger price swings and are less concerned about short-term volatility. Swing trading: This strategy involves taking positions that last for several days to a few weeks. It can be useful for traders who want to take advantage of short-term market movements, but do not want to be as active as day traders. Day trading: This strategy involves taking positions that are opened and closed within the same trading day. It can be useful for traders who want to take advantage of short-term market movements and can handle the high level of volatility that comes with day trading. Hedging: This strategy involves taking offsetting positions in different markets or instruments in order to reduce risk. For example, a trader might take a long position in a stock while simultaneously taking a short position in a stock index ETF. Option trading: This strategy involves buying or selling options contracts, which give the holder the right, but not the obligation, to buy or sell an underlying asset at a specific price at a specific time. It's important to keep in mind that no single strategy is guaranteed to be profitable in every market condition. It's also important to have a clear understanding of your own risk tolerance, and to be comfortable with the level of volatility that comes with your chosen strategy. Explain more strategies for day trading \u00b6 Here are a few additional strategies that day traders may use: Breakout trading: This strategy involves identifying key levels of support and resistance and entering a position when the price breaks through one of these levels. Traders may use technical analysis tools such as trend lines and chart patterns to identify these levels. Scalping: This strategy involves taking a large number of small positions and exiting them quickly, often within a matter of minutes. Scalping can be a high-frequency and high-pressure strategy, as traders look to capture small profits in a short amount of time. News-based trading: This strategy involves monitoring news and economic events and entering positions based on how the market is likely to react. News-based traders often use fundamental analysis tools such as economic indicators and earnings reports to make trading decisions. Mean-reversion: This strategy involves identifying trends and patterns in the market and entering positions with the expectation that the market will revert back to its mean. Mean-reversion traders often use technical analysis tools such as moving averages and Bollinger Bands to identify these trends and patterns. Algorithmic trading: This strategy involves using computer programs to make trading decisions, often based on a set of predefined rules. Algorithmic traders often use a combination of technical and fundamental analysis to identify trends and patterns in the market. It's important to note that every of these strategies have different level of risk and volatility, it's important to have a good understanding of the strategy you choose, and to be comfortable with the level of risk and volatility that comes with it. Moreover, it's important to have a trading plan that includes entry and exit rules, risk management, and position sizing. how to detect volatility for given day \u00b6 There are a few different ways to detect volatility for a given day in the market: Average True Range (ATR): The Average True Range is a technical indicator that measures the volatility of a security over a given period of time. It is calculated by taking the difference between the high and low prices for a given period, and then averaging those values over a specified number of periods. Bollinger Bands: Bollinger Bands are a technical indicator that consist of a moving average and two standard deviation lines that are plotted above and below the moving average. When the bands are close together, it indicates low volatility and when the bands are far apart, it indicates high volatility. Historical Volatility: Historical volatility is a measure of the volatility of a security over a given period of time. It is calculated by taking the standard deviation of the logarithmic returns of a security over a specified number of periods. Implied Volatility: Implied volatility is a measure of the expected volatility of a security based on the prices of options contracts on that security. It can be used to detect potential changes in volatility before they occur. Moving average: A moving average is a commonly used technical indicator that plots the average price of a security over a specified number of periods. When the moving average is trending up or down, it indicates that the security is trending. When the moving average is moving sideways, it indicates that the security is consolidating. VIX: VIX is a measure of volatility known as the \"Fear Index\" which is calculated from the S&P 500 index option prices. It's a widely used indicator to gauge the level of fear and uncertainty in the market and also it can be used as a proxy for volatility in the overall market. It's important to keep in mind that no single indicator can provide a perfect measure of volatility, it's important to use multiple indicators and consider other market data to form a more complete picture of volatility. Can ichimoku strategy used for volatility \u00b6 The Ichimoku Cloud, also known as the Ichimoku Kink\u014d Hy\u014d, is a technical analysis indicator that is commonly used to identify trends, support and resistance levels, and potential buy and sell signals. While it is not specifically designed to measure volatility, it can be used to identify changes in volatility by analyzing the position of the price relative to the cloud. The Ichimoku Cloud is composed of several elements, including the Tenkan-sen (conversion line), the Kijun-sen (base line), and the Senkou Span A and B (leading span A and B). The area between Senkou Span A and B forms the \"cloud\", and the position of the price relative to the cloud can indicate whether the market is in a bullish or bearish trend. When the price is above the cloud, it is considered bullish, and when the price is below the cloud, it is considered bearish. If the price is inside the cloud, it indicates a period of consolidation, which can signal a change in volatility. Additionally, the distance between the Tenkan-sen and the Kijun-sen can also be used to identify changes in volatility. When the distance is wide, it indicates a high level of volatility, and when the distance is narrow, it indicates a low level of volatility. In summary, while Ichimoku strategy is mainly used to identify trends, support and resistance levels, and potential buy and sell signals, it can also be used to identify changes in volatility by analyzing the position of the price relative to the cloud and the distance between the Tenkan-sen and the Kijun-sen. However, it's important to use multiple indicators and consider other market data to form a more complete picture of volatility. What are the best strategies for day option trading \u00b6 Day trading options can be a high-risk, high-reward strategy that can be profitable for experienced traders. Here are a few strategies that day traders may use when trading options: Covered call: This strategy involves holding a long position in an underlying asset and selling call options against that position. The goal of this strategy is to generate income from the option premium while also limiting the potential loss on the underlying asset. Bullish vertical spread: This strategy involves buying a call option at a lower strike price and selling a call option at a higher strike price. The goal of this strategy is to profit from a bullish market move while also limiting the potential loss. Bearish vertical spread: This strategy involves buying a put option at a higher strike price and selling a put option at a lower strike price. The goal of this strategy is to profit from a bearish market move while also limiting the potential loss. Iron condor: This strategy involves selling a call option and a put option at different strike prices to generate income from the option premium. The goal of this strategy is to profit from a neutral market move while also limiting the potential loss. Straddle: This strategy involves buying both a call and a put option at the same strike price, with the goal of profiting from a large move in either direction in the underlying asset. It's important to keep in mind that options trading is a complex and high-risk strategy, and it's important to have a good understanding of the market and to use proper risk management techniques. Additionally, it's important to use multiple indicators and other market data to form a more complete picture of the market. It's also highly recommended to paper trade and backtest any strategy before using it in live trading. How to formulate those strategies given the stocks and option data in delta lake and pyspark \u00b6 Formulating option trading strategies using stock and option data in Delta Lake and PySpark would involve several steps: Retrieve the data: You would need to read the stock and option data from Delta Lake using PySpark's DataFrame API. You can use the spark.read method to read the data into a DataFrame and then use the select method to select the relevant columns. Clean and transform the data: After retrieving the data, you would need to perform some cleaning and data transformation to prepare the data for analysis. This may include removing any missing or null values, converting data types, and creating new columns or features. Analysis and modeling: Once the data is cleaned and transformed, you can use PySpark's machine learning library, MLlib, to analyze the data and create models. For example, you can use the linear regression model to analyze the relationship between the stock price and the option prices. Backtesting and evaluating the strategies: Once you have formulated your strategies, you can backtest them using historical data to see how they would have performed in the past. You can use the PySpark's DataFrame API to filter and select the historical data that corresponds to the conditions of your strategy. Deployment and execution: Once you have evaluated your strategies and are satisfied with their performance, you can deploy them in a live trading environment and execute them using the APIs provided by your trading platform. It's important to keep in mind that this process is not trivial and it may require a good understanding of stock and options trading, as well as experience with PySpark, Delta Lake, and machine learning. It's also important to validate and backtest the strategies before deploying them in live trading environment Provide linear regression model to analyze the relationship between the stock and option prices \u00b6 To analyze the relationship between stock and option prices using a linear regression model in PySpark, you would need to perform the following steps: Prepare the data: You would need to retrieve the stock and option data from Delta Lake and clean and transform it to prepare it for analysis. You would need to select the relevant columns such as the stock price, option strike price, expiration date, and option price. Create the feature vector: You would need to create a feature vector containing the relevant columns from the data. The feature vector would typically include the stock price and the option strike price, and potentially other relevant features such as expiration date. Create the label: You would need to create the label, which is the variable that you want to predict. In this case, the label would be the option price. Split the data: You would need to split the data into a training set and a test set. The training set would be used to train the model, and the test set would be used to evaluate the model's performance. Train the model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data. Evaluate the model: Once the model is trained, you can evaluate its performance by applying it to the test data and comparing the predicted option prices to the actual option prices. Python from pyspark.ml.regression import LinearRegression from pyspark.ml.feature import VectorAssembler # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns and create the feature vector feature_cols = [ \"stock_price\" , \"option_strike_price\" , \"expiration_date\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) This is a basic example and you may need to customize it to suit your specific requirements. It's also important to validate and backtest the model before deploying it in live trading environment. Additionally, you may need to consider other factors such as volatility, volume, and other technical indicators to improve the performance of the linear regression model, you could also consider other factors such as volatility, volume, and other technical indicators. Volatility: You can calculate volatility using standard deviation or other volatility indicators such as the average true range (ATR) and add it as a feature in your model. Volume: You can add volume data as a feature in your model and see if it improves the performance of the model. Technical indicators: You can use other technical indicators such as moving averages, relative strength index (RSI), and the Ichimoku Cloud to extract additional information from the stock and option data. It's important to keep in mind that including more features in the model can increase its complexity and may also increase the risk of overfitting. It's important to use techniques such as regularization and feature selection to avoid overfitting. Additionally, you can use various evaluation metrics such as mean squared error, root mean squared error, and R-squared to evaluate the performance of the model and compare it against other models. In summary, linear regression model is a basic model and you may need to consider other factors such as volatility, volume, and other technical indicators to improve the performance of the model. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. Incorporate ichomoku cloud strategy to the linear regression \u00b6 You can incorporate the Ichimoku Cloud strategy into a linear regression model by adding the Ichimoku Cloud indicator as a feature Calculate the Ichimoku Cloud: You would need to calculate the Ichimoku Cloud indicator using the historical stock prices. This indicator is calculated using several moving averages and lines, including the Tenkan-sen, Kijun-sen, and Senkou Span A and B. Add the Ichimoku Cloud as a feature: Once you have calculated the Ichimoku Cloud indicator, you can add it as a feature in the linear regression model. You can do this by adding the values of the Tenkan-sen, Kijun-sen, Senkou Span A, and Senkou Span B lines as separate columns in the data. Train the linear regression model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data, which now includes the Ichimoku Cloud indicator as a feature. Evaluate the model: You can evaluate the performance of the model by applying it to the test data and comparing the predicted option prices to the actual option prices. It's important to keep in mind that incorporating the Ichimoku Cloud strategy into a linear regression model may not necessarily improve the performance of the model, and it's important to validate and backtest the model before deploying it in live trading environment. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. Pyspark Linear Regression \u00b6 Python from pyspark.sql.functions import col from pyspark.ml.regression import LinearRegression from pyspark.ml.feature import VectorAssembler # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Calculate the Ichimoku Cloud indicator data = data . withColumn ( \"tenkan_sen\" , ( col ( \"high\" ) + col ( \"low\" )) / 2 ) data = data . withColumn ( \"kijun_sen\" , ( data . selectExpr ( \"avg(high) OVER (ORDER BY date ROWS 26 PRECEDING) as avg_high\" , \"avg(low) OVER (ORDER BY date ROWS 26 PRECEDING) as avg_low\" ) . selectExpr ( \"(avg_high + avg_low) / 2 as kijun_sen\" ))) data = data . withColumn ( \"senkou_span_a\" , ( data . tenkan_sen + data . kijun_sen ) / 2 ) data = data . withColumn ( \"senkou_span_b\" , ( data . selectExpr ( \"avg(high) OVER (ORDER BY date ROWS 52 PRECEDING) as avg_high\" , \"avg(low) OVER (ORDER BY date ROWS 52 PRECEDING) as avg_low\" ) . selectExpr ( \"(avg_high + avg_low) / 2 as senkou_span_b\" ))) # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific calculations for the Ichimoku Cloud indicator. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. Add volume and volatility as additional features to Linear Regression \u00b6 You can add volume and volatility as additional features in the linear regression model by following these steps: Calculate the volatility: You can calculate volatility using standard deviation or other volatility indicators such as the average true range (ATR) and add it as a new column in the data. Add the volume data: You can add the volume data as a new column in the data. Create the feature vector: You would need to update the feature vector to include the new columns for volatility and volume. Train the linear regression model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data, which now includes the volatility and volume as additional features. Evaluate the model: You can evaluate the performance of the model by applying it to the test data and comparing the predicted option prices to the actual option prices. Here's an example of how you could add volume and volatility as additional features in the linear regression model: Python from pyspark.sql.functions import col , stddev # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Calculate the volatility data = data . withColumn ( \"volatility\" , stddev ( col ( \"stock_price\" ))) # Add the volume data data = data . withColumn ( \"volume\" , col ( \"volume\" )) # Calculate the Ichimoku Cloud indicator #... # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) It's important to keep in mind that adding volume and volatility as additional features in the linear regression model may not necessarily improve the performance of the model, and it's important to validate and backtest the model before deploying it in live trading environment. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to consider whether volume and volatility are useful features for your specific problem and whether they are available for the options data you are working with. It's possible that the addition of these features might not bring any value to the model. Linear Regression using volatility from ichimoku cloud \u00b6 You can use the volatility calculated from the Ichimoku Cloud to generate a linear regression model in PySpark as follow: Calculate volatility from Ichimoku Cloud: You would need to calculate volatility from the Ichimoku Cloud, for example, by calculating the standard deviation of the difference between the stock price and the Senkou Span B line. Add the volatility as a feature: You would need to add the calculated volatility as a new column in the data. Create the feature vector: You would need to update the feature vector to include the new column for volatility. Train the linear regression model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data, which now includes the volatility as an additional feature. Evaluate the model: You can evaluate the performance of the model by applying it to the test data and comparing the predicted option prices to the actual option prices. Here's an example of how you could use the volatility calculated from the Ichimoku Cloud as an additional feature in the linear regression model: Python from pyspark.sql.functions import col , stddev # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Calculate the volatility from Ichimoku Cloud data = data . withColumn ( \"volatility\" , stddev ( col ( \"stock_price\" ) - col ( \"senkou_span_b\" ))) # Calculate the Ichimoku Cloud indicator #... # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific calculations for the Ichimoku Cloud indicator and volatility. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to consider whether volatility calculated from the Ichimoku Cloud is a useful feature for your specific problem. You can try different ways of calculating volatility and compare the performance of the model using different volatility measures to find the best one for your problem. What are other most popular ML model for prediction over time series financial data \u00b6 There are several machine learning models that are commonly used for prediction over time series financial data. Some of the most popular models include: ARIMA: AutoRegressive Integrated Moving Average is a popular time series forecasting model that is based on the idea of using a linear combination of past observations to predict future values. LSTM: Long Short-Term Memory is a type of Recurrent Neural Network (RNN) that is capable of handling sequential data and is commonly used for time series forecasting. Random Forest: Random Forest is an ensemble of decision trees that can be used for both classification and regression tasks. It can be used to model time series data by considering each time step as a separate observation. Gradient Boosting: Gradient Boosting is an ensemble of decision trees that can be used for both classification and regression tasks. It can be used to model time series data by considering each time step as a separate observation XGBoost: A Gradient Boosting model with more advanced features such as regularization and handling missing values. Prophet: A procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. These models can be used to predict various financial time series such as stock prices, exchange rates, and commodity prices. However, it's important to note that time series data can be complex and often requires feature engineering and preprocessing to obtain a good model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. Create pyspark code for predicting option price as previously done using ARIMA model rather than Linear regression \u00b6 Example of how you could use the ARIMA model to predict option prices in PySpark: Python from pyspark.sql.functions import col from pyspark.ml.regression import ARIMARegressor from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"date\" , \"option_price\" ) # Convert the date column to timestamp data = data . withColumn ( \"date\" , col ( \"date\" ) . cast ( \"timestamp\" )) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the ARIMA model arima = ARIMARegressor ( p = 1 , d = 1 , q = 1 ) arima_model = arima . fit ( train_data ) # Make predictions on the test data predictions = arima_model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) This example is using the default parameters for the ARIMA model, but it's important to note that finding the optimal parameters for the model is a time-consuming task and often requires trial and error. Also, you may need to preprocess the data further, such as handling missing values or transforming the data to a stationary series. Additionally, it's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific calculations for the model. Additionally, it's important to validate and backtest the model before deploying it in live trading Using Random Forest \u00b6 Python from pyspark.ml.regression import RandomForestRegressor from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" , \"option_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the Random Forest Regression model rf = RandomForestRegressor () rf_model = rf . fit ( train_data ) # Make predictions on the test data predictions = rf_model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the Random Forest Regression model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. You could also try different parameters for the Random Forest Regressor, such as the number of trees and the maximum depth of the trees, and test their impact on the model's performance. Using Gradient Boosting \u00b6 An example of how you could use the Gradient Boosting Regression model to predict option prices in PySpark: Python from pyspark.ml.regression import GBTRegressor from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" , \"option_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the Gradient Boosting Regression model gbt = GBTRegressor () gbt_model = gbt . fit ( train_data ) # Make predictions on the test data predictions = gbt_model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the Gradient Boosting Regression model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. You could also try different parameters for the Gradient Boosting Regressor, such as the number of trees and the maximum depth of the trees, and test their impact on the model's performance. It's also important to consider if Gradient Boosting is appropriate for your specific problem, as it can be sensitive to noise and outliers in the data. Using XGBoost \u00b6 XGBoost is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be highly efficient and scalable and is widely used for machine learning problems such as regression and classification. To use XGBoost in PySpark, you would need to install the XGBoost4J-Spark library which is a library that allows XGBoost to be integrated with PySpark. Here is an example of how you could use the XGBoost Regression model to predict option prices in PySpark: Python from pyspark.ml import Pipeline from pyspark.ml.regression import XGBoostRegressor from pyspark.ml.feature import VectorAssembler from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" , \"option_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the XGBoost Regression model xgb = XGBoostRegressor ( numRound = 10 , maxDepth = 3 , objective = 'reg:squarederror' ) # Set up the pipeline pipeline = Pipeline ( stages = [ assembler , xgb ]) # Train the model model = pipeline . fit ( train_data ) # Make predictions on the test data predictions = model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) t's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the XGBoost Regression model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. You could also try different parameters for the XGBoost Regressor, such as the number of trees and the maximum depth of the trees, and test their impact on the model's performance. XGBoost is a powerful algorithm and it's important to tune the parameters for better performance. Using Prophet \u00b6 Prophet is a time series forecasting library developed by Facebook that is based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It's not implemented in PySpark but it can be used in Python, so to use it you will have to move your data to a pandas dataframe. Here is an example of how you could use the Prophet model to predict option prices in Python: Python import pandas as pd from fbprophet import Prophet # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # convert spark dataframe to pandas data = data . toPandas () # Select the relevant columns for the model data = data [[ \"date\" , \"option_price\" ]] data = data . rename ( columns = { \"date\" : \"ds\" , \"option_price\" : \"y\" }) # split data into train and test train = data . iloc [: int ( 0.8 * len ( data ))] test = data . iloc [ int ( 0.8 * len ( data )):] # fit the model m = Prophet () m . fit ( train ) # make predictions future = m . make_future_dataframe ( periods = len ( test )) forecast = m . predict ( future ) # evaluate the model predictions = forecast [[ 'ds' , 'yhat' ]] . tail ( len ( test )) predictions . columns = [ 'date' , 'prediction' ] predictions = predictions . join ( test . set_index ( 'ds' ), on = 'date' ) rmse = (( predictions [ 'y' ] - predictions [ 'prediction' ]) ** 2 ) . mean () ** .5 print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the Prophet model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that Prophet needs the date column to be in a specific format and named as \"ds\" and the target column as \"y\". Prophet is a very useful library for time series forecasting and it can handle missing values and outliers automatically which makes it a great choice for non-experts. Adapt linear regression code for call put options \u00b6 Adapting linear regression code for call put options would involve using different features and/or target variables specific to call put options. Call options give the holder the right to buy an underlying asset at a specified price (strike price), while put options give the holder the right to sell an underlying asset at a specified price. Here is an example of how you could adapt linear regression code for call put options in PySpark: Python from pyspark.ml.regression import LinearRegression from pyspark.ml.feature import VectorAssembler from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" , \"call_price\" , \"put_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the feature vector feature_cols = [ \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the Linear Regression model for call options lr_call = LinearRegression ( labelCol = \"call_price\" ) # Create the Linear Regression model for put options lr_put = LinearRegression ( labelCol = \"put_price\" ) # Set up the pipeline pipeline_call = Pipeline ( stages = [ assembler , lr_call ]) pipeline_put = Pipeline ( stages = [ assembler , lr_put ]) # Train the model for call options model_call = pipeline_call . fit ( train_data ) # Train the model for put options model_put = pipeline_put . fit ( train_data ) # Make predictions on the test data for call options predictions_call = model_call . transform ( test_data ) # Make predictions on the test data for put options predictions_put = model_put . transform ( test_data ) # Evaluate the model for call options evaluator_call = RegressionEvaluator ( labelCol = \"call_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse_call = evaluator_call . evaluate ( predictions_call ) print ( \"Root Mean Squared Error (RMSE) for call options on test data = %g \" % rmse_call ) # Evaluate the model for put options evaluator_put = RegressionEvaluator ( labelCol = \"put_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse_put = evaluator_put . evaluate ( predictions_put ) print ( \"Root Mean Squared Error (RMSE) for put options on test data = %g \" % rmse_put ) This is a simple example of how you could adapt linear regression code for call put options. In this example, we are using different feature columns and target columns for call and put options. Additionally, we are fitting and evaluating separate models for call and put options. You could also experiment with different feature combinations, regularization techniques, and evaluation metrics to improve the performance of the model. It's also important to validate and backtest the model before deploying it in live trading environment and consider using different models and comparing their performance using different evaluation metrics to choose the best one. Incorporate news feed to the model \u00b6 Incorporating news feed into the model would involve gathering news data related to the underlying asset and using it as an additional feature in the model. There are several ways to gather news data, such as using web scraping or an API from a news source. Here is an example of how you could incorporate news feed into the model in PySpark: Python from pyspark.sql.functions import udf from pyspark.ml.feature import HashingTF , IDF , Tokenizer # Read the news data news_data = spark . read . format ( \"csv\" ) . load ( \"/path/to/news_data\" ) # Perform preprocessing on the news data # Tokenize the text tokenizer = Tokenizer ( inputCol = \"text\" , outputCol = \"words\" ) news_data = tokenizer . transform ( news_data ) # Convert the words to a numerical feature vector hashing_tf = HashingTF ( inputCol = \"words\" , outputCol = \"rawFeatures\" ) news_data = hashing_tf . transform ( news_data ) # Scale the feature vector idf = IDF ( inputCol = \"rawFeatures\" , outputCol = \"news_features\" ) idf_model = idf . fit ( news_data ) news_data = idf_model . transform ( news_data ) # Join the news data with the option data data = data . join ( news_data , data . date == news_data . date ) # Create the feature vector feature_cols = [ \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" , \"news_features\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the Linear Regression model lr = LinearRegression ( labelCol = \"call_price\" or \"put_price\" ) # Set up the pipeline pipeline = Pipeline ( stages = [ assembler , lr ]) # Train the model model = pipeline . fit ( data ) # Make predictions on the test data predictions = model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"call_price\" or \"put_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the preprocessing and feature extraction. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that news data may be noisy, and it's important to preprocess the data and extract relevant features before using it in the model. The example above uses TF-IDF to convert the news text into a numerical feature vector, but you could also use other techniques such as word embeddings, LDA, or sentiment analysis. It's also important to note that this example is using a linear regression model, you could also use other models that are more suited to handle text data such as LSTM or other neural network models. In addition, It's also important to keep in mind that news data is time-sensitive, it would be more meaningful if it's used in a time window that is close to the time of prediction, and also it would be more useful if it's filtered to include only news related to the specific company or sector of the underlying asset. It's also important to note that incorporating news feed into the model may increase the complexity of the model and make it more difficult to interpret and understand. In summary, incorporating news feed into the model can potentially improve the performance of the model by providing additional information about the underlying asset. However, it's important to properly preprocess and extract relevant features from the news data and to validate and backtest the model before deploying it in a live trading environment. Enable GPU during model training \u00b6 Enabling GPU during model training can significantly speed up the training process, especially for large and complex models. Here are the general steps to enable GPU during model training in PySpark: Verify that your system has a CUDA-enabled GPU and the necessary drivers are installed. You can check this by running the command nvidia-smi in the terminal. nstall the latest version of PySpark and the deep learning libraries such as TensorFlow or PyTorch that you plan to use for model training. Configure the Spark environment to use the GPU. You can do this by setting the following environment variables: export SPARK_LOCAL_DIRS=/path/to/gpu_storage export SPARK_WORKER_CORES=1 export SPARK_WORKER_INSTANCES=1 export SPARK_WORKER_MEMORY=2g export SPARK_DRIVER_MEMORY=2g Start Spark with the following command: pyspark --master local[*] --conf spark.driver.memory=2g --conf spark.executor.memory=2g --conf spark.executorEnv.JAVA_HOME=/path/to/jdk In the PySpark script, set the TensorFlow or PyTorch session to use the GPU. For example, for TensorFlow you can use the following code: Python import tensorflow as tf tf . config . list_physical_devices ( 'GPU' ) tf . config . experimental . set_memory_growth ( tf . config . list_physical_devices ( 'GPU' )[ 0 ], True ) train your model using the GPU Python with tf . device ( '/gpu:0' ): model . fit ( train_data ) It's important to note that these are general steps, and the specific details may vary depending on the system and libraries you are using. Additionally, it's important to monitor the GPU usage during training to ensure that the GPU is being utilized effectively. It's also important to note that running deep learning models on a GPU requires a large amount of memory, so it's important to have a GPU with enough memory to handle the size of your model. It's also important to note that not all models can run on a GPU, so it's important to check the compatibility of your model with GPU before trying to run it on GPU. Linear regression for multiple symbols using pyspark \u00b6 Approaching linear regression for multiple symbols using PySpark would involve creating separate models for each symbol and then training and evaluating them independently. Here is an example of how you could approach linear regression for multiple symbols using PySpark: Python from pyspark.sql.functions import udf from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator # Create a function to filter the data for a specific symbol def filter_data ( symbol ): data = option_data . filter ( option_data . symbol == symbol ) return data # Define the feature columns feature_cols = [ \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" ] # Create the assembler for the feature vector assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the Linear Regression model lr = LinearRegression ( labelCol = \"call_price\" or \"put_price\" ) # Set up the pipeline pipeline = Pipeline ( stages = [ assembler , lr ]) # Create a list of symbols symbols = [ \"AAPL\" , \"GOOG\" , \"MSFT\" , \"AMZN\" ] # Create a dictionary to store the models models = {} # Train and evaluate a model for each symbol for symbol in symbols : # Filter the data for the symbol symbol_data = filter_data ( symbol ) train_data , test_data = symbol_data . randomSplit ([ 0.8 , 0.2 ]) # Train the model model = pipeline . fit ( train_data ) # Make predictions on the test data predictions = model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"call_price\" or \"put_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) for symbol {} on test data = {} \" . format ( symbol , rmse )) # Add the model to the dictionary models [ symbol ] = model In this example, we are filtering the option data for each symbol, creating a separate model for each symbol, training and evaluating them independently, and storing them in a dictionary for later use. This approach allows you to train and evaluate models for multiple symbols independently, which can be useful if the symbols have different characteristics or if you want to compare the performance of the models for different symbols. It's important to note that this is just an example, you would need to customize it to suit your specific requirements, such as the specific columns in your data, the specific parameters for the preprocessing and feature extraction and the symbols. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that this example uses a simple linear regression model, but you could also use other models that are more suited to handle time-series data such as ARIMA or other ML models like Random Forest, XGBoost, Gradient Boosting etc. Also, it's important to note that this example trains and evaluates the model separately for each symbol, but you could also consider using a more sophisticated approach like k-fold cross-validation to get more robust evaluation metric. In summary, approaching linear regression for multiple symbols using PySpark involves creating separate models for each symbol and then training and evaluating them independently. This allows you to train and evaluate models for multiple symbols independently, and also allows you to compare the performance of the models for different symbols. Generate buy sell signal for stocks using previous ichimoku cloud for a large number of symbols efficiently \u00b6 Generating buy and sell signals for stocks using the Ichimoku Cloud for a large number of symbols efficiently would involve creating a script that can automate the process of calculating the Ichimoku Cloud indicators and generating the buy and sell signals for each symbol. Here is an example of how you could generate buy and sell signals for stocks using the Ichimoku Cloud for a large number of symbols efficiently in PySpark: Python from pyspark.sql.functions import udf # Define a function to calculate the Ichimoku Cloud indicators def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators # Define a function to generate the buy and sell signals def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType ())) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are defining a function to calculate the Ichimoku Cloud indicators and a function to generate the buy and sell signals for each symbol. Then we are creating a UDF to apply the Ichimoku Cloud indicators calculation to the stock data and another UDF to apply the signals generation to the stock data. Then we are grouping the data by symbol and getting the last signal. It's important to note that this is just an example, you would need to customize it to suit your specific requirements, such as the specific columns in your data, the specific parameters for the Ichimoku Cloud indicators calculation, and the specific conditions for generating the buy and sell signals. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to note that this example uses a simple buy/sell signal generation based on the Ichimoku Cloud indicators, but you could also use other indicators or technical analysis tools to generate the signals and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that this example uses close prices for the calculation of Ichimoku Cloud indicators, but you could also use other prices such as high, low, open etc. In summary, generating buy and sell signals for stocks using the Ichimoku Cloud for a large number of symbols efficiently would involve creating a script that can automate the process of calculating the Ichimoku Cloud indicators and generating the buy and sell signals for each symbol using PySpark, and then grouping the data by symbol to get the last signal. More efficient version using numba \u00b6 Using Numba can help make the script more efficient by compiling the Python code into machine code, which can improve performance, especially for computationally intensive tasks such as calculating the Ichimoku Cloud indicators and generating the buy and sell signals. Here is an example of how you could make the script more efficient using Numba: Python from numba import jit import numpy as np @jit def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators @jit def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType ())) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are using the jit decorator from Numba to compile the ichimoku_cloud and generate_signals functions into machine code, which can significantly improve performance. It's important to note that Numba works best when the code is simple and the data is homogeneous, and it's not recommended to use it on more complex structures such as Pandas DataFrame. In summary, using Numba can help make the script more efficient by compiling the Python code into machine code, which can improve performance, especially for computationally intensive tasks such as calculating the Ichimoku Cloud indicators and generating the buy and sell signals for large number of symbols in PySpark. Complete implementation with stock data \u00b6 Read symbols from a Delta lake table, with columns event_time, close, high, low, trade_count, open, t, volume, vwap, date, timestamp, symbol, event_dt, and integrate it into the previous solution with Numba: Python from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession . builder . appName ( \"Ichimoku Cloud\" ) . getOrCreate () # Read symbols from a Delta lake table stock_data = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/lake/table\" ) # Keep only the columns we need stock_data = stock_data . select ( \"symbol\" , \"event_time\" , \"close\" , \"high\" , \"low\" , \"trade_count\" , \"open\" , \"t\" , \"volume\" , \"vwap\" , \"date\" , \"timestamp\" , \"event_dt\" ) @jit def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators @jit def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType ())) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are reading symbols from a delta lake table with the specified columns and integrating it into the previous solution with Numba. We are using the SparkSession.read.format(\"delta\").load() method to read the data from the Delta lake table, and selecting only the columns we need for our analysis. We are then applying the ichimoku_cloud and generate_signals functions, which were previously decorated with the jit decorator from Numba, to the data. Finally, we are grouping the data by symbol and getting the last signal. It's important to note that this is a simplified example and you may need to adjust it to suit your specific requirements, such as handling missing data or adding additional data pre-processing steps. In summary, reading symbols from a Delta lake table with specified columns and integrating it into the previous solution with Numba is a matter of reading the data using the appropriate method and integrating the existing solution with it. Handling missing data \u00b6 Handling missing data is an important step in any data analysis or modeling task, as missing data can lead to inaccurate or unreliable results. There are several ways to handle missing data, depending on the specific situation and the amount of missing data. Some common methods are: Deletion: This method involves deleting the rows or columns that contain missing data. This method is simple and easy to implement, but it can result in a loss of information and a decrease in the sample size. It is generally not recommended if the amount of missing data is large. Imputation: This method involves replacing the missing values with estimates based on the available data. Common imputation methods include mean, median, and mode imputation. This method is more sophisticated than deletion, but it can still introduce bias if the missing data is not missing at random. Interpolation: This method involves estimating missing values based on the values of other observations. Common interpolation methods include linear interpolation and spline interpolation. This method is more sophisticated than imputation, but it can still introduce bias if the missing data is not missing at random. Multiple imputation: This method involves generating multiple imputed datasets, each with different imputed values for the missing data. Then, statistical analysis is performed on each dataset and the results are combined to produce a final estimate. Predictive models: This method involves building a model to predict the missing values based on the available data. This method is more sophisticated than other methods, but it can be more accurate and less biased if the missing data is not missing at random. Leave it as it is: If the missing data is not significant you can just leave it as it is. It's important to note that the best method for handling missing data will depend on the specific situation and the amount of missing data, and it's always good to use multiple techniques and compare the results. In summary, handling missing data is an important step in any data analysis or modeling task. There are several ways to handle missing data, depending on the specific situation and the amount of missing data, including deletion, imputation, interpolation, multiple imputation, and predictive models. It's important to select the appropriate method and to use multiple techniques and compare the results. Add interpolation for missing data \u00b6 Add an interpolation method for handling missing data to the previous example: Python from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession . builder . appName ( \"Ichimoku Cloud\" ) . getOrCreate () # Read symbols from a Delta lake table stock_data = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/lake/table\" ) # Keep only the columns we need stock_data = stock_data . select ( \"symbol\" , \"event_time\" , \"close\" , \"high\" , \"low\" , \"trade_count\" , \"open\" , \"t\" , \"volume\" , \"vwap\" , \"date\" , \"timestamp\" , \"event_dt\" ) # Interpolate missing values stock_data = stock_data . na . fill ( method = 'linear' ) @jit def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators @jit def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType )) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are using the na.fill() method to interpolate the missing values in the stock_data DataFrame. We pass the method='linear' argument to interpolate the missing values using linear interpolation. You can use other interpolation methods as well, such as 'spline' or 'pchip'. It's important to note that interpolation can be a good method for handling missing data if the data is missing at random and it's not a big portion of data. In summary, adding interpolation method for handling missing data to the previous example is a matter of using the appropriate method to interpolate the missing values and integrating it with the existing solution. It's important to note that interpolation can be a good method for handling missing data if the data is missing at random and it's not a big portion of data. However if the missing data is not missing at random, then interpolation can introduce bias to the analysis, it's always good to use multiple techniques and compare the results. Plot is using ploty \u00b6 Plot the stock data and signals using Plotly in Python: Python import plotly.express as px # Create a scatter plot of the stock data fig = px . scatter ( stock_data , x = \"event_time\" , y = \"close\" , color = \"signals\" , title = \"Stock Data and Signals\" ) # Add a line for the Tenkan Sen indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"tenkan_sen\" ], name = \"Tenkan Sen\" , line = dict ( color = 'firebrick' , width = 2 ))) # Add a line for the Kijun Sen indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"kijun_sen\" ], name = \"Kijun Sen\" , line = dict ( color = 'darkblue' , width = 2 ))) # Add a line for the Senkou Span A indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"senkou_span_a\" ], name = \"Senkou Span A\" , line = dict ( color = 'green' , width = 2 ))) # Add a line for the Senkou Span B indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"senkou_span_b\" ], name = \"Senkou Span B\" , line = dict ( color = 'purple' , width = 2 ))) # Show the plot fig . show () In this example, we are using the plotly.express library to create a scatter plot of the stock data using the px.scatter() function. We are passing the stock_data DataFrame, the x-axis data (event_time), the y-axis data (close), and the color data (signals) to the function. We are also setting the title of the plot to \"Stock Data and Signals\". After that, we are adding traces for the Tenkan Sen, Kijun Sen, Senkou Span A, and Senkou Span B indicators using the fig.add_trace() function. We are passing the x-axis data (event_time) and y-axis data (tenkan_sen, kijun_sen, senkou_span_a, senkou_span_b) for each indicator, and setting the name and color of the line. Finally, we are using the fig.show() function to display the plot. It's important to note that this is a simplified example and you may need to adjust it to suit your specific requirements, such as customizing the plot's appearance or adding additional data to the plot. In summary, plotting the stock data and signals using Plotly in Python is a matter of using the appropriate plotly functions to create the plot, set the data and appearance, and displaying the plot. Plot grouped_data using plotly \u00b6 Python import plotly.graph_objs as go #create a list of symbols and signals symbols = grouped_data . select ( \"symbol\" ) . rdd . flatMap ( lambda x : x ) . collect () signals = grouped_data . select ( \"signals\" ) . rdd . flatMap ( lambda x : x ) . collect () # Create a scatter plot of the grouped data fig = go . Figure ( data = [ go . Scatter ( x = symbols , y = signals , mode = 'markers' )]) # Set the title of the plot fig . update_layout ( title = 'Symbols and Signals' ) # Set the y axis title fig . update_yaxes ( title_text = 'Signals' ) # Set the x axis title fig . update_xaxes ( title_text = 'Symbols' ) # Show the plot fig . show () In this example, We are creating a list of symbols and signals from grouped_data DataFrame by using select() and rdd.flatMap(lambda x: x).collect() methods. Then, we are using the go.Scatter() function from the plotly.graph_objs library to create a scatter plot of the grouped data. We are passing the symbols and signals as the x and y data respectively. After that, we are using the update_layout(), update_yaxes(), and update_xaxes() methods to set the title of the plot, and the y and x axis titles. Finally, we are using the fig.show() function to display the plot. This is a simplified example, you may need to adjust it to suit your specific requirements, such as customizing the plot's appearance or adding additional data to the plot. In summary, plotting the grouped_data using Plotly in Python is a matter of creating a list of symbols and signals, using the appropriate plotly functions to create the plot, set the data and appearance, and displaying the plot. Why pyspark has following error while trying to display grouped_data.show(10) \u00b6 File \"/usr/bin/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads return cloudpickle.loads(obj, encoding=encoding) ModuleNotFoundError: No module named 'numba' The error \"ModuleNotFoundError: No module named 'numba'\" suggests that the PySpark environment doesn't have the Numba library installed. When you run your script in PySpark, it uses the Python environment that is installed on the cluster. If the cluster does not have the Numba library installed, PySpark will not be able to find it and will throw the error. There are a few ways to address this issue: You can install the Numba library on the cluster where you are running PySpark. You can do this by using pip or conda. You can bundle the Numba library with your script, so that it gets packaged with the script and is available to PySpark when it runs. You can use the --py-files option when running your script to do this. Alternatively, you can use a different approach to handle the missing data, or use a different library that is already available in the cluster. It's important to note that if you are running the script in a local environment, you should have the library installed in your machine. why pyspark has following error while trying to display group_data.show(10) \u00b6 return f(*args, **kwargs) File \"/usr/bin/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 93, in return lambda *a: g(f(*a)) AttributeError: 'float' object has no attribute 'rolling' The error \"attributeError: 'float' object has no attribute 'rolling'\" suggests that you are trying to use the rolling() method on a float object, which does not have this method. The rolling() method is available on pandas DataFrame and Series objects, but not on a single float value. This error is likely caused by a problem with the data in the grouped_data DataFrame. The ichimoku_cloud function expects the input data to be a pandas DataFrame or Series object, but the input data passed to it is a single float value. To fix this issue, you should check the data in the grouped_data DataFrame and make sure it is in the correct format. You can check the data type of the column that you are passing to the ichimoku_cloud function by using the dtypes property of the DataFrame. grouped_data.dtypes In addition, you should check that the column you are passing to the function is not empty and it is not missing any data. grouped_data.select(\"column_name\").na.drop().count() If the column is of wrong data type you should cast it to the correct type. grouped_data = grouped_data.withColumn(\"column_name\", grouped_data[\"column_name\"].cast(DoubleType())) In summary, the error message is likely caused by a problem with the data in the grouped_data DataFrame, specifically the data passed to the ichimoku_cloud function is not in the correct format. You should check the data in the DataFrame and make sure it is in the correct format, and also check that the column you are passing to the function is not empty and it is not missing any data.","title":"Design/Build trade bot"},{"location":"synapse/build-trade-bot/#designbuild-trade-bot","text":"Designing a stock and option trade bot that can automate downloading tickers every five minutes, store them in Delta Lake using PySpark, and run various strategies to generate buy and sell signals can be a complex process. Here are the general steps you could take to accomplish this: Gather the necessary data: You will need to acquire historical ticker data for the stocks and options you want to trade. This data can be obtained from financial data providers such as Yahoo Finance or Alpha Vantage. Store the data: Once you have the data, you can store it in Delta Lake. PySpark is a good choice for this because it allows you to process large amounts of data quickly and easily. Develop the trading strategies: You will need to develop trading strategies that use the data to generate buy and sell signals. There are many different strategies that can be used, such as moving averages, momentum indicators, and technical analysis. Implement the bot: You will need to write code to automate the process of downloading the data, storing it in Delta Lake, and running the trading strategies. Test the bot: Before deploying the bot, you will need to test it thoroughly to ensure that it is working correctly and that the strategies are generating accurate buy and sell signals. Deploy the bot: Once you are satisfied that the bot is working correctly, you can deploy it to a trading platform so that it can execute trades automatically. Please note that this is a high-level overview of the process and each step may require significant effort and knowledge. It's important to have a good understanding of trading and market data, as well as experience working with programming languages and big data technologies before attempting to build a trading bot like this.","title":"Design/Build trade bot"},{"location":"synapse/build-trade-bot/#create-delta-lake-schema-to-record-stock-buy-and-sell","text":"JSON { \"type\" : \"record\" , \"name\" : \"stock_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } The above schema defines a record for a stock transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, number of shares, and price per share. This schema can be used with Delta Lake to record stock buy and sell transactions.","title":"Create delta lake schema to record stock buy and sell"},{"location":"synapse/build-trade-bot/#also-define-schema-to-store-buy-sell-of-options","text":"JSON { \"type\" : \"record\" , \"name\" : \"options_transaction\" , \"fields\" : [ { \"name\" : \"transaction_id\" , \"type\" : \"string\" }, { \"name\" : \"timestamp\" , \"type\" : \"string\" }, { \"name\" : \"transaction_type\" , \"type\" : \"string\" }, { \"name\" : \"stock_symbol\" , \"type\" : \"string\" }, { \"name\" : \"option_type\" , \"type\" : \"string\" }, { \"name\" : \"strike_price\" , \"type\" : \"double\" }, { \"name\" : \"expiration_date\" , \"type\" : \"string\" }, { \"name\" : \"shares\" , \"type\" : \"int\" }, { \"name\" : \"price\" , \"type\" : \"double\" } ] } This schema defines a record for an options transaction, which includes the transaction ID, timestamp, transaction type (buy or sell), stock symbol, option type (call or put), strike price, expiration date, number of shares, and price per share. This schema can be used with Delta Lake to record buy and sell transactions for options.","title":"Also define schema to store buy sell of options"},{"location":"synapse/build-trade-bot/#create-pyspark-schema-from-previous-schemas-and-provide-code-to-read-write-and-delete-records","text":"Here is the code to create PySpark schemas from the previous JSON schemas, and to read, write, and delete records using the PySpark Delta API: Python from pyspark.sql.types import * # Create PySpark schema for stock transactions stock_schema = StructType . fromJson ( json . loads ( \"\"\"{ \"type\": \"record\", \"name\": \"stock_transaction\", \"fields\": [ {\"name\": \"transaction_id\", \"type\": \"string\"}, {\"name\": \"timestamp\", \"type\": \"string\"}, {\"name\": \"transaction_type\", \"type\": \"string\"}, {\"name\": \"stock_symbol\", \"type\": \"string\"}, {\"name\": \"shares\", \"type\": \"int\"}, {\"name\": \"price\", \"type\": \"double\"} ] }\"\"\" )) # Create PySpark schema for options transactions options_schema = StructType . fromJson ( json . loads ( \"\"\"{ \"type\": \"record\", \"name\": \"options_transaction\", \"fields\": [ {\"name\": \"transaction_id\", \"type\": \"string\"}, {\"name\": \"timestamp\", \"type\": \"string\"}, {\"name\": \"transaction_type\", \"type\": \"string\"}, {\"name\": \"stock_symbol\", \"type\": \"string\"}, {\"name\": \"option_type\", \"type\": \"string\"}, {\"name\": \"strike_price\", \"type\": \"double\"}, {\"name\": \"expiration_date\", \"type\": \"string\"}, {\"name\": \"shares\", \"type\": \"int\"}, {\"name\": \"price\", \"type\": \"double\"} ] }\"\"\" )) # Create a Delta Lake table for stock transactions ( spark . read . format ( \"json\" ) . schema ( stock_schema ) . load ( \"path/to/stock_transactions\" ) . write . format ( \"delta\" ) . save ( \"path/to/stock_transactions_table\" )) # Create a Delta Lake table for options transactions ( spark . read . format ( \"json\" ) . schema ( options_schema ) . load ( \"path/to/options_transactions\" ) . write . format ( \"delta\" ) . save ( \"path/to/options_transactions_table\" )) # Read records from stock transactions table stock_transactions_df = spark . read . format ( \"delta\" ) . load ( \"path/to/stock_transactions_table\" ) # Read records from options transactions table options_transactions_df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_transactions_table\" ) # Write new records to stock transactions table new_stock_transactions_df = spark . createDataFrame ([ ( \"1\" , \"2022-01-01\" , \"buy\" , \"AAPL\" , 100 , 200.0 ), ( \"2\" , \"2022-01-02\" , \"sell\" , \"GOOG\" , 50 , 1500.0 ) ], stock_schema ) new_stock_transactions_df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/stock_transactions_table\" ) # Delete specific records from options transactions table options_","title":"Create pyspark schema from previous schemas and provide code to read write and delete records"},{"location":"synapse/build-trade-bot/#or-create-pyspark-schema-from-previous-schemas-and-provide-code-to-read-write-and-delete-records","text":"Here is the PySpark code to create the schema for the stock_transaction record: Python from pyspark.sql.types import * stock_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) And here is the PySpark code to create the schema for the options_transaction record: Python options_transaction_schema = StructType ([ StructField ( \"transaction_id\" , StringType ()), StructField ( \"timestamp\" , StringType ()), StructField ( \"transaction_type\" , StringType ()), StructField ( \"stock_symbol\" , StringType ()), StructField ( \"option_type\" , StringType ()), StructField ( \"strike_price\" , DoubleType ()), StructField ( \"expiration_date\" , StringType ()), StructField ( \"shares\" , IntegerType ()), StructField ( \"price\" , DoubleType ()) ]) To read a Delta Lake table into a PySpark DataFrame, you can use the read method: df = spark.read.format(\"delta\").load(\"path/to/delta/table\") To write a DataFrame to a Delta Lake table, you can use the write method: df.write.format(\"delta\").save(\"path/to/delta/table\") To delete a Delta Lake table, you can use the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table \") It is important to note that the above code will delete the entire table. If you would like to delete specific records, you can use a filter in the delete method: spark.sql(\"DELETE FROM delta. path/to/delta/table WHERE stock_symbol='AAPL'\")","title":"OR Create pyspark schema from previous schemas and provide code to read write and delete records"},{"location":"synapse/build-trade-bot/#generate-fast-api-apis-to-interact-with-add-update-delete-process-from-pyspark","text":"Here is an example of how you could use the FastAPI library to create a REST API that interacts with a Delta Lake table using PySpark: Python from fastapi import FastAPI , HTTPException from pyspark.sql import SparkSession app = FastAPI () # Create a SparkSession spark = SparkSession . builder . appName ( \"DeltaLakeAPI\" ) . getOrCreate () @app . post ( \"/stock\" ) async def add_stock_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], stock_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/stock/ {transaction_id} \" ) async def update_stock_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/stock/ {transaction_id} \" ) async def delete_stock_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } This is an example to handle the stock transactions. You can use the same approach to handle option transaction as well. In the above example, the add_stock_transaction endpoint accepts a JSON object containing the transaction data, creates a DataFrame from that data, and writes it to the Delta Lake table. The update_stock_transaction endpoint accepts a transaction ID and a JSON object containing the updated transaction data, reads the Delta Lake table into a DataFrame, filters the DataFrame for the specified transaction, updates the values in the DataFrame with the incoming data, and overwrites the Delta Lake table with the updated DataFrame. The delete_stock_transaction endpoint accepts a transaction ID and deletes the specified transaction from the Delta Lake table. It is important to note that the above code is an example and may require additional error handling and validation. Also, you should update the path of delta table in the code to use your own delta table.","title":"Generate fast-api apis to interact with add, update delete process from pyspark"},{"location":"synapse/build-trade-bot/#generate-similar-api-for-options-schema","text":"Here's an example of how you could use the same approach to create REST APIs for the options_transaction schema: Python @app . post ( \"/options\" ) async def add_options_transaction ( transaction : dict ): # Create a DataFrame from the incoming transaction data df = spark . createDataFrame ([ transaction ], options_transaction_schema ) # Write the DataFrame to the Delta Lake table df . write . format ( \"delta\" ) . mode ( \"append\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction added successfully\" } @app . put ( \"/options/ {transaction_id} \" ) async def update_options_transaction ( transaction_id : str , transaction : dict ): # Read the Delta Lake table into a DataFrame df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) # Filter the DataFrame for the specified transaction df = df . filter ( df [ \"transaction_id\" ] == transaction_id ) # Update the values in the DataFrame with the incoming data for key , value in transaction . items (): df = df . withColumn ( key , value ) # Overwrite the Delta Lake table with the updated DataFrame df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"path/to/options_delta/table\" ) return { \"message\" : \"Transaction updated successfully\" } @app . delete ( \"/options/ {transaction_id} \" ) async def delete_options_transaction ( transaction_id : str ): # Delete the specified transaction from the Delta Lake table spark . sql ( f \"DELETE FROM delta.`path/to/options_delta/table` WHERE transaction_id=' { transaction_id } '\" ) return { \"message\" : \"Transaction deleted successfully\" } The above code is similar to the previous code for handling stock transactions, but it uses the options_transaction schema and different endpoint paths and file paths. As with the previous example, you should update the path of delta table in the code to use your own delta table.","title":"Generate similar api for options schema"},{"location":"synapse/build-trade-bot/#provide-api-for-profit-and-loss-for-both-stocks-and-options","text":"Here's an example of how you could create a REST API endpoint for calculating profit and loss for both stock and options transactions using PySpark: Python @app . get ( \"/profit_loss/ {stock_symbol} \" ) async def get_profit_loss ( stock_symbol : str ): # Read the stock transaction table into a DataFrame stock_df = spark . read . format ( \"delta\" ) . load ( \"path/to/stock_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Read the options transaction table into a DataFrame options_df = spark . read . format ( \"delta\" ) . load ( \"path/to/options_delta/table\" ) . filter ( col ( \"stock_symbol\" ) == stock_symbol ) # Calculate the profit and loss for the stock transactions stock_profit_loss = stock_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) stock_profit_loss = stock_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] # Calculate the profit and loss for the options transactions options_profit_loss = options_df . withColumn ( \"profit_loss\" , ( col ( \"price\" ) - col ( \"cost\" )) * col ( \"shares\" )) options_profit_loss = options_profit_loss . groupBy () . sum ( \"profit_loss\" ) . collect ()[ 0 ][ 0 ] return { \"stock_profit_loss\" : stock_profit_loss , \"options_profit_loss\" : options_profit_loss } In the above code, the get_profit_loss endpoint accepts a stock symbol, reads the stock transaction table and options transaction table into DataFrames, filters the DataFrames for transactions with the specified stock symbol, calculates the profit and loss for the stock and options transactions using the formula (price - cost) * shares, and returns the calculated values in a JSON response. In the above code, cost is not present in the schema, you need to add that accordingly. Also, you should update the path of delta table in the code to use your own delta table. It is important to note that the above code is an example and may require additional error handling and validation.","title":"Provide api for profit and loss for both stocks and options"},{"location":"synapse/build-trade-bot/#theory","text":"","title":"Theory"},{"location":"synapse/build-trade-bot/#what-are-the-best-strategies-for-volatile-market","text":"There are a variety of different strategies that traders can use in volatile markets, depending on their goals, risk tolerance, and overall trading style. Some common strategies used in volatile markets include: Position trading: This strategy involves taking a long-term view of the market and holding positions for weeks or months. It can be useful for traders who are comfortable with larger price swings and are less concerned about short-term volatility. Swing trading: This strategy involves taking positions that last for several days to a few weeks. It can be useful for traders who want to take advantage of short-term market movements, but do not want to be as active as day traders. Day trading: This strategy involves taking positions that are opened and closed within the same trading day. It can be useful for traders who want to take advantage of short-term market movements and can handle the high level of volatility that comes with day trading. Hedging: This strategy involves taking offsetting positions in different markets or instruments in order to reduce risk. For example, a trader might take a long position in a stock while simultaneously taking a short position in a stock index ETF. Option trading: This strategy involves buying or selling options contracts, which give the holder the right, but not the obligation, to buy or sell an underlying asset at a specific price at a specific time. It's important to keep in mind that no single strategy is guaranteed to be profitable in every market condition. It's also important to have a clear understanding of your own risk tolerance, and to be comfortable with the level of volatility that comes with your chosen strategy.","title":"what are the best strategies for volatile market"},{"location":"synapse/build-trade-bot/#explain-more-strategies-for-day-trading","text":"Here are a few additional strategies that day traders may use: Breakout trading: This strategy involves identifying key levels of support and resistance and entering a position when the price breaks through one of these levels. Traders may use technical analysis tools such as trend lines and chart patterns to identify these levels. Scalping: This strategy involves taking a large number of small positions and exiting them quickly, often within a matter of minutes. Scalping can be a high-frequency and high-pressure strategy, as traders look to capture small profits in a short amount of time. News-based trading: This strategy involves monitoring news and economic events and entering positions based on how the market is likely to react. News-based traders often use fundamental analysis tools such as economic indicators and earnings reports to make trading decisions. Mean-reversion: This strategy involves identifying trends and patterns in the market and entering positions with the expectation that the market will revert back to its mean. Mean-reversion traders often use technical analysis tools such as moving averages and Bollinger Bands to identify these trends and patterns. Algorithmic trading: This strategy involves using computer programs to make trading decisions, often based on a set of predefined rules. Algorithmic traders often use a combination of technical and fundamental analysis to identify trends and patterns in the market. It's important to note that every of these strategies have different level of risk and volatility, it's important to have a good understanding of the strategy you choose, and to be comfortable with the level of risk and volatility that comes with it. Moreover, it's important to have a trading plan that includes entry and exit rules, risk management, and position sizing.","title":"Explain more strategies for day trading"},{"location":"synapse/build-trade-bot/#how-to-detect-volatility-for-given-day","text":"There are a few different ways to detect volatility for a given day in the market: Average True Range (ATR): The Average True Range is a technical indicator that measures the volatility of a security over a given period of time. It is calculated by taking the difference between the high and low prices for a given period, and then averaging those values over a specified number of periods. Bollinger Bands: Bollinger Bands are a technical indicator that consist of a moving average and two standard deviation lines that are plotted above and below the moving average. When the bands are close together, it indicates low volatility and when the bands are far apart, it indicates high volatility. Historical Volatility: Historical volatility is a measure of the volatility of a security over a given period of time. It is calculated by taking the standard deviation of the logarithmic returns of a security over a specified number of periods. Implied Volatility: Implied volatility is a measure of the expected volatility of a security based on the prices of options contracts on that security. It can be used to detect potential changes in volatility before they occur. Moving average: A moving average is a commonly used technical indicator that plots the average price of a security over a specified number of periods. When the moving average is trending up or down, it indicates that the security is trending. When the moving average is moving sideways, it indicates that the security is consolidating. VIX: VIX is a measure of volatility known as the \"Fear Index\" which is calculated from the S&P 500 index option prices. It's a widely used indicator to gauge the level of fear and uncertainty in the market and also it can be used as a proxy for volatility in the overall market. It's important to keep in mind that no single indicator can provide a perfect measure of volatility, it's important to use multiple indicators and consider other market data to form a more complete picture of volatility.","title":"how to detect volatility for given day"},{"location":"synapse/build-trade-bot/#can-ichimoku-strategy-used-for-volatility","text":"The Ichimoku Cloud, also known as the Ichimoku Kink\u014d Hy\u014d, is a technical analysis indicator that is commonly used to identify trends, support and resistance levels, and potential buy and sell signals. While it is not specifically designed to measure volatility, it can be used to identify changes in volatility by analyzing the position of the price relative to the cloud. The Ichimoku Cloud is composed of several elements, including the Tenkan-sen (conversion line), the Kijun-sen (base line), and the Senkou Span A and B (leading span A and B). The area between Senkou Span A and B forms the \"cloud\", and the position of the price relative to the cloud can indicate whether the market is in a bullish or bearish trend. When the price is above the cloud, it is considered bullish, and when the price is below the cloud, it is considered bearish. If the price is inside the cloud, it indicates a period of consolidation, which can signal a change in volatility. Additionally, the distance between the Tenkan-sen and the Kijun-sen can also be used to identify changes in volatility. When the distance is wide, it indicates a high level of volatility, and when the distance is narrow, it indicates a low level of volatility. In summary, while Ichimoku strategy is mainly used to identify trends, support and resistance levels, and potential buy and sell signals, it can also be used to identify changes in volatility by analyzing the position of the price relative to the cloud and the distance between the Tenkan-sen and the Kijun-sen. However, it's important to use multiple indicators and consider other market data to form a more complete picture of volatility.","title":"Can ichimoku strategy used for volatility"},{"location":"synapse/build-trade-bot/#what-are-the-best-strategies-for-day-option-trading","text":"Day trading options can be a high-risk, high-reward strategy that can be profitable for experienced traders. Here are a few strategies that day traders may use when trading options: Covered call: This strategy involves holding a long position in an underlying asset and selling call options against that position. The goal of this strategy is to generate income from the option premium while also limiting the potential loss on the underlying asset. Bullish vertical spread: This strategy involves buying a call option at a lower strike price and selling a call option at a higher strike price. The goal of this strategy is to profit from a bullish market move while also limiting the potential loss. Bearish vertical spread: This strategy involves buying a put option at a higher strike price and selling a put option at a lower strike price. The goal of this strategy is to profit from a bearish market move while also limiting the potential loss. Iron condor: This strategy involves selling a call option and a put option at different strike prices to generate income from the option premium. The goal of this strategy is to profit from a neutral market move while also limiting the potential loss. Straddle: This strategy involves buying both a call and a put option at the same strike price, with the goal of profiting from a large move in either direction in the underlying asset. It's important to keep in mind that options trading is a complex and high-risk strategy, and it's important to have a good understanding of the market and to use proper risk management techniques. Additionally, it's important to use multiple indicators and other market data to form a more complete picture of the market. It's also highly recommended to paper trade and backtest any strategy before using it in live trading.","title":"What are the best strategies for day option trading"},{"location":"synapse/build-trade-bot/#how-to-formulate-those-strategies-given-the-stocks-and-option-data-in-delta-lake-and-pyspark","text":"Formulating option trading strategies using stock and option data in Delta Lake and PySpark would involve several steps: Retrieve the data: You would need to read the stock and option data from Delta Lake using PySpark's DataFrame API. You can use the spark.read method to read the data into a DataFrame and then use the select method to select the relevant columns. Clean and transform the data: After retrieving the data, you would need to perform some cleaning and data transformation to prepare the data for analysis. This may include removing any missing or null values, converting data types, and creating new columns or features. Analysis and modeling: Once the data is cleaned and transformed, you can use PySpark's machine learning library, MLlib, to analyze the data and create models. For example, you can use the linear regression model to analyze the relationship between the stock price and the option prices. Backtesting and evaluating the strategies: Once you have formulated your strategies, you can backtest them using historical data to see how they would have performed in the past. You can use the PySpark's DataFrame API to filter and select the historical data that corresponds to the conditions of your strategy. Deployment and execution: Once you have evaluated your strategies and are satisfied with their performance, you can deploy them in a live trading environment and execute them using the APIs provided by your trading platform. It's important to keep in mind that this process is not trivial and it may require a good understanding of stock and options trading, as well as experience with PySpark, Delta Lake, and machine learning. It's also important to validate and backtest the strategies before deploying them in live trading environment","title":"How to formulate those strategies given the stocks and option data in delta lake and pyspark"},{"location":"synapse/build-trade-bot/#provide-linear-regression-model-to-analyze-the-relationship-between-the-stock-and-option-prices","text":"To analyze the relationship between stock and option prices using a linear regression model in PySpark, you would need to perform the following steps: Prepare the data: You would need to retrieve the stock and option data from Delta Lake and clean and transform it to prepare it for analysis. You would need to select the relevant columns such as the stock price, option strike price, expiration date, and option price. Create the feature vector: You would need to create a feature vector containing the relevant columns from the data. The feature vector would typically include the stock price and the option strike price, and potentially other relevant features such as expiration date. Create the label: You would need to create the label, which is the variable that you want to predict. In this case, the label would be the option price. Split the data: You would need to split the data into a training set and a test set. The training set would be used to train the model, and the test set would be used to evaluate the model's performance. Train the model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data. Evaluate the model: Once the model is trained, you can evaluate its performance by applying it to the test data and comparing the predicted option prices to the actual option prices. Python from pyspark.ml.regression import LinearRegression from pyspark.ml.feature import VectorAssembler # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns and create the feature vector feature_cols = [ \"stock_price\" , \"option_strike_price\" , \"expiration_date\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) This is a basic example and you may need to customize it to suit your specific requirements. It's also important to validate and backtest the model before deploying it in live trading environment. Additionally, you may need to consider other factors such as volatility, volume, and other technical indicators to improve the performance of the linear regression model, you could also consider other factors such as volatility, volume, and other technical indicators. Volatility: You can calculate volatility using standard deviation or other volatility indicators such as the average true range (ATR) and add it as a feature in your model. Volume: You can add volume data as a feature in your model and see if it improves the performance of the model. Technical indicators: You can use other technical indicators such as moving averages, relative strength index (RSI), and the Ichimoku Cloud to extract additional information from the stock and option data. It's important to keep in mind that including more features in the model can increase its complexity and may also increase the risk of overfitting. It's important to use techniques such as regularization and feature selection to avoid overfitting. Additionally, you can use various evaluation metrics such as mean squared error, root mean squared error, and R-squared to evaluate the performance of the model and compare it against other models. In summary, linear regression model is a basic model and you may need to consider other factors such as volatility, volume, and other technical indicators to improve the performance of the model. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model.","title":"Provide linear regression model to analyze the relationship between the stock and option prices"},{"location":"synapse/build-trade-bot/#incorporate-ichomoku-cloud-strategy-to-the-linear-regression","text":"You can incorporate the Ichimoku Cloud strategy into a linear regression model by adding the Ichimoku Cloud indicator as a feature Calculate the Ichimoku Cloud: You would need to calculate the Ichimoku Cloud indicator using the historical stock prices. This indicator is calculated using several moving averages and lines, including the Tenkan-sen, Kijun-sen, and Senkou Span A and B. Add the Ichimoku Cloud as a feature: Once you have calculated the Ichimoku Cloud indicator, you can add it as a feature in the linear regression model. You can do this by adding the values of the Tenkan-sen, Kijun-sen, Senkou Span A, and Senkou Span B lines as separate columns in the data. Train the linear regression model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data, which now includes the Ichimoku Cloud indicator as a feature. Evaluate the model: You can evaluate the performance of the model by applying it to the test data and comparing the predicted option prices to the actual option prices. It's important to keep in mind that incorporating the Ichimoku Cloud strategy into a linear regression model may not necessarily improve the performance of the model, and it's important to validate and backtest the model before deploying it in live trading environment. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model.","title":"Incorporate ichomoku cloud strategy to the linear regression"},{"location":"synapse/build-trade-bot/#pyspark-linear-regression","text":"Python from pyspark.sql.functions import col from pyspark.ml.regression import LinearRegression from pyspark.ml.feature import VectorAssembler # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Calculate the Ichimoku Cloud indicator data = data . withColumn ( \"tenkan_sen\" , ( col ( \"high\" ) + col ( \"low\" )) / 2 ) data = data . withColumn ( \"kijun_sen\" , ( data . selectExpr ( \"avg(high) OVER (ORDER BY date ROWS 26 PRECEDING) as avg_high\" , \"avg(low) OVER (ORDER BY date ROWS 26 PRECEDING) as avg_low\" ) . selectExpr ( \"(avg_high + avg_low) / 2 as kijun_sen\" ))) data = data . withColumn ( \"senkou_span_a\" , ( data . tenkan_sen + data . kijun_sen ) / 2 ) data = data . withColumn ( \"senkou_span_b\" , ( data . selectExpr ( \"avg(high) OVER (ORDER BY date ROWS 52 PRECEDING) as avg_high\" , \"avg(low) OVER (ORDER BY date ROWS 52 PRECEDING) as avg_low\" ) . selectExpr ( \"(avg_high + avg_low) / 2 as senkou_span_b\" ))) # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific calculations for the Ichimoku Cloud indicator. Additionally, it's important to validate and backtest the model before deploying it in live trading environment.","title":"Pyspark Linear Regression"},{"location":"synapse/build-trade-bot/#add-volume-and-volatility-as-additional-features-to-linear-regression","text":"You can add volume and volatility as additional features in the linear regression model by following these steps: Calculate the volatility: You can calculate volatility using standard deviation or other volatility indicators such as the average true range (ATR) and add it as a new column in the data. Add the volume data: You can add the volume data as a new column in the data. Create the feature vector: You would need to update the feature vector to include the new columns for volatility and volume. Train the linear regression model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data, which now includes the volatility and volume as additional features. Evaluate the model: You can evaluate the performance of the model by applying it to the test data and comparing the predicted option prices to the actual option prices. Here's an example of how you could add volume and volatility as additional features in the linear regression model: Python from pyspark.sql.functions import col , stddev # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Calculate the volatility data = data . withColumn ( \"volatility\" , stddev ( col ( \"stock_price\" ))) # Add the volume data data = data . withColumn ( \"volume\" , col ( \"volume\" )) # Calculate the Ichimoku Cloud indicator #... # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) It's important to keep in mind that adding volume and volatility as additional features in the linear regression model may not necessarily improve the performance of the model, and it's important to validate and backtest the model before deploying it in live trading environment. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to consider whether volume and volatility are useful features for your specific problem and whether they are available for the options data you are working with. It's possible that the addition of these features might not bring any value to the model.","title":"Add volume and volatility as additional features to Linear Regression"},{"location":"synapse/build-trade-bot/#linear-regression-using-volatility-from-ichimoku-cloud","text":"You can use the volatility calculated from the Ichimoku Cloud to generate a linear regression model in PySpark as follow: Calculate volatility from Ichimoku Cloud: You would need to calculate volatility from the Ichimoku Cloud, for example, by calculating the standard deviation of the difference between the stock price and the Senkou Span B line. Add the volatility as a feature: You would need to add the calculated volatility as a new column in the data. Create the feature vector: You would need to update the feature vector to include the new column for volatility. Train the linear regression model: Once the data is prepared, you can use PySpark's MLlib library to train a linear regression model using the training data, which now includes the volatility as an additional feature. Evaluate the model: You can evaluate the performance of the model by applying it to the test data and comparing the predicted option prices to the actual option prices. Here's an example of how you could use the volatility calculated from the Ichimoku Cloud as an additional feature in the linear regression model: Python from pyspark.sql.functions import col , stddev # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Calculate the volatility from Ichimoku Cloud data = data . withColumn ( \"volatility\" , stddev ( col ( \"stock_price\" ) - col ( \"senkou_span_b\" ))) # Calculate the Ichimoku Cloud indicator #... # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) data = assembler . transform ( data ) # Create the label data = data . withColumn ( \"label\" , data [ \"option_price\" ]) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Train the linear regression model lr = LinearRegression ( featuresCol = 'features' , labelCol = 'label' ) lr_model = lr . fit ( train_data ) # Evaluate the model on the test data predictions = lr_model . transform ( test_data ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific calculations for the Ichimoku Cloud indicator and volatility. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. Additionally, it's important to use techniques such as regularization and feature selection to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to consider whether volatility calculated from the Ichimoku Cloud is a useful feature for your specific problem. You can try different ways of calculating volatility and compare the performance of the model using different volatility measures to find the best one for your problem.","title":"Linear Regression using volatility from ichimoku cloud"},{"location":"synapse/build-trade-bot/#what-are-other-most-popular-ml-model-for-prediction-over-time-series-financial-data","text":"There are several machine learning models that are commonly used for prediction over time series financial data. Some of the most popular models include: ARIMA: AutoRegressive Integrated Moving Average is a popular time series forecasting model that is based on the idea of using a linear combination of past observations to predict future values. LSTM: Long Short-Term Memory is a type of Recurrent Neural Network (RNN) that is capable of handling sequential data and is commonly used for time series forecasting. Random Forest: Random Forest is an ensemble of decision trees that can be used for both classification and regression tasks. It can be used to model time series data by considering each time step as a separate observation. Gradient Boosting: Gradient Boosting is an ensemble of decision trees that can be used for both classification and regression tasks. It can be used to model time series data by considering each time step as a separate observation XGBoost: A Gradient Boosting model with more advanced features such as regularization and handling missing values. Prophet: A procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. These models can be used to predict various financial time series such as stock prices, exchange rates, and commodity prices. However, it's important to note that time series data can be complex and often requires feature engineering and preprocessing to obtain a good model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment.","title":"What are other most popular ML model for prediction over time series financial data"},{"location":"synapse/build-trade-bot/#create-pyspark-code-for-predicting-option-price-as-previously-done-using-arima-model-rather-than-linear-regression","text":"Example of how you could use the ARIMA model to predict option prices in PySpark: Python from pyspark.sql.functions import col from pyspark.ml.regression import ARIMARegressor from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"date\" , \"option_price\" ) # Convert the date column to timestamp data = data . withColumn ( \"date\" , col ( \"date\" ) . cast ( \"timestamp\" )) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the ARIMA model arima = ARIMARegressor ( p = 1 , d = 1 , q = 1 ) arima_model = arima . fit ( train_data ) # Make predictions on the test data predictions = arima_model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) This example is using the default parameters for the ARIMA model, but it's important to note that finding the optimal parameters for the model is a time-consuming task and often requires trial and error. Also, you may need to preprocess the data further, such as handling missing values or transforming the data to a stationary series. Additionally, it's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific calculations for the model. Additionally, it's important to validate and backtest the model before deploying it in live trading","title":"Create pyspark code for predicting option price as previously done using ARIMA model rather than Linear regression"},{"location":"synapse/build-trade-bot/#using-random-forest","text":"Python from pyspark.ml.regression import RandomForestRegressor from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" , \"option_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the Random Forest Regression model rf = RandomForestRegressor () rf_model = rf . fit ( train_data ) # Make predictions on the test data predictions = rf_model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the Random Forest Regression model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. You could also try different parameters for the Random Forest Regressor, such as the number of trees and the maximum depth of the trees, and test their impact on the model's performance.","title":"Using Random Forest"},{"location":"synapse/build-trade-bot/#using-gradient-boosting","text":"An example of how you could use the Gradient Boosting Regression model to predict option prices in PySpark: Python from pyspark.ml.regression import GBTRegressor from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" , \"option_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the Gradient Boosting Regression model gbt = GBTRegressor () gbt_model = gbt . fit ( train_data ) # Make predictions on the test data predictions = gbt_model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the Gradient Boosting Regression model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. You could also try different parameters for the Gradient Boosting Regressor, such as the number of trees and the maximum depth of the trees, and test their impact on the model's performance. It's also important to consider if Gradient Boosting is appropriate for your specific problem, as it can be sensitive to noise and outliers in the data.","title":"Using Gradient Boosting"},{"location":"synapse/build-trade-bot/#using-xgboost","text":"XGBoost is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be highly efficient and scalable and is widely used for machine learning problems such as regression and classification. To use XGBoost in PySpark, you would need to install the XGBoost4J-Spark library which is a library that allows XGBoost to be integrated with PySpark. Here is an example of how you could use the XGBoost Regression model to predict option prices in PySpark: Python from pyspark.ml import Pipeline from pyspark.ml.regression import XGBoostRegressor from pyspark.ml.feature import VectorAssembler from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" , \"option_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the feature vector feature_cols = [ \"stock_price\" , \"tenkan_sen\" , \"kijun_sen\" , \"senkou_span_a\" , \"senkou_span_b\" , \"volatility\" , \"volume\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the XGBoost Regression model xgb = XGBoostRegressor ( numRound = 10 , maxDepth = 3 , objective = 'reg:squarederror' ) # Set up the pipeline pipeline = Pipeline ( stages = [ assembler , xgb ]) # Train the model model = pipeline . fit ( train_data ) # Make predictions on the test data predictions = model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"option_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) t's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the XGBoost Regression model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. You could also try different parameters for the XGBoost Regressor, such as the number of trees and the maximum depth of the trees, and test their impact on the model's performance. XGBoost is a powerful algorithm and it's important to tune the parameters for better performance.","title":"Using XGBoost"},{"location":"synapse/build-trade-bot/#using-prophet","text":"Prophet is a time series forecasting library developed by Facebook that is based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It's not implemented in PySpark but it can be used in Python, so to use it you will have to move your data to a pandas dataframe. Here is an example of how you could use the Prophet model to predict option prices in Python: Python import pandas as pd from fbprophet import Prophet # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # convert spark dataframe to pandas data = data . toPandas () # Select the relevant columns for the model data = data [[ \"date\" , \"option_price\" ]] data = data . rename ( columns = { \"date\" : \"ds\" , \"option_price\" : \"y\" }) # split data into train and test train = data . iloc [: int ( 0.8 * len ( data ))] test = data . iloc [ int ( 0.8 * len ( data )):] # fit the model m = Prophet () m . fit ( train ) # make predictions future = m . make_future_dataframe ( periods = len ( test )) forecast = m . predict ( future ) # evaluate the model predictions = forecast [[ 'ds' , 'yhat' ]] . tail ( len ( test )) predictions . columns = [ 'date' , 'prediction' ] predictions = predictions . join ( test . set_index ( 'ds' ), on = 'date' ) rmse = (( predictions [ 'y' ] - predictions [ 'prediction' ]) ** 2 ) . mean () ** .5 print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the Prophet model. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that Prophet needs the date column to be in a specific format and named as \"ds\" and the target column as \"y\". Prophet is a very useful library for time series forecasting and it can handle missing values and outliers automatically which makes it a great choice for non-experts.","title":"Using Prophet"},{"location":"synapse/build-trade-bot/#adapt-linear-regression-code-for-call-put-options","text":"Adapting linear regression code for call put options would involve using different features and/or target variables specific to call put options. Call options give the holder the right to buy an underlying asset at a specified price (strike price), while put options give the holder the right to sell an underlying asset at a specified price. Here is an example of how you could adapt linear regression code for call put options in PySpark: Python from pyspark.ml.regression import LinearRegression from pyspark.ml.feature import VectorAssembler from pyspark.ml.evaluation import RegressionEvaluator # Read the data from Delta Lake data = spark . read . format ( \"delta\" ) . load ( \"/path/to/data\" ) # Select the relevant columns for the model data = data . select ( \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" , \"call_price\" , \"put_price\" ) # Split the data into training and test sets train_data , test_data = data . randomSplit ([ 0.8 , 0.2 ]) # Create the feature vector feature_cols = [ \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the Linear Regression model for call options lr_call = LinearRegression ( labelCol = \"call_price\" ) # Create the Linear Regression model for put options lr_put = LinearRegression ( labelCol = \"put_price\" ) # Set up the pipeline pipeline_call = Pipeline ( stages = [ assembler , lr_call ]) pipeline_put = Pipeline ( stages = [ assembler , lr_put ]) # Train the model for call options model_call = pipeline_call . fit ( train_data ) # Train the model for put options model_put = pipeline_put . fit ( train_data ) # Make predictions on the test data for call options predictions_call = model_call . transform ( test_data ) # Make predictions on the test data for put options predictions_put = model_put . transform ( test_data ) # Evaluate the model for call options evaluator_call = RegressionEvaluator ( labelCol = \"call_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse_call = evaluator_call . evaluate ( predictions_call ) print ( \"Root Mean Squared Error (RMSE) for call options on test data = %g \" % rmse_call ) # Evaluate the model for put options evaluator_put = RegressionEvaluator ( labelCol = \"put_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse_put = evaluator_put . evaluate ( predictions_put ) print ( \"Root Mean Squared Error (RMSE) for put options on test data = %g \" % rmse_put ) This is a simple example of how you could adapt linear regression code for call put options. In this example, we are using different feature columns and target columns for call and put options. Additionally, we are fitting and evaluating separate models for call and put options. You could also experiment with different feature combinations, regularization techniques, and evaluation metrics to improve the performance of the model. It's also important to validate and backtest the model before deploying it in live trading environment and consider using different models and comparing their performance using different evaluation metrics to choose the best one.","title":"Adapt linear regression code for call put options"},{"location":"synapse/build-trade-bot/#incorporate-news-feed-to-the-model","text":"Incorporating news feed into the model would involve gathering news data related to the underlying asset and using it as an additional feature in the model. There are several ways to gather news data, such as using web scraping or an API from a news source. Here is an example of how you could incorporate news feed into the model in PySpark: Python from pyspark.sql.functions import udf from pyspark.ml.feature import HashingTF , IDF , Tokenizer # Read the news data news_data = spark . read . format ( \"csv\" ) . load ( \"/path/to/news_data\" ) # Perform preprocessing on the news data # Tokenize the text tokenizer = Tokenizer ( inputCol = \"text\" , outputCol = \"words\" ) news_data = tokenizer . transform ( news_data ) # Convert the words to a numerical feature vector hashing_tf = HashingTF ( inputCol = \"words\" , outputCol = \"rawFeatures\" ) news_data = hashing_tf . transform ( news_data ) # Scale the feature vector idf = IDF ( inputCol = \"rawFeatures\" , outputCol = \"news_features\" ) idf_model = idf . fit ( news_data ) news_data = idf_model . transform ( news_data ) # Join the news data with the option data data = data . join ( news_data , data . date == news_data . date ) # Create the feature vector feature_cols = [ \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" , \"news_features\" ] assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the Linear Regression model lr = LinearRegression ( labelCol = \"call_price\" or \"put_price\" ) # Set up the pipeline pipeline = Pipeline ( stages = [ assembler , lr ]) # Train the model model = pipeline . fit ( data ) # Make predictions on the test data predictions = model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"call_price\" or \"put_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) on test data = %g \" % rmse ) It's important to note that this is just an example and you would need to customize it to suit your specific requirements, such as the specific columns in your data and the specific parameters for the preprocessing and feature extraction. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that news data may be noisy, and it's important to preprocess the data and extract relevant features before using it in the model. The example above uses TF-IDF to convert the news text into a numerical feature vector, but you could also use other techniques such as word embeddings, LDA, or sentiment analysis. It's also important to note that this example is using a linear regression model, you could also use other models that are more suited to handle text data such as LSTM or other neural network models. In addition, It's also important to keep in mind that news data is time-sensitive, it would be more meaningful if it's used in a time window that is close to the time of prediction, and also it would be more useful if it's filtered to include only news related to the specific company or sector of the underlying asset. It's also important to note that incorporating news feed into the model may increase the complexity of the model and make it more difficult to interpret and understand. In summary, incorporating news feed into the model can potentially improve the performance of the model by providing additional information about the underlying asset. However, it's important to properly preprocess and extract relevant features from the news data and to validate and backtest the model before deploying it in a live trading environment.","title":"Incorporate news feed to the model"},{"location":"synapse/build-trade-bot/#enable-gpu-during-model-training","text":"Enabling GPU during model training can significantly speed up the training process, especially for large and complex models. Here are the general steps to enable GPU during model training in PySpark: Verify that your system has a CUDA-enabled GPU and the necessary drivers are installed. You can check this by running the command nvidia-smi in the terminal. nstall the latest version of PySpark and the deep learning libraries such as TensorFlow or PyTorch that you plan to use for model training. Configure the Spark environment to use the GPU. You can do this by setting the following environment variables: export SPARK_LOCAL_DIRS=/path/to/gpu_storage export SPARK_WORKER_CORES=1 export SPARK_WORKER_INSTANCES=1 export SPARK_WORKER_MEMORY=2g export SPARK_DRIVER_MEMORY=2g Start Spark with the following command: pyspark --master local[*] --conf spark.driver.memory=2g --conf spark.executor.memory=2g --conf spark.executorEnv.JAVA_HOME=/path/to/jdk In the PySpark script, set the TensorFlow or PyTorch session to use the GPU. For example, for TensorFlow you can use the following code: Python import tensorflow as tf tf . config . list_physical_devices ( 'GPU' ) tf . config . experimental . set_memory_growth ( tf . config . list_physical_devices ( 'GPU' )[ 0 ], True ) train your model using the GPU Python with tf . device ( '/gpu:0' ): model . fit ( train_data ) It's important to note that these are general steps, and the specific details may vary depending on the system and libraries you are using. Additionally, it's important to monitor the GPU usage during training to ensure that the GPU is being utilized effectively. It's also important to note that running deep learning models on a GPU requires a large amount of memory, so it's important to have a GPU with enough memory to handle the size of your model. It's also important to note that not all models can run on a GPU, so it's important to check the compatibility of your model with GPU before trying to run it on GPU.","title":"Enable GPU during model training"},{"location":"synapse/build-trade-bot/#linear-regression-for-multiple-symbols-using-pyspark","text":"Approaching linear regression for multiple symbols using PySpark would involve creating separate models for each symbol and then training and evaluating them independently. Here is an example of how you could approach linear regression for multiple symbols using PySpark: Python from pyspark.sql.functions import udf from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator # Create a function to filter the data for a specific symbol def filter_data ( symbol ): data = option_data . filter ( option_data . symbol == symbol ) return data # Define the feature columns feature_cols = [ \"stock_price\" , \"strike_price\" , \"time_to_expiration\" , \"volatility\" ] # Create the assembler for the feature vector assembler = VectorAssembler ( inputCols = feature_cols , outputCol = \"features\" ) # Create the Linear Regression model lr = LinearRegression ( labelCol = \"call_price\" or \"put_price\" ) # Set up the pipeline pipeline = Pipeline ( stages = [ assembler , lr ]) # Create a list of symbols symbols = [ \"AAPL\" , \"GOOG\" , \"MSFT\" , \"AMZN\" ] # Create a dictionary to store the models models = {} # Train and evaluate a model for each symbol for symbol in symbols : # Filter the data for the symbol symbol_data = filter_data ( symbol ) train_data , test_data = symbol_data . randomSplit ([ 0.8 , 0.2 ]) # Train the model model = pipeline . fit ( train_data ) # Make predictions on the test data predictions = model . transform ( test_data ) # Evaluate the model evaluator = RegressionEvaluator ( labelCol = \"call_price\" or \"put_price\" , predictionCol = \"prediction\" , metricName = \"rmse\" ) rmse = evaluator . evaluate ( predictions ) print ( \"Root Mean Squared Error (RMSE) for symbol {} on test data = {} \" . format ( symbol , rmse )) # Add the model to the dictionary models [ symbol ] = model In this example, we are filtering the option data for each symbol, creating a separate model for each symbol, training and evaluating them independently, and storing them in a dictionary for later use. This approach allows you to train and evaluate models for multiple symbols independently, which can be useful if the symbols have different characteristics or if you want to compare the performance of the models for different symbols. It's important to note that this is just an example, you would need to customize it to suit your specific requirements, such as the specific columns in your data, the specific parameters for the preprocessing and feature extraction and the symbols. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to use techniques such as feature selection and regularization to avoid overfitting, and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that this example uses a simple linear regression model, but you could also use other models that are more suited to handle time-series data such as ARIMA or other ML models like Random Forest, XGBoost, Gradient Boosting etc. Also, it's important to note that this example trains and evaluates the model separately for each symbol, but you could also consider using a more sophisticated approach like k-fold cross-validation to get more robust evaluation metric. In summary, approaching linear regression for multiple symbols using PySpark involves creating separate models for each symbol and then training and evaluating them independently. This allows you to train and evaluate models for multiple symbols independently, and also allows you to compare the performance of the models for different symbols.","title":"Linear regression for multiple symbols using pyspark"},{"location":"synapse/build-trade-bot/#generate-buy-sell-signal-for-stocks-using-previous-ichimoku-cloud-for-a-large-number-of-symbols-efficiently","text":"Generating buy and sell signals for stocks using the Ichimoku Cloud for a large number of symbols efficiently would involve creating a script that can automate the process of calculating the Ichimoku Cloud indicators and generating the buy and sell signals for each symbol. Here is an example of how you could generate buy and sell signals for stocks using the Ichimoku Cloud for a large number of symbols efficiently in PySpark: Python from pyspark.sql.functions import udf # Define a function to calculate the Ichimoku Cloud indicators def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators # Define a function to generate the buy and sell signals def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType ())) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are defining a function to calculate the Ichimoku Cloud indicators and a function to generate the buy and sell signals for each symbol. Then we are creating a UDF to apply the Ichimoku Cloud indicators calculation to the stock data and another UDF to apply the signals generation to the stock data. Then we are grouping the data by symbol and getting the last signal. It's important to note that this is just an example, you would need to customize it to suit your specific requirements, such as the specific columns in your data, the specific parameters for the Ichimoku Cloud indicators calculation, and the specific conditions for generating the buy and sell signals. Additionally, it's important to validate and backtest the model before deploying it in live trading environment. It's also important to note that this example uses a simple buy/sell signal generation based on the Ichimoku Cloud indicators, but you could also use other indicators or technical analysis tools to generate the signals and also use various evaluation metrics to evaluate the performance of the model. It's also important to note that this example uses close prices for the calculation of Ichimoku Cloud indicators, but you could also use other prices such as high, low, open etc. In summary, generating buy and sell signals for stocks using the Ichimoku Cloud for a large number of symbols efficiently would involve creating a script that can automate the process of calculating the Ichimoku Cloud indicators and generating the buy and sell signals for each symbol using PySpark, and then grouping the data by symbol to get the last signal.","title":"Generate buy sell signal for stocks using previous ichimoku cloud for a large number of symbols efficiently"},{"location":"synapse/build-trade-bot/#more-efficient-version-using-numba","text":"Using Numba can help make the script more efficient by compiling the Python code into machine code, which can improve performance, especially for computationally intensive tasks such as calculating the Ichimoku Cloud indicators and generating the buy and sell signals. Here is an example of how you could make the script more efficient using Numba: Python from numba import jit import numpy as np @jit def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators @jit def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType ())) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are using the jit decorator from Numba to compile the ichimoku_cloud and generate_signals functions into machine code, which can significantly improve performance. It's important to note that Numba works best when the code is simple and the data is homogeneous, and it's not recommended to use it on more complex structures such as Pandas DataFrame. In summary, using Numba can help make the script more efficient by compiling the Python code into machine code, which can improve performance, especially for computationally intensive tasks such as calculating the Ichimoku Cloud indicators and generating the buy and sell signals for large number of symbols in PySpark.","title":"More efficient version using numba"},{"location":"synapse/build-trade-bot/#complete-implementation-with-stock-data","text":"Read symbols from a Delta lake table, with columns event_time, close, high, low, trade_count, open, t, volume, vwap, date, timestamp, symbol, event_dt, and integrate it into the previous solution with Numba: Python from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession . builder . appName ( \"Ichimoku Cloud\" ) . getOrCreate () # Read symbols from a Delta lake table stock_data = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/lake/table\" ) # Keep only the columns we need stock_data = stock_data . select ( \"symbol\" , \"event_time\" , \"close\" , \"high\" , \"low\" , \"trade_count\" , \"open\" , \"t\" , \"volume\" , \"vwap\" , \"date\" , \"timestamp\" , \"event_dt\" ) @jit def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators @jit def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType ())) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are reading symbols from a delta lake table with the specified columns and integrating it into the previous solution with Numba. We are using the SparkSession.read.format(\"delta\").load() method to read the data from the Delta lake table, and selecting only the columns we need for our analysis. We are then applying the ichimoku_cloud and generate_signals functions, which were previously decorated with the jit decorator from Numba, to the data. Finally, we are grouping the data by symbol and getting the last signal. It's important to note that this is a simplified example and you may need to adjust it to suit your specific requirements, such as handling missing data or adding additional data pre-processing steps. In summary, reading symbols from a Delta lake table with specified columns and integrating it into the previous solution with Numba is a matter of reading the data using the appropriate method and integrating the existing solution with it.","title":"Complete implementation with stock data"},{"location":"synapse/build-trade-bot/#handling-missing-data","text":"Handling missing data is an important step in any data analysis or modeling task, as missing data can lead to inaccurate or unreliable results. There are several ways to handle missing data, depending on the specific situation and the amount of missing data. Some common methods are: Deletion: This method involves deleting the rows or columns that contain missing data. This method is simple and easy to implement, but it can result in a loss of information and a decrease in the sample size. It is generally not recommended if the amount of missing data is large. Imputation: This method involves replacing the missing values with estimates based on the available data. Common imputation methods include mean, median, and mode imputation. This method is more sophisticated than deletion, but it can still introduce bias if the missing data is not missing at random. Interpolation: This method involves estimating missing values based on the values of other observations. Common interpolation methods include linear interpolation and spline interpolation. This method is more sophisticated than imputation, but it can still introduce bias if the missing data is not missing at random. Multiple imputation: This method involves generating multiple imputed datasets, each with different imputed values for the missing data. Then, statistical analysis is performed on each dataset and the results are combined to produce a final estimate. Predictive models: This method involves building a model to predict the missing values based on the available data. This method is more sophisticated than other methods, but it can be more accurate and less biased if the missing data is not missing at random. Leave it as it is: If the missing data is not significant you can just leave it as it is. It's important to note that the best method for handling missing data will depend on the specific situation and the amount of missing data, and it's always good to use multiple techniques and compare the results. In summary, handling missing data is an important step in any data analysis or modeling task. There are several ways to handle missing data, depending on the specific situation and the amount of missing data, including deletion, imputation, interpolation, multiple imputation, and predictive models. It's important to select the appropriate method and to use multiple techniques and compare the results.","title":"Handling missing data"},{"location":"synapse/build-trade-bot/#add-interpolation-for-missing-data","text":"Add an interpolation method for handling missing data to the previous example: Python from pyspark.sql import SparkSession # Create a Spark session spark = SparkSession . builder . appName ( \"Ichimoku Cloud\" ) . getOrCreate () # Read symbols from a Delta lake table stock_data = spark . read . format ( \"delta\" ) . load ( \"path/to/delta/lake/table\" ) # Keep only the columns we need stock_data = stock_data . select ( \"symbol\" , \"event_time\" , \"close\" , \"high\" , \"low\" , \"trade_count\" , \"open\" , \"t\" , \"volume\" , \"vwap\" , \"date\" , \"timestamp\" , \"event_dt\" ) # Interpolate missing values stock_data = stock_data . na . fill ( method = 'linear' ) @jit def ichimoku_cloud ( data ): # Calculate the indicators tenkan_sen = data . rolling ( window = 9 ) . mean () kijun_sen = data . rolling ( window = 26 ) . mean () senkou_span_a = ( tenkan_sen + kijun_sen ) / 2 senkou_span_b = data . rolling ( window = 52 ) . mean () chikou_span = data . shift ( - 26 ) # Create a new dataframe with the indicators indicators = pd . DataFrame ({ \"tenkan_sen\" : tenkan_sen , \"kijun_sen\" : kijun_sen , \"senkou_span_a\" : senkou_span_a , \"senkou_span_b\" : senkou_span_b , \"chikou_span\" : chikou_span }) return indicators @jit def generate_signals ( data ): # Initialize an empty list to store the signals signals = [] # Iterate through the data for index , row in data . iterrows (): if row [ \"close\" ] > row [ \"senkou_span_a\" ] and row [ \"close\" ] > row [ \"senkou_span_b\" ]: signals . append ( \"BUY\" ) elif row [ \"close\" ] < row [ \"senkou_span_a\" ] and row [ \"close\" ] < row [ \"senkou_span_b\" ]: signals . append ( \"SELL\" ) else : signals . append ( \"HOLD\" ) return signals # Create a UDF to apply the Ichimoku Cloud indicators calculation to the stock data ichimoku_udf = udf ( ichimoku_cloud , ArrayType ( DoubleType )) stock_data = stock_data . withColumn ( \"ichimoku_indicators\" , ichimoku_udf ( stock_data [ \"close\" ])) # Create a UDF to apply the signals generation to the stock data signals_udf = udf ( generate_signals , ArrayType ( StringType ())) stock_data = stock_data . withColumn ( \"signals\" , signals_udf ( stock_data [ \"ichimoku_indicators\" ])) #group by stock symbol grouped_data = stock_data . groupby ( \"symbol\" ) . agg ({ \"signals\" : \"last\" }) In this example, we are using the na.fill() method to interpolate the missing values in the stock_data DataFrame. We pass the method='linear' argument to interpolate the missing values using linear interpolation. You can use other interpolation methods as well, such as 'spline' or 'pchip'. It's important to note that interpolation can be a good method for handling missing data if the data is missing at random and it's not a big portion of data. In summary, adding interpolation method for handling missing data to the previous example is a matter of using the appropriate method to interpolate the missing values and integrating it with the existing solution. It's important to note that interpolation can be a good method for handling missing data if the data is missing at random and it's not a big portion of data. However if the missing data is not missing at random, then interpolation can introduce bias to the analysis, it's always good to use multiple techniques and compare the results.","title":"Add interpolation for missing data"},{"location":"synapse/build-trade-bot/#plot-is-using-ploty","text":"Plot the stock data and signals using Plotly in Python: Python import plotly.express as px # Create a scatter plot of the stock data fig = px . scatter ( stock_data , x = \"event_time\" , y = \"close\" , color = \"signals\" , title = \"Stock Data and Signals\" ) # Add a line for the Tenkan Sen indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"tenkan_sen\" ], name = \"Tenkan Sen\" , line = dict ( color = 'firebrick' , width = 2 ))) # Add a line for the Kijun Sen indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"kijun_sen\" ], name = \"Kijun Sen\" , line = dict ( color = 'darkblue' , width = 2 ))) # Add a line for the Senkou Span A indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"senkou_span_a\" ], name = \"Senkou Span A\" , line = dict ( color = 'green' , width = 2 ))) # Add a line for the Senkou Span B indicator fig . add_trace ( go . Scatter ( x = stock_data [ \"event_time\" ], y = stock_data [ \"senkou_span_b\" ], name = \"Senkou Span B\" , line = dict ( color = 'purple' , width = 2 ))) # Show the plot fig . show () In this example, we are using the plotly.express library to create a scatter plot of the stock data using the px.scatter() function. We are passing the stock_data DataFrame, the x-axis data (event_time), the y-axis data (close), and the color data (signals) to the function. We are also setting the title of the plot to \"Stock Data and Signals\". After that, we are adding traces for the Tenkan Sen, Kijun Sen, Senkou Span A, and Senkou Span B indicators using the fig.add_trace() function. We are passing the x-axis data (event_time) and y-axis data (tenkan_sen, kijun_sen, senkou_span_a, senkou_span_b) for each indicator, and setting the name and color of the line. Finally, we are using the fig.show() function to display the plot. It's important to note that this is a simplified example and you may need to adjust it to suit your specific requirements, such as customizing the plot's appearance or adding additional data to the plot. In summary, plotting the stock data and signals using Plotly in Python is a matter of using the appropriate plotly functions to create the plot, set the data and appearance, and displaying the plot.","title":"Plot is using ploty"},{"location":"synapse/build-trade-bot/#plot-grouped_data-using-plotly","text":"Python import plotly.graph_objs as go #create a list of symbols and signals symbols = grouped_data . select ( \"symbol\" ) . rdd . flatMap ( lambda x : x ) . collect () signals = grouped_data . select ( \"signals\" ) . rdd . flatMap ( lambda x : x ) . collect () # Create a scatter plot of the grouped data fig = go . Figure ( data = [ go . Scatter ( x = symbols , y = signals , mode = 'markers' )]) # Set the title of the plot fig . update_layout ( title = 'Symbols and Signals' ) # Set the y axis title fig . update_yaxes ( title_text = 'Signals' ) # Set the x axis title fig . update_xaxes ( title_text = 'Symbols' ) # Show the plot fig . show () In this example, We are creating a list of symbols and signals from grouped_data DataFrame by using select() and rdd.flatMap(lambda x: x).collect() methods. Then, we are using the go.Scatter() function from the plotly.graph_objs library to create a scatter plot of the grouped data. We are passing the symbols and signals as the x and y data respectively. After that, we are using the update_layout(), update_yaxes(), and update_xaxes() methods to set the title of the plot, and the y and x axis titles. Finally, we are using the fig.show() function to display the plot. This is a simplified example, you may need to adjust it to suit your specific requirements, such as customizing the plot's appearance or adding additional data to the plot. In summary, plotting the grouped_data using Plotly in Python is a matter of creating a list of symbols and signals, using the appropriate plotly functions to create the plot, set the data and appearance, and displaying the plot.","title":"Plot grouped_data using plotly"},{"location":"synapse/build-trade-bot/#why-pyspark-has-following-error-while-trying-to-display-grouped_datashow10","text":"File \"/usr/bin/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads return cloudpickle.loads(obj, encoding=encoding) ModuleNotFoundError: No module named 'numba' The error \"ModuleNotFoundError: No module named 'numba'\" suggests that the PySpark environment doesn't have the Numba library installed. When you run your script in PySpark, it uses the Python environment that is installed on the cluster. If the cluster does not have the Numba library installed, PySpark will not be able to find it and will throw the error. There are a few ways to address this issue: You can install the Numba library on the cluster where you are running PySpark. You can do this by using pip or conda. You can bundle the Numba library with your script, so that it gets packaged with the script and is available to PySpark when it runs. You can use the --py-files option when running your script to do this. Alternatively, you can use a different approach to handle the missing data, or use a different library that is already available in the cluster. It's important to note that if you are running the script in a local environment, you should have the library installed in your machine.","title":"Why pyspark has following error while trying to display grouped_data.show(10)"},{"location":"synapse/build-trade-bot/#why-pyspark-has-following-error-while-trying-to-display-group_datashow10","text":"return f(*args, **kwargs) File \"/usr/bin/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 93, in return lambda *a: g(f(*a)) AttributeError: 'float' object has no attribute 'rolling' The error \"attributeError: 'float' object has no attribute 'rolling'\" suggests that you are trying to use the rolling() method on a float object, which does not have this method. The rolling() method is available on pandas DataFrame and Series objects, but not on a single float value. This error is likely caused by a problem with the data in the grouped_data DataFrame. The ichimoku_cloud function expects the input data to be a pandas DataFrame or Series object, but the input data passed to it is a single float value. To fix this issue, you should check the data in the grouped_data DataFrame and make sure it is in the correct format. You can check the data type of the column that you are passing to the ichimoku_cloud function by using the dtypes property of the DataFrame. grouped_data.dtypes In addition, you should check that the column you are passing to the function is not empty and it is not missing any data. grouped_data.select(\"column_name\").na.drop().count() If the column is of wrong data type you should cast it to the correct type. grouped_data = grouped_data.withColumn(\"column_name\", grouped_data[\"column_name\"].cast(DoubleType())) In summary, the error message is likely caused by a problem with the data in the grouped_data DataFrame, specifically the data passed to the ichimoku_cloud function is not in the correct format. You should check the data in the DataFrame and make sure it is in the correct format, and also check that the column you are passing to the function is not empty and it is not missing any data.","title":"why pyspark has following error while trying to display group_data.show(10)"},{"location":"synapse/introduction/","text":"\ud83d\udcd8 Synapse Introduction \u00b6 Architectural Diagram \u00b6 SQL Pool Table design performance considerations \u00b6 Table Indexing Recommended use Clustered Columnstore Recommended for tables with greater than 100 million rows, offers the highest data compression with best overall query performance. Heap Tables Smaller tables with less than 100 million rows, commonly used as a staging table prior to transformation. Clustered Index Large lookup tables (> 100 million rows) where querying will only result in a single row returned. Clustered Index + non-clustered secondary index Large tables (> 100 million rows) when single (or very few) records are being returned in queries. Table Distribution/Partition Type Recommended use Hash distribution Tables that are larger than 2 GBs with infrequent insert/update/delete operations, works well for large fact tables in a star schema. Round robin distribution Default distribution, when little is known about the data or how it will be used. Use this distribution for staging tables. Replicated tables Smaller lookup tables, less than 1.5 GB in size.","title":"\ud83d\udcd8 Synapse Introduction"},{"location":"synapse/introduction/#synapse-introduction","text":"","title":"\ud83d\udcd8 Synapse Introduction"},{"location":"synapse/introduction/#architectural-diagram","text":"","title":"Architectural Diagram"},{"location":"synapse/introduction/#sql-pool-table-design-performance-considerations","text":"Table Indexing Recommended use Clustered Columnstore Recommended for tables with greater than 100 million rows, offers the highest data compression with best overall query performance. Heap Tables Smaller tables with less than 100 million rows, commonly used as a staging table prior to transformation. Clustered Index Large lookup tables (> 100 million rows) where querying will only result in a single row returned. Clustered Index + non-clustered secondary index Large tables (> 100 million rows) when single (or very few) records are being returned in queries. Table Distribution/Partition Type Recommended use Hash distribution Tables that are larger than 2 GBs with infrequent insert/update/delete operations, works well for large fact tables in a star schema. Round robin distribution Default distribution, when little is known about the data or how it will be used. Use this distribution for staging tables. Replicated tables Smaller lookup tables, less than 1.5 GB in size.","title":"SQL Pool Table design performance considerations"},{"location":"synapse/links/","text":"\ud83d\udd17 Synapse links \u00b6 Git Repositories: \u00b6 DP-203T00: Data Engineering on Azure Synapse hands on labs Synapse GA hands on labs Synapse from Azure docs Synapse build together blog - series VsCode tools Synapse Azure docs version generate using template + Synapse analytics workshop 400 - Optimizing SQL Pools Synapse analytics day Microsoft MCW Synapse analytics Azure Synapse Analytics end-to-end solution Azure Synapse L300 and L400 workshops assets Monitoring Azure Databricks in an Azure Log Analytics Workspace Tutorials \u00b6 Create, develop, and maintain Synapse notebooks in Azure Synapse Analytics Create and run a notebook with Kqlmagic Load data into Azure Synapse Analytics using Azure Data Factory or a Synapse pipeline Building the lakehouse architecture with azure synapse analytics Getting started with Apache Spark, Python and PySpark Command Line Installation - Good cloudera site for installation Pandas to read/write Azure Data Lake Storage Gen2 data Explore the Data Lake with Azure Synapse Serverless SQL pool and Azure Synapse Spark Logging \u00b6 Common Data Model (CDM) Connector for Azure Synapse Spark Spark log analytics Synapse pyspark Notebook examples - slider, import, etc Stackoverflow Monitor Synapse Spark with Log Analytics Some good apache products \u00b6 Apache Iceberg Apache Superset","title":"\ud83d\udd17  Synapse links"},{"location":"synapse/links/#synapse-links","text":"","title":"\ud83d\udd17  Synapse links"},{"location":"synapse/links/#git-repositories","text":"DP-203T00: Data Engineering on Azure Synapse hands on labs Synapse GA hands on labs Synapse from Azure docs Synapse build together blog - series VsCode tools Synapse Azure docs version generate using template + Synapse analytics workshop 400 - Optimizing SQL Pools Synapse analytics day Microsoft MCW Synapse analytics Azure Synapse Analytics end-to-end solution Azure Synapse L300 and L400 workshops assets Monitoring Azure Databricks in an Azure Log Analytics Workspace","title":"Git Repositories:"},{"location":"synapse/links/#tutorials","text":"Create, develop, and maintain Synapse notebooks in Azure Synapse Analytics Create and run a notebook with Kqlmagic Load data into Azure Synapse Analytics using Azure Data Factory or a Synapse pipeline Building the lakehouse architecture with azure synapse analytics Getting started with Apache Spark, Python and PySpark Command Line Installation - Good cloudera site for installation Pandas to read/write Azure Data Lake Storage Gen2 data Explore the Data Lake with Azure Synapse Serverless SQL pool and Azure Synapse Spark","title":"Tutorials"},{"location":"synapse/links/#logging","text":"Common Data Model (CDM) Connector for Azure Synapse Spark Spark log analytics Synapse pyspark Notebook examples - slider, import, etc Stackoverflow Monitor Synapse Spark with Log Analytics","title":"Logging"},{"location":"synapse/links/#some-good-apache-products","text":"Apache Iceberg Apache Superset","title":"Some good apache products"},{"location":"synapse/problem-solutions/","text":"\ud83d\udd13 Synapse / PySpark problem and solutions \u00b6 Sql Server to PySpark in Synapse \u00b6 To create a DataFrame in PySpark that reads data from a table in SQL Server using Synapse, you can use the following code as an example: Python from pyspark.sql import SparkSession # Create a SparkSession spark = SparkSession . builder \\ . appName ( \"Read from SQL Server table\" ) \\ . config ( \"spark.sql.databricks.directSQL.enabled\" , \"true\" ) \\ . config ( \"spark.databricks.io.cache.enabled\" , \"true\" ) \\ . config ( \"spark.databricks.io.cache.maxBytes\" , \"2g\" ) \\ . config ( \"spark.databricks.io.cache.ttl\" , \"10m\" ) \\ . getOrCreate () # Create a DataFrame that reads data from the specified SQL Server table table_name = \"[dbo].[your_table_name]\" url = \"jdbc:sqlserver://your_server_name.database.windows.net:1433;database=your_database_name\" properties = { \"user\" : \"your_username\" , \"password\" : \"your_password\" , \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" } df = spark . read . jdbc ( url = url , table = table_name , properties = properties ) # Show the DataFrame df . show () Please modify the values in the properties such as user, password, table name, server and database name accordingly to make sure the code works for you. You also have to have the sqlserver connector downloaded in your environment. Bash Session !pip install pyspark-sql-mssql Check file exists in PySpark or Python \u00b6 In PySpark, you can use the os.path module from the Python standard library to check if a file exists. Here is an example of how you can use the os.path.exists() function to check if a file exists: Python import os file_path = \"/path/to/your/file\" if os . path . exists ( file_path ): print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) Alternatively, you could also use os.path.isfile() function to check if a path points to a regular file. Python if os . path . isfile ( file_path ): print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) !!!note that in both cases, file_path should be a string representing the path to the file you want to check. Text Only You also can use spark.sql to check the existance of a file in hadoop filesystem, you can use the spark.sql(\"\"\"SHOW PARTITIONS tablename\"\"\") or spark.sql(\"\"\"describe formatted tablename\"\"\") to check if a file exist or not. Python from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"CheckFileExist\" ) . getOrCreate () file_path = \"/path/to/your/file\" results = spark . sql ( \"describe formatted {file_path} \" ) . collect () if results : print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) It will work for file that is in hadoop filesystem. Check file in ADLS \u00b6 To check if a file exists in Azure Data Lake Storage (ADLS) using PySpark, you can use the dbutils.fs module from the Databricks environment. The dbutils.fs.ls() function can be used to list the files in a directory, and you can check the length of the returned list to see if a file exists. Here is an example: Python from pyspark.dbutils import DBUtils file_path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/file\" dbutils = DBUtils ( spark ) results = dbutils . fs . ls ( file_path ) if len ( results ) > 0 : print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) Alternatively, you could also use dbutils.fs.exists(path) function to check file exist or not Python from pyspark.dbutils import DBUtils file_path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/file\" dbutils = DBUtils ( spark ) exist = dbutils . fs . exists ( file_path ) if exist : print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) Please make sure that the connection to the ADLS is setup properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. You have to have the azure-storage and azure-storage-file-datalake package installed in your environment. Bash Session !pip install azure-storage !pip install azure-storage-file-datalake Try catch in python \u00b6 In Python, try and except are used to handle exceptions, which are events that occur during the execution of a program that disrupt the normal flow of instructions. The try block contains the code that may raise an exception. If an exception is raised within the try block, the code in the corresponding except block will be executed. If no exception is raised, the code in the except block will be skipped. Here is an example of how to use the try and except statements in Python: Python try : # some code that may raise an exception result = 1 / 0 except ZeroDivisionError : # code to be executed if the exception is raised print ( \"Cannot divide by zero.\" ) In this example, the try block contains code that tries to divide 1 by 0, which raises a ZeroDivisionError exception. The except block contains code that prints a message if this exception is raised. You can also use finally block, which contains code that will be executed no matter whether an exception is raised or not. Python try : # some code that may raise an exception result = 1 / 0 except ZeroDivisionError : # code to be executed if the exception is raised print ( \"Cannot divide by zero.\" ) finally : print ( \"Closing any open resources\" ) You also can chain multiple except block, to handle different type of exception and different action correspondingly Python try : # some code that may raise an exception result = 1 / 0 except ZeroDivisionError : # code to be executed if ZeroDivisionError exception is raised print ( \"Cannot divide by zero.\" ) except Exception as e : # code to be executed if any exception other than ZeroDivisionError exception is raised print ( f \"an error occur : { e } \" ) It's a good practice to always handle the exceptions, to prevent the application from crashing, or unexpected behavior. Get latest file from datalake using pyspark in synapse \u00b6 You can use the dbutils.fs module from the Databricks environment in Synapse to get the latest file from a directory in Azure Data Lake Storage (ADLS). Here's an example of how you can do this: Python from pyspark.dbutils import DBUtils dbutils = DBUtils ( spark ) # directory path in ADLS dir_path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/directory\" # list all files in the directory files = dbutils . fs . ls ( dir_path ) # sort files by last modification time and get the latest file latest_file = sorted ( files , key = lambda x : x . modtime , reverse = True )[ 0 ] print ( f 'latest file is { latest_file . path } ' ) his code lists all the files in the specified directory in ADLS using the dbutils.fs.ls() function, then sorts the files by the last modification time in descending order, so that the latest file is at the top of the list. Then it access the first element of the list which is the latest file. It will print the path of the latest file. Please make sure that the connection to the ADLS is setup properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. You have to have the azure-storage and azure-storage-file-datalake package installed in your environment. Python ! pip install azure - storage ! pip install azure - storage - file - datalake You also can filter based on file type or regex on filename if needed before sorting the files Python files = dbutils . fs . ls ( dir_path ) files = filter ( lambda x : x . name . endswith ( '.csv' ), files ) latest_file = sorted ( files , key = lambda x : x . modtime , reverse = True )[ 0 ] Using mssparkutils in synapse to get latest file from ADLS \u00b6 In Synapse, you can use the mssparkutils library to retrieve the latest file from Azure Data Lake Storage (ADLS) using PySpark. This library provides a getLatestFile() function that allows you to retrieve the latest file from a given directory in ADLS. Here's an example of how you can use the mssparkutils.fileio.datalake.getLatestFile() function to retrieve the latest file from ADLS: Python from mssparkutils.fileio.datalake import getLatestFile path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" latest_file = getLatestFile ( path ) print ( latest_file ) You can also specify a date pattern and/or a file extension to narrow down the search: Python latest_file = getLatestFile ( path , date_pattern = \"yyyyMMdd\" , extension = \".csv\" ) You have to have the mssparkutils package installed in your environment Python ! pip install mssparkutils This function use the dbutils module to get the files, so make sure that the connection to the ADLS is set up properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. This solution is a more robust and versatile approach, that considers both the lexicographical ordering of the files and their modification date to ensure that the correct latest file is returned. How to partition and query efficiently parquet file in data lake or ADLS using pyspark \u00b6 Parquet is a columnar storage format that is well-suited for storing large, complex datasets. Partitioning the data in Parquet format can improve the efficiency of queries by allowing Spark to only scan the necessary data, rather than the entire dataset. Here's an example of how you can partition a DataFrame in PySpark and write it to a Parquet file in Azure Data Lake Storage (ADLS) using the partitionBy() method: Python from pyspark.sql.functions import * # Create a DataFrame df = spark . range ( 100 ) . selectExpr ( \"id\" , \"id % 10 as mod_10\" ) # Partition the DataFrame by the \"mod_10\" column df . write . partitionBy ( \"mod_10\" ) . parquet ( \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" ) Once the data is partitioned, you can use the filter() function to only scan the necessary partitions when querying the data: Python # read parquet data df_parquet = spark . read . parquet ( \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" ) # filter only data where `mod_10` is 5 df_parquet . filter ( col ( \"mod_10\" ) == 5 ) . show () You can also use predicate pushdown feature to push the filter conditions to the storage layer instead of doing the filtering in Spark, this approach further improves query performance by reducing the amount of data that needs to be read and processed. Python # read parquet data and enable predicate pushdown df_parquet = spark . read . parquet ( \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" ) . filter ( \"mod_10 = 5\" ) df_parquet . show () You also have to have the azure-storage and azure-storage-file-datalake package installed in your environment. Bash Session !pip install azure-storage !pip install azure-storage-file-datalake And make sure that the connection to the ADLS is set up properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. By partitioning and filtering the data in this way, you can improve the performance of your queries by reducing the amount of data that Spark needs to scan and process. It's also important to mention that it would be best to be selective in the partitioning key, using a high cardinality column, such as date or timestamp, will increase the number of small partitions which will increase the number of metadata that needs to be scanned and may cause performance degradation rather than improvement. What should be the value for spark.sql.analyzer.maxIternations \u00b6 The spark.sql.analyzer.maxIterations configuration property sets the maximum number of iterations that Spark's Catalyst optimizer will perform when analyzing a query plan. The Catalyst optimizer is the optimization engine that is used by Spark SQL to analyze and optimize query plans. The default value for spark.sql.analyzer.maxIterations is 100. This means that the Catalyst optimizer will perform up to 100 iterations to optimize a query plan. In most cases, this default value should be sufficient for most queries. However, in certain cases, increasing the value of spark.sql.analyzer.maxIterations may be beneficial. For example, if you have a particularly complex query or if the Catalyst optimizer is not able to find the optimal query plan in the default 100 iterations, then increasing this value may help. However, increasing the number of iterations will increase the amount of time required to analyze and optimize the query plan. It's worth noting that in some cases, an increase in the number of iterations does not necessary lead to a better optimized plan, it may lead to an increase in the cost of the plan instead, causing performance degradation. It is recommended to test different values for spark.sql.analyzer.maxIterations to find Create SQL user-defined table type \u00b6 To create a user-defined table type with the specified columns in SQL, you can use the following script: SQL CREATE TYPE dbo . MyTableType AS TABLE ( Security_ID INT , IsInit BIT , IsBuyList BIT , IsComposite BIT , IsPenalty BIT , PROJECT_NAME VARCHAR ( 255 ), CreatedDatetime DATETIME , CreatedBy VARCHAR ( 255 ), UpdatedDatetime DATETIME , UpdatedBy VARCHAR ( 255 ) ); This script creates a user-defined table type called dbo.MyTableType with the specified columns: Security_ID, IsInit, IsBuyList, IsComposite, IsPenalty, PROJECT_NAME, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy. The script uses the CREATE TYPE statement to create a new user-defined table type and the AS TABLE clause to define the structure of the table type. Each column is defined with a name and data type. You can use this table type as a parameter in stored procedures, User defined functions or even in other table types. SQL CREATE PROCEDURE sp_MyProcedure ( @ table_param dbo . MyTableType READONLY ) AS BEGIN SELECT * FROM @ table_param ; END","title":"\ud83d\udd13 Synapse / PySpark problem and solutions"},{"location":"synapse/problem-solutions/#synapse-pyspark-problem-and-solutions","text":"","title":"\ud83d\udd13 Synapse / PySpark problem and solutions"},{"location":"synapse/problem-solutions/#sql-server-to-pyspark-in-synapse","text":"To create a DataFrame in PySpark that reads data from a table in SQL Server using Synapse, you can use the following code as an example: Python from pyspark.sql import SparkSession # Create a SparkSession spark = SparkSession . builder \\ . appName ( \"Read from SQL Server table\" ) \\ . config ( \"spark.sql.databricks.directSQL.enabled\" , \"true\" ) \\ . config ( \"spark.databricks.io.cache.enabled\" , \"true\" ) \\ . config ( \"spark.databricks.io.cache.maxBytes\" , \"2g\" ) \\ . config ( \"spark.databricks.io.cache.ttl\" , \"10m\" ) \\ . getOrCreate () # Create a DataFrame that reads data from the specified SQL Server table table_name = \"[dbo].[your_table_name]\" url = \"jdbc:sqlserver://your_server_name.database.windows.net:1433;database=your_database_name\" properties = { \"user\" : \"your_username\" , \"password\" : \"your_password\" , \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" } df = spark . read . jdbc ( url = url , table = table_name , properties = properties ) # Show the DataFrame df . show () Please modify the values in the properties such as user, password, table name, server and database name accordingly to make sure the code works for you. You also have to have the sqlserver connector downloaded in your environment. Bash Session !pip install pyspark-sql-mssql","title":"Sql Server to PySpark in Synapse"},{"location":"synapse/problem-solutions/#check-file-exists-in-pyspark-or-python","text":"In PySpark, you can use the os.path module from the Python standard library to check if a file exists. Here is an example of how you can use the os.path.exists() function to check if a file exists: Python import os file_path = \"/path/to/your/file\" if os . path . exists ( file_path ): print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) Alternatively, you could also use os.path.isfile() function to check if a path points to a regular file. Python if os . path . isfile ( file_path ): print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) !!!note that in both cases, file_path should be a string representing the path to the file you want to check. Text Only You also can use spark.sql to check the existance of a file in hadoop filesystem, you can use the spark.sql(\"\"\"SHOW PARTITIONS tablename\"\"\") or spark.sql(\"\"\"describe formatted tablename\"\"\") to check if a file exist or not. Python from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"CheckFileExist\" ) . getOrCreate () file_path = \"/path/to/your/file\" results = spark . sql ( \"describe formatted {file_path} \" ) . collect () if results : print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) It will work for file that is in hadoop filesystem.","title":"Check file exists in PySpark or Python"},{"location":"synapse/problem-solutions/#check-file-in-adls","text":"To check if a file exists in Azure Data Lake Storage (ADLS) using PySpark, you can use the dbutils.fs module from the Databricks environment. The dbutils.fs.ls() function can be used to list the files in a directory, and you can check the length of the returned list to see if a file exists. Here is an example: Python from pyspark.dbutils import DBUtils file_path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/file\" dbutils = DBUtils ( spark ) results = dbutils . fs . ls ( file_path ) if len ( results ) > 0 : print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) Alternatively, you could also use dbutils.fs.exists(path) function to check file exist or not Python from pyspark.dbutils import DBUtils file_path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/file\" dbutils = DBUtils ( spark ) exist = dbutils . fs . exists ( file_path ) if exist : print ( \"File exists.\" ) else : print ( \"File does not exist.\" ) Please make sure that the connection to the ADLS is setup properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. You have to have the azure-storage and azure-storage-file-datalake package installed in your environment. Bash Session !pip install azure-storage !pip install azure-storage-file-datalake","title":"Check file in ADLS"},{"location":"synapse/problem-solutions/#try-catch-in-python","text":"In Python, try and except are used to handle exceptions, which are events that occur during the execution of a program that disrupt the normal flow of instructions. The try block contains the code that may raise an exception. If an exception is raised within the try block, the code in the corresponding except block will be executed. If no exception is raised, the code in the except block will be skipped. Here is an example of how to use the try and except statements in Python: Python try : # some code that may raise an exception result = 1 / 0 except ZeroDivisionError : # code to be executed if the exception is raised print ( \"Cannot divide by zero.\" ) In this example, the try block contains code that tries to divide 1 by 0, which raises a ZeroDivisionError exception. The except block contains code that prints a message if this exception is raised. You can also use finally block, which contains code that will be executed no matter whether an exception is raised or not. Python try : # some code that may raise an exception result = 1 / 0 except ZeroDivisionError : # code to be executed if the exception is raised print ( \"Cannot divide by zero.\" ) finally : print ( \"Closing any open resources\" ) You also can chain multiple except block, to handle different type of exception and different action correspondingly Python try : # some code that may raise an exception result = 1 / 0 except ZeroDivisionError : # code to be executed if ZeroDivisionError exception is raised print ( \"Cannot divide by zero.\" ) except Exception as e : # code to be executed if any exception other than ZeroDivisionError exception is raised print ( f \"an error occur : { e } \" ) It's a good practice to always handle the exceptions, to prevent the application from crashing, or unexpected behavior.","title":"Try catch in python"},{"location":"synapse/problem-solutions/#get-latest-file-from-datalake-using-pyspark-in-synapse","text":"You can use the dbutils.fs module from the Databricks environment in Synapse to get the latest file from a directory in Azure Data Lake Storage (ADLS). Here's an example of how you can do this: Python from pyspark.dbutils import DBUtils dbutils = DBUtils ( spark ) # directory path in ADLS dir_path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/directory\" # list all files in the directory files = dbutils . fs . ls ( dir_path ) # sort files by last modification time and get the latest file latest_file = sorted ( files , key = lambda x : x . modtime , reverse = True )[ 0 ] print ( f 'latest file is { latest_file . path } ' ) his code lists all the files in the specified directory in ADLS using the dbutils.fs.ls() function, then sorts the files by the last modification time in descending order, so that the latest file is at the top of the list. Then it access the first element of the list which is the latest file. It will print the path of the latest file. Please make sure that the connection to the ADLS is setup properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. You have to have the azure-storage and azure-storage-file-datalake package installed in your environment. Python ! pip install azure - storage ! pip install azure - storage - file - datalake You also can filter based on file type or regex on filename if needed before sorting the files Python files = dbutils . fs . ls ( dir_path ) files = filter ( lambda x : x . name . endswith ( '.csv' ), files ) latest_file = sorted ( files , key = lambda x : x . modtime , reverse = True )[ 0 ]","title":"Get latest file from datalake using pyspark in synapse"},{"location":"synapse/problem-solutions/#using-mssparkutils-in-synapse-to-get-latest-file-from-adls","text":"In Synapse, you can use the mssparkutils library to retrieve the latest file from Azure Data Lake Storage (ADLS) using PySpark. This library provides a getLatestFile() function that allows you to retrieve the latest file from a given directory in ADLS. Here's an example of how you can use the mssparkutils.fileio.datalake.getLatestFile() function to retrieve the latest file from ADLS: Python from mssparkutils.fileio.datalake import getLatestFile path = \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" latest_file = getLatestFile ( path ) print ( latest_file ) You can also specify a date pattern and/or a file extension to narrow down the search: Python latest_file = getLatestFile ( path , date_pattern = \"yyyyMMdd\" , extension = \".csv\" ) You have to have the mssparkutils package installed in your environment Python ! pip install mssparkutils This function use the dbutils module to get the files, so make sure that the connection to the ADLS is set up properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. This solution is a more robust and versatile approach, that considers both the lexicographical ordering of the files and their modification date to ensure that the correct latest file is returned.","title":"Using mssparkutils in synapse to get latest file from ADLS"},{"location":"synapse/problem-solutions/#how-to-partition-and-query-efficiently-parquet-file-in-data-lake-or-adls-using-pyspark","text":"Parquet is a columnar storage format that is well-suited for storing large, complex datasets. Partitioning the data in Parquet format can improve the efficiency of queries by allowing Spark to only scan the necessary data, rather than the entire dataset. Here's an example of how you can partition a DataFrame in PySpark and write it to a Parquet file in Azure Data Lake Storage (ADLS) using the partitionBy() method: Python from pyspark.sql.functions import * # Create a DataFrame df = spark . range ( 100 ) . selectExpr ( \"id\" , \"id % 10 as mod_10\" ) # Partition the DataFrame by the \"mod_10\" column df . write . partitionBy ( \"mod_10\" ) . parquet ( \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" ) Once the data is partitioned, you can use the filter() function to only scan the necessary partitions when querying the data: Python # read parquet data df_parquet = spark . read . parquet ( \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" ) # filter only data where `mod_10` is 5 df_parquet . filter ( col ( \"mod_10\" ) == 5 ) . show () You can also use predicate pushdown feature to push the filter conditions to the storage layer instead of doing the filtering in Spark, this approach further improves query performance by reducing the amount of data that needs to be read and processed. Python # read parquet data and enable predicate pushdown df_parquet = spark . read . parquet ( \"adl://yourstorageaccount.azuredatalakestore.net/path/to/your/directory\" ) . filter ( \"mod_10 = 5\" ) df_parquet . show () You also have to have the azure-storage and azure-storage-file-datalake package installed in your environment. Bash Session !pip install azure-storage !pip install azure-storage-file-datalake And make sure that the connection to the ADLS is set up properly, by providing the right credentials, such as client ID, client secret, tenant ID, and subscription ID or using service principle. By partitioning and filtering the data in this way, you can improve the performance of your queries by reducing the amount of data that Spark needs to scan and process. It's also important to mention that it would be best to be selective in the partitioning key, using a high cardinality column, such as date or timestamp, will increase the number of small partitions which will increase the number of metadata that needs to be scanned and may cause performance degradation rather than improvement.","title":"How to partition and query efficiently parquet file in data lake or ADLS using pyspark"},{"location":"synapse/problem-solutions/#what-should-be-the-value-for-sparksqlanalyzermaxiternations","text":"The spark.sql.analyzer.maxIterations configuration property sets the maximum number of iterations that Spark's Catalyst optimizer will perform when analyzing a query plan. The Catalyst optimizer is the optimization engine that is used by Spark SQL to analyze and optimize query plans. The default value for spark.sql.analyzer.maxIterations is 100. This means that the Catalyst optimizer will perform up to 100 iterations to optimize a query plan. In most cases, this default value should be sufficient for most queries. However, in certain cases, increasing the value of spark.sql.analyzer.maxIterations may be beneficial. For example, if you have a particularly complex query or if the Catalyst optimizer is not able to find the optimal query plan in the default 100 iterations, then increasing this value may help. However, increasing the number of iterations will increase the amount of time required to analyze and optimize the query plan. It's worth noting that in some cases, an increase in the number of iterations does not necessary lead to a better optimized plan, it may lead to an increase in the cost of the plan instead, causing performance degradation. It is recommended to test different values for spark.sql.analyzer.maxIterations to find","title":"What should be the value for spark.sql.analyzer.maxIternations"},{"location":"synapse/problem-solutions/#create-sql-user-defined-table-type","text":"To create a user-defined table type with the specified columns in SQL, you can use the following script: SQL CREATE TYPE dbo . MyTableType AS TABLE ( Security_ID INT , IsInit BIT , IsBuyList BIT , IsComposite BIT , IsPenalty BIT , PROJECT_NAME VARCHAR ( 255 ), CreatedDatetime DATETIME , CreatedBy VARCHAR ( 255 ), UpdatedDatetime DATETIME , UpdatedBy VARCHAR ( 255 ) ); This script creates a user-defined table type called dbo.MyTableType with the specified columns: Security_ID, IsInit, IsBuyList, IsComposite, IsPenalty, PROJECT_NAME, CreatedDatetime, CreatedBy, UpdatedDatetime, UpdatedBy. The script uses the CREATE TYPE statement to create a new user-defined table type and the AS TABLE clause to define the structure of the table type. Each column is defined with a name and data type. You can use this table type as a parameter in stored procedures, User defined functions or even in other table types. SQL CREATE PROCEDURE sp_MyProcedure ( @ table_param dbo . MyTableType READONLY ) AS BEGIN SELECT * FROM @ table_param ; END","title":"Create SQL user-defined table type"},{"location":"synapse/pyspark-azure-tables/","text":"\u2728 Creating tables in PySpark Azure \u00b6 Python from pyspark.sql import SparkSession from pyspark.sql.types import * spark = SparkSession \\ . builder \\ . appName ( \"Delong-Synapse\" ) \\ . getOrCreate () spark . range ( 1 , 2000 ) . count () Python # sc = spark._jsc.sc() # spark config spark_conf = spark . _conf # spark config https//spark.apache.org/docs/latest/configuration.html properties = [ 'spark.executor.cores' , 'spark.executor.instances' , 'spark.executor.memory' , 'spark.driver.maxResultSize' , 'spark.sql.warehouse.dir' # location for managed table data and metadata ] for prop in properties : print ( f ' { prop } : { spark_conf . get ( prop ) } ' ) Creating Tables \u00b6 Python # Azure storage access info account_name = 'td04agc96tgi00sa' container_name = 'test' # relative_path = '' file_name = 'sales.csv' linked_service_name = 'ls_blob' blob_sas_token = mssparkutils . credentials . getConnectionStringOrCreds ( linked_service_name ) wasbs_path = f 'wasbs:// { container_name } @ { account_name } .blob.core.windows.net/ { file_name } ' print ( wasbs_path ) spark . conf . set ( f 'fs.azure.sas. { container_name } . { account_name } .blob.core.windows.net' , blob_sas_token ) df = spark . read \\ . option ( 'header' , 'True' ) \\ . option ( 'inferSchema' , \"true\" ) \\ . csv ( wasbs_path ) display ( df ) Python from pyspark.sql.functions import to_date , col df_final = ( df . withColumn ( \"order_id\" , df [ \"Order ID\" ]) . drop ( \"Order ID\" ) . withColumn ( \"order_date\" , to_date ( col ( \"Order Date\" ), \"M/d/yyyy\" )) . drop ( \"Order Date\" ) . withColumn ( \"item_type\" , df [ \"Item Type\" ]) . drop ( \"Item Type\" ) . withColumn ( \"sales_channel\" , df [ \"Sales Channel\" ]) . drop ( \"Sales Channel\" ) . withColumn ( \"units_sold\" , df [ \"Units Sold\" ] . cast ( 'float' )) . drop ( \"Units Sold\" ) . withColumn ( \"unit_price\" , df [ \"Unit Price\" ] . cast ( 'float' )) . drop ( \"Unit Price\" ) . withColumn ( \"total_revenue\" , df [ \"Total Revenue\" ] . cast ( 'float' )) . drop ( \"Total Revenue\" ) . drop ( \"Region\" , \"Country\" , \"Order Priority\" , \"Ship Date\" , \"Total Profit\" , \"Total Cost\" , \"Unit Cost\" ) . distinct () ) display ( df_final ) Create temporary view and parquet file for managed and unmanaged tables Python df_final . createOrReplaceTempView ( 'df_final_view' ) spark . sql ( \"Select * from df_final_view\" ) . show ( 5 ) df_final . repartition ( 2 ) . write . mode ( \"overwrite\" ) . save ( \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/df_final.parquet\" ) Create Managed Tables \u00b6 Method 1 \u00b6 Python df_final . write . mode ( \"overwrite\" ) . saveAsTable ( \"salesTable_managed1\" ) # No path required spark . catalog . listTables () # list managed tables Method 2 \u00b6 SQL Command SQL CREATE TABLE IF NOT EXISTS salesTable_managed2 AS SELECT * FROM df_final_view Python # Or using spark.sql() spark . sql ( \"CREATE TABLE IF NOT EXISTS salesTable_managed2 AS SELECT * FROM df_final_view\" ) Method 3 \u00b6 Crate table and insert data. Use this approach if you have to change column types or replace or append data SQL CREATE TABLE salestable_managed3 ( order_id INT , order_date DATE , item_type STRING , sales_channel STRING , units_sold FLOAT , unit_price FLOAT , total_revenue FLOAT ); INSERT INTO salestable_managed3 SELECT * FROM df_final_view Python spark . catalog . listTables () Unmanaged Table \u00b6 Metod 1 \u00b6 Python df_final . repartition ( 2 ) . write . mode ( \"overwrite\" ) \\ . option ( \"path\" , \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/tables/salesTable_unmanaged1\" ) \\ . saveAsTable ( \"salesTable_unmanaged1\" ) Method 2 \u00b6 location as part of SQL query SQL CREATE EXTERNAL TABLE IF NOT EXISTS salesTable_unmanaged2 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 'wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/tables/salesTable_unmanaged2' AS SELECT * FROM df_final_view Method 3 \u00b6 If the table already exists in location. SQL CREATE TABLE salestable_unmanaged3 ( order_id INT , order_date DATE , item_type STRING , sales_channel STRING , units_sold FLOAT , unit_price FLOAT , total_revenue FLOAT ) USING DELTA OPTIONS ( path \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/tables/salesTable_unmanaged3\" ); Select * from salesTable_unmanaged3 limit 10 ; Python spark . catalog . listTables () Delete tables \u00b6 Python mssparkutils . fs . rm ( \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/\" , True ) Executing Drop command delete metadata from both type of tables and data from managed tables only wherease data will be preserved in the external locations SQL -- DROP TABLE salestable_managed1; -- DROP TABLE salestable_unmanaged1;","title":"\u2728 Creating tables in PySpark Azure"},{"location":"synapse/pyspark-azure-tables/#creating-tables-in-pyspark-azure","text":"Python from pyspark.sql import SparkSession from pyspark.sql.types import * spark = SparkSession \\ . builder \\ . appName ( \"Delong-Synapse\" ) \\ . getOrCreate () spark . range ( 1 , 2000 ) . count () Python # sc = spark._jsc.sc() # spark config spark_conf = spark . _conf # spark config https//spark.apache.org/docs/latest/configuration.html properties = [ 'spark.executor.cores' , 'spark.executor.instances' , 'spark.executor.memory' , 'spark.driver.maxResultSize' , 'spark.sql.warehouse.dir' # location for managed table data and metadata ] for prop in properties : print ( f ' { prop } : { spark_conf . get ( prop ) } ' )","title":"\u2728 Creating tables in PySpark Azure"},{"location":"synapse/pyspark-azure-tables/#creating-tables","text":"Python # Azure storage access info account_name = 'td04agc96tgi00sa' container_name = 'test' # relative_path = '' file_name = 'sales.csv' linked_service_name = 'ls_blob' blob_sas_token = mssparkutils . credentials . getConnectionStringOrCreds ( linked_service_name ) wasbs_path = f 'wasbs:// { container_name } @ { account_name } .blob.core.windows.net/ { file_name } ' print ( wasbs_path ) spark . conf . set ( f 'fs.azure.sas. { container_name } . { account_name } .blob.core.windows.net' , blob_sas_token ) df = spark . read \\ . option ( 'header' , 'True' ) \\ . option ( 'inferSchema' , \"true\" ) \\ . csv ( wasbs_path ) display ( df ) Python from pyspark.sql.functions import to_date , col df_final = ( df . withColumn ( \"order_id\" , df [ \"Order ID\" ]) . drop ( \"Order ID\" ) . withColumn ( \"order_date\" , to_date ( col ( \"Order Date\" ), \"M/d/yyyy\" )) . drop ( \"Order Date\" ) . withColumn ( \"item_type\" , df [ \"Item Type\" ]) . drop ( \"Item Type\" ) . withColumn ( \"sales_channel\" , df [ \"Sales Channel\" ]) . drop ( \"Sales Channel\" ) . withColumn ( \"units_sold\" , df [ \"Units Sold\" ] . cast ( 'float' )) . drop ( \"Units Sold\" ) . withColumn ( \"unit_price\" , df [ \"Unit Price\" ] . cast ( 'float' )) . drop ( \"Unit Price\" ) . withColumn ( \"total_revenue\" , df [ \"Total Revenue\" ] . cast ( 'float' )) . drop ( \"Total Revenue\" ) . drop ( \"Region\" , \"Country\" , \"Order Priority\" , \"Ship Date\" , \"Total Profit\" , \"Total Cost\" , \"Unit Cost\" ) . distinct () ) display ( df_final ) Create temporary view and parquet file for managed and unmanaged tables Python df_final . createOrReplaceTempView ( 'df_final_view' ) spark . sql ( \"Select * from df_final_view\" ) . show ( 5 ) df_final . repartition ( 2 ) . write . mode ( \"overwrite\" ) . save ( \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/df_final.parquet\" )","title":"Creating Tables"},{"location":"synapse/pyspark-azure-tables/#create-managed-tables","text":"","title":"Create Managed Tables"},{"location":"synapse/pyspark-azure-tables/#method-1","text":"Python df_final . write . mode ( \"overwrite\" ) . saveAsTable ( \"salesTable_managed1\" ) # No path required spark . catalog . listTables () # list managed tables","title":"Method 1"},{"location":"synapse/pyspark-azure-tables/#method-2","text":"SQL Command SQL CREATE TABLE IF NOT EXISTS salesTable_managed2 AS SELECT * FROM df_final_view Python # Or using spark.sql() spark . sql ( \"CREATE TABLE IF NOT EXISTS salesTable_managed2 AS SELECT * FROM df_final_view\" )","title":"Method 2"},{"location":"synapse/pyspark-azure-tables/#method-3","text":"Crate table and insert data. Use this approach if you have to change column types or replace or append data SQL CREATE TABLE salestable_managed3 ( order_id INT , order_date DATE , item_type STRING , sales_channel STRING , units_sold FLOAT , unit_price FLOAT , total_revenue FLOAT ); INSERT INTO salestable_managed3 SELECT * FROM df_final_view Python spark . catalog . listTables ()","title":"Method 3"},{"location":"synapse/pyspark-azure-tables/#unmanaged-table","text":"","title":"Unmanaged Table"},{"location":"synapse/pyspark-azure-tables/#metod-1","text":"Python df_final . repartition ( 2 ) . write . mode ( \"overwrite\" ) \\ . option ( \"path\" , \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/tables/salesTable_unmanaged1\" ) \\ . saveAsTable ( \"salesTable_unmanaged1\" )","title":"Metod 1"},{"location":"synapse/pyspark-azure-tables/#method-2_1","text":"location as part of SQL query SQL CREATE EXTERNAL TABLE IF NOT EXISTS salesTable_unmanaged2 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 'wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/tables/salesTable_unmanaged2' AS SELECT * FROM df_final_view","title":"Method 2"},{"location":"synapse/pyspark-azure-tables/#method-3_1","text":"If the table already exists in location. SQL CREATE TABLE salestable_unmanaged3 ( order_id INT , order_date DATE , item_type STRING , sales_channel STRING , units_sold FLOAT , unit_price FLOAT , total_revenue FLOAT ) USING DELTA OPTIONS ( path \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/tables/salesTable_unmanaged3\" ); Select * from salesTable_unmanaged3 limit 10 ; Python spark . catalog . listTables ()","title":"Method 3"},{"location":"synapse/pyspark-azure-tables/#delete-tables","text":"Python mssparkutils . fs . rm ( \"wasbs://test@td04agc96tgi00sa.blob.core.windows.net/filestore/\" , True ) Executing Drop command delete metadata from both type of tables and data from managed tables only wherease data will be preserved in the external locations SQL -- DROP TABLE salestable_managed1; -- DROP TABLE salestable_unmanaged1;","title":"Delete tables"},{"location":"synapse/pyspark-cheat-sheet/","text":"\ud83d\udcc4 PySpark Cheat Sheet \u00b6 A quick reference guide to the most commonly used patterns and functions in PySpark SQL. If you can't find what you're looking for, check out the PySpark Official Documentation and add it here! Common Patterns \u00b6 Importing Functions & Types \u00b6 Python # Easily reference these as F.my_function() and T.my_type() below from pyspark.sql import functions as F , types as T Filtering \u00b6 Python # Filter on equals condition df = df . filter ( df . is_adult == 'Y' ) # Filter on >, <, >=, <= condition df = df . filter ( df . age > 25 ) # Multiple conditions require parentheses around each condition df = df . filter (( df . age > 25 ) & ( df . is_adult == 'Y' )) # Compare against a list of allowed values df = df . filter ( col ( 'first_name' ) . isin ([ 3 , 4 , 7 ])) # Sort results df = df . orderBy ( df . age . asc ())) df = df . orderBy ( df . age . desc ())) Joins \u00b6 Python # Left join in another dataset df = df . join ( person_lookup_table , 'person_id' , 'left' ) # Match on different columns in left & right datasets df = df . join ( other_table , df . id == other_table . person_id , 'left' ) # Match on multiple columns df = df . join ( other_table , [ 'first_name' , 'last_name' ], 'left' ) # Useful for one-liner lookup code joins if you have a bunch def lookup_and_replace ( df1 , df2 , df1_key , df2_key , df2_value ): return ( df1 . join ( df2 [[ df2_key , df2_value ]], df1 [ df1_key ] == df2 [ df2_key ], 'left' ) . withColumn ( df1_key , F . coalesce ( F . col ( df2_value ), F . col ( df1_key ))) . drop ( df2_key ) . drop ( df2_value ) ) df = lookup_and_replace ( people , pay_codes , id , pay_code_id , pay_code_desc ) Column Operations \u00b6 Python # Add a new static column df = df . withColumn ( 'status' , F . lit ( 'PASS' )) # Construct a new dynamic column df = df . withColumn ( 'full_name' , F . when ( ( df . fname . isNotNull () & df . lname . isNotNull ()), F . concat ( df . fname , df . lname ) ) . otherwise ( F . lit ( 'N/A' )) # Pick which columns to keep, optionally rename some df = df . select ( 'name' , 'age' , F . col ( 'dob' ) . alias ( 'date_of_birth' ), ) # Remove columns df = df . drop ( 'mod_dt' , 'mod_username' ) # Rename a column df = df . withColumnRenamed ( 'dob' , 'date_of_birth' ) # Keep all the columns which also occur in another dataset df = df . select ( * ( F . col ( c ) for c in df2 . columns )) # Batch Rename/Clean Columns for col in df . columns : df = df . withColumnRenamed ( col , col . lower () . replace ( ' ' , '_' ) . replace ( '-' , '_' )) Casting & Coalescing Null Values & Duplicates \u00b6 Python # Cast a column to a different type df = df . withColumn ( 'price' , df . price . cast ( T . DoubleType ())) # Replace all nulls with a specific value df = df . fillna ({ 'first_name' : 'Tom' , 'age' : 0 , }) # Take the first value that is not null df = df . withColumn ( 'last_name' , F . coalesce ( df . last_name , df . surname , F . lit ( 'N/A' ))) # Drop duplicate rows in a dataset (distinct) df = df . dropDuplicates () # Drop duplicate rows, but consider only specific columns df = df . dropDuplicates ([ 'name' , 'height' ]) # Replace empty strings with null (leave out subset keyword arg to replace in all columns) df = df . replace ({ \"\" : None }, subset = [ \"name\" ]) # Convert Python/PySpark/NumPy NaN operator to null df = df . replace ( float ( \"nan\" ), None ) String Operations \u00b6 String Filters \u00b6 Python # Contains - col.contains(string) df = df . filter ( df . name . contains ( 'o' )) # Starts With - col.startswith(string) df = df . filter ( df . name . startswith ( 'Al' )) # Ends With - col.endswith(string) df = df . filter ( df . name . endswith ( 'ice' )) # Is Null - col.isNull() df = df . filter ( df . is_adult . isNull ()) # Is Not Null - col.isNotNull() df = df . filter ( df . first_name . isNotNull ()) # Like - col.like(string_with_sql_wildcards) df = df . filter ( df . name . like ( 'Al%' )) # Regex Like - col.rlike(regex) df = df . filter ( df . name . rlike ( '[A-Z]*ice$' )) # Is In List - col.isin(*cols) df = df . filter ( df . name . isin ( 'Bob' , 'Mike' )) String Functions \u00b6 Python # Substring - col.substr(startPos, length) df = df . withColumn ( 'short_id' , df . id . substr ( 0 , 10 )) # Trim - F.trim(col) df = df . withColumn ( 'name' , F . trim ( df . name )) # Left Pad - F.lpad(col, len, pad) # Right Pad - F.rpad(col, len, pad) df = df . withColumn ( 'id' , F . lpad ( 'id' , 4 , '0' )) # Left Trim - F.ltrim(col) # Right Trim - F.rtrim(col) df = df . withColumn ( 'id' , F . ltrim ( 'id' )) # Concatenate - F.concat(*cols) df = df . withColumn ( 'full_name' , F . concat ( 'fname' , F . lit ( ' ' ), 'lname' )) # Concatenate with Separator/Delimiter - F.concat_ws(delimiter, *cols) df = df . withColumn ( 'full_name' , F . concat_ws ( '-' , 'fname' , 'lname' )) # Regex Replace - F.regexp_replace(str, pattern, replacement)[source] df = df . withColumn ( 'id' , F . regexp_replace ( id , '0F1(.*)' , '1F1-$1' )) # Regex Extract - F.regexp_extract(str, pattern, idx) df = df . withColumn ( 'id' , F . regexp_extract ( id , '[0-9]*' , 0 )) Number Operations \u00b6 Python # Round - F.round(col, scale=0) df = df . withColumn ( 'price' , F . round ( 'price' , 0 )) # Floor - F.floor(col) df = df . withColumn ( 'price' , F . floor ( 'price' )) # Ceiling - F.ceil(col) df = df . withColumn ( 'price' , F . ceil ( 'price' )) # Absolute Value - F.abs(col) df = df . withColumn ( 'price' , F . abs ( 'price' )) # X raised to power Y \u2013 F.pow(x, y) df = df . withColumn ( 'exponential_growth' , F . pow ( 'x' , 'y' )) # Select smallest value out of multiple columns \u2013 F.least(*cols) df = df . withColumn ( 'least' , F . least ( 'subtotal' , 'total' )) # Select largest value out of multiple columns \u2013 F.greatest(*cols) df = df . withColumn ( 'greatest' , F . greatest ( 'subtotal' , 'total' )) Date & Timestamp Operations \u00b6 Python # Convert a string of known format to a date (excludes time information) df = df . withColumn ( 'date_of_birth' , F . to_date ( 'date_of_birth' , 'yyyy-MM-dd' )) # Convert a string of known format to a timestamp (includes time information) df = df . withColumn ( 'time_of_birth' , F . to_timestamp ( 'time_of_birth' , 'yyyy-MM-dd HH:mm:ss' )) # Get year from date: F.year(col) # Get month from date: F.month(col) # Get day from date: F.dayofmonth(col) # Get hour from date: F.hour(col) # Get minute from date: F.minute(col) # Get second from date: F.second(col) df = df . filter ( F . year ( 'date_of_birth' ) == F . lit ( '2017' )) # Add & subtract days df = df . withColumn ( 'three_days_after' , F . date_add ( 'date_of_birth' , 3 )) df = df . withColumn ( 'three_days_before' , F . date_sub ( 'date_of_birth' , 3 )) # Add & Subtract months df = df . withColumn ( 'next_month' , F . add_month ( 'date_of_birth' , 1 )) # Get number of days between two dates df = df . withColumn ( 'days_between' , F . datediff ( 'start' , 'end' )) # Get number of months between two dates df = df . withColumn ( 'months_between' , F . months_between ( 'start' , 'end' )) # Keep only rows where date_of_birth is between 2017-05-10 and 2018-07-21 df = df . filter ( ( F . col ( 'date_of_birth' ) >= F . lit ( '2017-05-10' )) & ( F . col ( 'date_of_birth' ) <= F . lit ( '2018-07-21' )) ) Array Operations \u00b6 Python # Column Array - F.array(*cols) df = df . withColumn ( 'full_name' , F . array ( 'fname' , 'lname' )) # Empty Array - F.array(*cols) df = df . withColumn ( 'empty_array_column' , F . array ([])) # Array Size/Length \u2013 F.size(col) df = df . withColumn ( 'array_length' , F . size ( 'my_array' )) # Flatten Array \u2013 F.flatten(col) df = df . withColumn ( 'flattened' , F . flatten ( 'my_array' )) # Unique/Distinct Elements \u2013 F.array_distinct(col) df = df . withColumn ( 'unique_elements' , F . array_distinct ( 'my_array' )) Aggregation Operations \u00b6 Python # Row Count: F.count() # Sum of Rows in Group: F.sum(*cols) # Mean of Rows in Group: F.mean(*cols) # Max of Rows in Group: F.max(*cols) # Min of Rows in Group: F.min(*cols) # First Row in Group: F.alias(*cols) df = df . groupBy ( 'gender' ) . agg ( F . max ( 'age' ) . alias ( 'max_age_by_gender' )) # Collect a Set of all Rows in Group: F.collect_set(col) # Collect a List of all Rows in Group: F.collect_list(col) df = df . groupBy ( 'age' ) . agg ( F . collect_set ( 'name' ) . alias ( 'person_names' )) # Just take the lastest row for each combination (Window Functions) from pyspark.sql import Window as W window = W . partitionBy ( \"first_name\" , \"last_name\" ) . orderBy ( F . desc ( \"date\" )) df = df . withColumn ( \"row_number\" , F . row_number () . over ( window )) df = df . filter ( F . col ( \"row_number\" ) == 1 ) df = df . drop ( \"row_number\" ) Advanced Operations \u00b6 Repartitioning \u00b6 Python # Repartition \u2013 df.repartition(num_output_partitions) df = df . repartition ( 1 ) UDFs (User Defined Functions \u00b6 Python # Multiply each row's age column by two times_two_udf = F . udf ( lambda x : x * 2 ) df = df . withColumn ( 'age' , times_two_udf ( df . age )) # Randomly choose a value to use as a row's name import random random_name_udf = F . udf ( lambda : random . choice ([ 'Bob' , 'Tom' , 'Amy' , 'Jenna' ])) df = df . withColumn ( 'name' , random_name_udf ())","title":"\ud83d\udcc4 PySpark Cheat Sheet"},{"location":"synapse/pyspark-cheat-sheet/#pyspark-cheat-sheet","text":"A quick reference guide to the most commonly used patterns and functions in PySpark SQL. If you can't find what you're looking for, check out the PySpark Official Documentation and add it here!","title":"\ud83d\udcc4 PySpark Cheat Sheet"},{"location":"synapse/pyspark-cheat-sheet/#common-patterns","text":"","title":"Common Patterns"},{"location":"synapse/pyspark-cheat-sheet/#importing-functions-types","text":"Python # Easily reference these as F.my_function() and T.my_type() below from pyspark.sql import functions as F , types as T","title":"Importing Functions &amp; Types"},{"location":"synapse/pyspark-cheat-sheet/#filtering","text":"Python # Filter on equals condition df = df . filter ( df . is_adult == 'Y' ) # Filter on >, <, >=, <= condition df = df . filter ( df . age > 25 ) # Multiple conditions require parentheses around each condition df = df . filter (( df . age > 25 ) & ( df . is_adult == 'Y' )) # Compare against a list of allowed values df = df . filter ( col ( 'first_name' ) . isin ([ 3 , 4 , 7 ])) # Sort results df = df . orderBy ( df . age . asc ())) df = df . orderBy ( df . age . desc ()))","title":"Filtering"},{"location":"synapse/pyspark-cheat-sheet/#joins","text":"Python # Left join in another dataset df = df . join ( person_lookup_table , 'person_id' , 'left' ) # Match on different columns in left & right datasets df = df . join ( other_table , df . id == other_table . person_id , 'left' ) # Match on multiple columns df = df . join ( other_table , [ 'first_name' , 'last_name' ], 'left' ) # Useful for one-liner lookup code joins if you have a bunch def lookup_and_replace ( df1 , df2 , df1_key , df2_key , df2_value ): return ( df1 . join ( df2 [[ df2_key , df2_value ]], df1 [ df1_key ] == df2 [ df2_key ], 'left' ) . withColumn ( df1_key , F . coalesce ( F . col ( df2_value ), F . col ( df1_key ))) . drop ( df2_key ) . drop ( df2_value ) ) df = lookup_and_replace ( people , pay_codes , id , pay_code_id , pay_code_desc )","title":"Joins"},{"location":"synapse/pyspark-cheat-sheet/#column-operations","text":"Python # Add a new static column df = df . withColumn ( 'status' , F . lit ( 'PASS' )) # Construct a new dynamic column df = df . withColumn ( 'full_name' , F . when ( ( df . fname . isNotNull () & df . lname . isNotNull ()), F . concat ( df . fname , df . lname ) ) . otherwise ( F . lit ( 'N/A' )) # Pick which columns to keep, optionally rename some df = df . select ( 'name' , 'age' , F . col ( 'dob' ) . alias ( 'date_of_birth' ), ) # Remove columns df = df . drop ( 'mod_dt' , 'mod_username' ) # Rename a column df = df . withColumnRenamed ( 'dob' , 'date_of_birth' ) # Keep all the columns which also occur in another dataset df = df . select ( * ( F . col ( c ) for c in df2 . columns )) # Batch Rename/Clean Columns for col in df . columns : df = df . withColumnRenamed ( col , col . lower () . replace ( ' ' , '_' ) . replace ( '-' , '_' ))","title":"Column Operations"},{"location":"synapse/pyspark-cheat-sheet/#casting-coalescing-null-values-duplicates","text":"Python # Cast a column to a different type df = df . withColumn ( 'price' , df . price . cast ( T . DoubleType ())) # Replace all nulls with a specific value df = df . fillna ({ 'first_name' : 'Tom' , 'age' : 0 , }) # Take the first value that is not null df = df . withColumn ( 'last_name' , F . coalesce ( df . last_name , df . surname , F . lit ( 'N/A' ))) # Drop duplicate rows in a dataset (distinct) df = df . dropDuplicates () # Drop duplicate rows, but consider only specific columns df = df . dropDuplicates ([ 'name' , 'height' ]) # Replace empty strings with null (leave out subset keyword arg to replace in all columns) df = df . replace ({ \"\" : None }, subset = [ \"name\" ]) # Convert Python/PySpark/NumPy NaN operator to null df = df . replace ( float ( \"nan\" ), None )","title":"Casting &amp; Coalescing Null Values &amp; Duplicates"},{"location":"synapse/pyspark-cheat-sheet/#string-operations","text":"","title":"String Operations"},{"location":"synapse/pyspark-cheat-sheet/#string-filters","text":"Python # Contains - col.contains(string) df = df . filter ( df . name . contains ( 'o' )) # Starts With - col.startswith(string) df = df . filter ( df . name . startswith ( 'Al' )) # Ends With - col.endswith(string) df = df . filter ( df . name . endswith ( 'ice' )) # Is Null - col.isNull() df = df . filter ( df . is_adult . isNull ()) # Is Not Null - col.isNotNull() df = df . filter ( df . first_name . isNotNull ()) # Like - col.like(string_with_sql_wildcards) df = df . filter ( df . name . like ( 'Al%' )) # Regex Like - col.rlike(regex) df = df . filter ( df . name . rlike ( '[A-Z]*ice$' )) # Is In List - col.isin(*cols) df = df . filter ( df . name . isin ( 'Bob' , 'Mike' ))","title":"String Filters"},{"location":"synapse/pyspark-cheat-sheet/#string-functions","text":"Python # Substring - col.substr(startPos, length) df = df . withColumn ( 'short_id' , df . id . substr ( 0 , 10 )) # Trim - F.trim(col) df = df . withColumn ( 'name' , F . trim ( df . name )) # Left Pad - F.lpad(col, len, pad) # Right Pad - F.rpad(col, len, pad) df = df . withColumn ( 'id' , F . lpad ( 'id' , 4 , '0' )) # Left Trim - F.ltrim(col) # Right Trim - F.rtrim(col) df = df . withColumn ( 'id' , F . ltrim ( 'id' )) # Concatenate - F.concat(*cols) df = df . withColumn ( 'full_name' , F . concat ( 'fname' , F . lit ( ' ' ), 'lname' )) # Concatenate with Separator/Delimiter - F.concat_ws(delimiter, *cols) df = df . withColumn ( 'full_name' , F . concat_ws ( '-' , 'fname' , 'lname' )) # Regex Replace - F.regexp_replace(str, pattern, replacement)[source] df = df . withColumn ( 'id' , F . regexp_replace ( id , '0F1(.*)' , '1F1-$1' )) # Regex Extract - F.regexp_extract(str, pattern, idx) df = df . withColumn ( 'id' , F . regexp_extract ( id , '[0-9]*' , 0 ))","title":"String Functions"},{"location":"synapse/pyspark-cheat-sheet/#number-operations","text":"Python # Round - F.round(col, scale=0) df = df . withColumn ( 'price' , F . round ( 'price' , 0 )) # Floor - F.floor(col) df = df . withColumn ( 'price' , F . floor ( 'price' )) # Ceiling - F.ceil(col) df = df . withColumn ( 'price' , F . ceil ( 'price' )) # Absolute Value - F.abs(col) df = df . withColumn ( 'price' , F . abs ( 'price' )) # X raised to power Y \u2013 F.pow(x, y) df = df . withColumn ( 'exponential_growth' , F . pow ( 'x' , 'y' )) # Select smallest value out of multiple columns \u2013 F.least(*cols) df = df . withColumn ( 'least' , F . least ( 'subtotal' , 'total' )) # Select largest value out of multiple columns \u2013 F.greatest(*cols) df = df . withColumn ( 'greatest' , F . greatest ( 'subtotal' , 'total' ))","title":"Number Operations"},{"location":"synapse/pyspark-cheat-sheet/#date-timestamp-operations","text":"Python # Convert a string of known format to a date (excludes time information) df = df . withColumn ( 'date_of_birth' , F . to_date ( 'date_of_birth' , 'yyyy-MM-dd' )) # Convert a string of known format to a timestamp (includes time information) df = df . withColumn ( 'time_of_birth' , F . to_timestamp ( 'time_of_birth' , 'yyyy-MM-dd HH:mm:ss' )) # Get year from date: F.year(col) # Get month from date: F.month(col) # Get day from date: F.dayofmonth(col) # Get hour from date: F.hour(col) # Get minute from date: F.minute(col) # Get second from date: F.second(col) df = df . filter ( F . year ( 'date_of_birth' ) == F . lit ( '2017' )) # Add & subtract days df = df . withColumn ( 'three_days_after' , F . date_add ( 'date_of_birth' , 3 )) df = df . withColumn ( 'three_days_before' , F . date_sub ( 'date_of_birth' , 3 )) # Add & Subtract months df = df . withColumn ( 'next_month' , F . add_month ( 'date_of_birth' , 1 )) # Get number of days between two dates df = df . withColumn ( 'days_between' , F . datediff ( 'start' , 'end' )) # Get number of months between two dates df = df . withColumn ( 'months_between' , F . months_between ( 'start' , 'end' )) # Keep only rows where date_of_birth is between 2017-05-10 and 2018-07-21 df = df . filter ( ( F . col ( 'date_of_birth' ) >= F . lit ( '2017-05-10' )) & ( F . col ( 'date_of_birth' ) <= F . lit ( '2018-07-21' )) )","title":"Date &amp; Timestamp Operations"},{"location":"synapse/pyspark-cheat-sheet/#array-operations","text":"Python # Column Array - F.array(*cols) df = df . withColumn ( 'full_name' , F . array ( 'fname' , 'lname' )) # Empty Array - F.array(*cols) df = df . withColumn ( 'empty_array_column' , F . array ([])) # Array Size/Length \u2013 F.size(col) df = df . withColumn ( 'array_length' , F . size ( 'my_array' )) # Flatten Array \u2013 F.flatten(col) df = df . withColumn ( 'flattened' , F . flatten ( 'my_array' )) # Unique/Distinct Elements \u2013 F.array_distinct(col) df = df . withColumn ( 'unique_elements' , F . array_distinct ( 'my_array' ))","title":"Array Operations"},{"location":"synapse/pyspark-cheat-sheet/#aggregation-operations","text":"Python # Row Count: F.count() # Sum of Rows in Group: F.sum(*cols) # Mean of Rows in Group: F.mean(*cols) # Max of Rows in Group: F.max(*cols) # Min of Rows in Group: F.min(*cols) # First Row in Group: F.alias(*cols) df = df . groupBy ( 'gender' ) . agg ( F . max ( 'age' ) . alias ( 'max_age_by_gender' )) # Collect a Set of all Rows in Group: F.collect_set(col) # Collect a List of all Rows in Group: F.collect_list(col) df = df . groupBy ( 'age' ) . agg ( F . collect_set ( 'name' ) . alias ( 'person_names' )) # Just take the lastest row for each combination (Window Functions) from pyspark.sql import Window as W window = W . partitionBy ( \"first_name\" , \"last_name\" ) . orderBy ( F . desc ( \"date\" )) df = df . withColumn ( \"row_number\" , F . row_number () . over ( window )) df = df . filter ( F . col ( \"row_number\" ) == 1 ) df = df . drop ( \"row_number\" )","title":"Aggregation Operations"},{"location":"synapse/pyspark-cheat-sheet/#advanced-operations","text":"","title":"Advanced Operations"},{"location":"synapse/pyspark-cheat-sheet/#repartitioning","text":"Python # Repartition \u2013 df.repartition(num_output_partitions) df = df . repartition ( 1 )","title":"Repartitioning"},{"location":"synapse/pyspark-cheat-sheet/#udfs-user-defined-functions","text":"Python # Multiply each row's age column by two times_two_udf = F . udf ( lambda x : x * 2 ) df = df . withColumn ( 'age' , times_two_udf ( df . age )) # Randomly choose a value to use as a row's name import random random_name_udf = F . udf ( lambda : random . choice ([ 'Bob' , 'Tom' , 'Amy' , 'Jenna' ])) df = df . withColumn ( 'name' , random_name_udf ())","title":"UDFs (User Defined Functions"},{"location":"synapse/pyspark-courses/","text":"\ud83d\udd05 Courses and Tutorials \u00b6 NoSQL, Big Data, and Spark Foundations Specialization \u00b6 Introduction to NoSQL Databases Introduction to Big Data with Spark and Hadoop Week 1: What is Big Data? Reading: Course Introduction Video: What is Big Data? Video: Impact of Big Data Video: Parallel Processing, Scaling, and Data Parallelism Video: Big Data Tools and Ecosystem Video: Open Source and Big Data Video: Beyond the Hype Video: Big Data Use Cases Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Big Data Week 2: Introduction to the Hadoop Ecosystem Video: Introduction to Hadoop Video: Intro to MapReduce Video: Hadoop Ecosystem Video: HDFS Video: HIVE Video: HBASE LTI Item: Hands-on Lab: Hadoop MapReduce LTI Item: Hands-on lab : Hadoop Cluster (Optional) Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Hadoop Week 3: Apache Spark Video: Why use Apache Spark? Video: Functional Programming Basics Video: Parallel Programming using Resilient Distributed Datasets Video: Scale out / Data Parallelism in Apache Spark Video: Dataframes and SparkSQL LTI Item: Hands-on Lab: Getting Started with Spark using Python Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Apache Spark Week 4: DataFrames and SparkSQL Video: RDDs in Parallel Programming and Spark Video: Data-frames and Datasets Video: Catalyst and Tungsten Video: ETL with DataFrames LTI Item: Hands-on Lab: Introduction to Data-Frames Video: Real-world usage of SparkSQL LTI Item: Hands-On Lab: Introduction to SparkSQL Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Data-Frames & SparkSQL Week 5: Development and Runtime Environment Options Video: Apache Spark Architecture Video: Overview of Apache Spark Cluster Modes Video: How to Run an Apache Spark Application LTI Item: Hands-on Lab: Submit Apache Spark Applications Reading: Summary & Highlights Practice Quiz: Practice Quiz: Spark Architecture Video: Using Apache Spark on IBM Cloud LTI Item: Activate Trial Account Ungraded Plugin: Hands-on Lab: Getting started with Spark on IBM Cloud Video: Setting Apache Spark Configuration Video: Running Spark on Kubernetes LTI Item: Hands-on Lab: Apache Spark on Kubernetes Reading: Summary & Highlights Practice Quiz: Practice Quiz: Spark Runtime Environments Week 6: Monitoring & Tuning Video: The Apache Spark User Interface Video: Monitoring Application Progress Video: Debugging Apache Spark Application Issues Video: Understanding Memory Resources Video: Understanding Processor Resources LTI Item: Hands-on Lab: Monitoring and Performance Tuning Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Monitoring & Tuning Reading: Instructions for the Final Exam Reading: Congrats & Next Steps Reading: Team & Acknowledgements Data Engineering and Machine Learning using Spark Week 1: Spark for Data Engineering Reading: Course Introduction Video: Spark Structured Streaming Video: GraphFrames on Apache Spark Video: ETL Workloads Video: Introduction to the pipeline editor in Elyra (Optional) Ungraded Plugin: Reading: Create component oriented data science pipelines using CLAIMED, - Elyra, KubeFlow Pipelines, MLX and Kubernetes LTI Item: Hands-on Lab: ETL using Apache Spark Reading: Summary & Highlights Practice Quiz: Practice Quiz: Spark for Data Engineering Week 2: SparkML Video: SparkML Fundamentals Video: Classification and Regression using Apache Spark Video: SparkML Clustering LTI Item: Obtain an IBM Cloud Feature Code Ungraded Plugin: Jupyter Notebook for Hands-on Lab: Machine Learning with Apache Spark ML LTI Item: Optional: Hands on Lab: Introduction to SparkML Reading: Summary & Highlights Practice Quiz: Practice Quiz: SparkML Week 3: Final Project Reading: Project Overview LTI Item: Hands-on Lab: ETL and Machine Learning Reading: Congratulations & Next Steps Reading: Team & Acknowledgements Data Engineering with MS Azure Synapse Apache Spark Pools \u00b6 Week 1: Big Data Engineering Video: Introduction to the course Reading: Course syllabus Reading: How to be successful in this course Discussion Prompt: Meet and greet Video: What is an Apache Spark pool in Azure Synapse Analytics? Video: How do Apache Spark pools in Azure Synapse Analytics? Reading: When do you use Apache Spark pools in Azure Synapse Analytics? Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to spark notebooks Video: Understand the use-cases for spark notebooks Reading: Create a spark notebook in Azure Synapse Analytics Reading: Discover supported languages in spark notebooks Reading: Develop spark notebooks Reading: Develop spark notebooks Video: Run spark notebooks Reading: Run spark notebooks Reading: Load data in Spark notebooks Reading: Load data in Spark notebooks Reading: Save Spark notebooks Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to DataFrames in Spark pools in Azure Synapse Analytics Video: Load data in a Spark DataFrame Reading: Load data into a Spark DataFrame Reading: Create an Apache Spark table Video: Flatten nested structures and explode arrays with Apache Spark Reading: Flatten nested structures and explode arrays with Apache Spark in synapse Practice Quiz: Knowledge check Video: Lesson summary Week 2: Query pools and manage workloads in Azure Synapse Analytics Video: Describe the integration methods between SQL and Spark pools in Azure Synapse Analytics Video: Understand the use-cases for SQL and Spark pools integration Video: Authenticate in Azure Synapse Analytics Reading: Transfer data between SQL and Spark pool in Azure Synapse Analytics Reading: Authenticate between Spark and SQL pool in Azure Synapse Analytics Reading: Integrate SQL and Spark pools in Azure Synapse Analytics Video: Externalize the use of Spark pools within Azure Synapse Workspace Reading: Transfer data outside the Synapse workspace using the PySpark connector Video: Transfer data outside the Synapse workspace using the PySpark connector Practice Quiz: Knowledge check Video: Lesson summary Video: Monitor Spark pools in Azure Synapse Analytics Reading: Base-line Apache Spark performance with Apache Spark history server in Azure Synaps- Analytics Video: Optimize Apache Spark jobs in Azure Synapse Analytics Reading: Automate scaling of Apache Spark pools in Azure Synapse Analytics Practice Quiz: Knowledge check Video: Lesson summary Week 3: Practice Exam on Perform data engineering with Azure Synapse Apache Spark Pools Reading: About the practice exam Video: Course 6 recap Video: Course wrap up Discussion Prompt: Reflect on learning Reading: Next steps Data Science with Databricks for Data Analysts Specialization \u00b6 Apache Spark (TM) SQL for Data Analysts Week 1: Welcome to Apache Spark SQL for Data Analysts 1 video Course goals1m 1 reading Before you begin5m Week 2: Spark makes big data easy 6 videos Introduction to module 21m What is big data?6m Common struggles with big data4m Big Data Needs2m Apache Spark Intro3m Spark SQL2m 1 practice exercise Module 2 Concept Review30m Week 3: Using Spark SQL on Databricks 9 videos Introduction to Module 31m Signing up for Databricks Community Edition1m Preparing your workspace2m Working with notebooks3m Using course materials6m Basic queries with Spark SQL reading introduction1m Data Visualization on Databricks reading introduction38s Data visualization tools1m Exploratory Data Analysis lab introduction25s 4 readings Course Materials5m Basic Queries reading activity30m Data Visualization reading activity30m Your turn! Exploratory Data Analysis lab30m Week 4: Spark Under the Hood 7 videos Introduction to module 454s Understanding optimizations7m The physical cluster3m The SparkUI and SQL tab2m Optimizing query logic4m Impact of Caching6m Optimizing with selective data loading6m 1 practice exercise Module 4 Concept Review30m Week 5: Complex Queries 5 videos Introduction to module 559s What is nested data? 2m Introduction to managing nested data1m Introduction to Manipulating Data 21s Introduction to Data Munging57s 3 readings Managing Nested Data reading activity30m Manipulating Data reading activity30m 5.3 Data Munging Lab30m Week 6: Applied Spark SQL 7 videos Introduction to module 61m Complex data - common strategies4m About higher-order functions3m Higher-order functions introduction19s Introducing Aggregating and Summarizing Data25s Partitioning Tables Introduction34s Sharing Insights Lab Introduction21s 4 readings Higher Order Functions reading activity30m Aggregating and Summarizing Data reading activity30m Partitioning Tables10m Sharing Insights10m Week 7: Data Storage and Optimization 5 videos Introduction to module 71m A quick refresher1m Introducing a new data management paradigm39s Introduction to the lesson48s What is Delta Lake5m 4 readings Data Warehouses10m Data Lakes10m Data Lakes vs Data Warehouses10m The Lakehouse15m Week 8: Delta Lake with Spark SQL 5 videos Introduction to the module1m Intro to Using Delta reading49s Managing Records in a Delta table34s Delta Engine Optimization Introduction40s Delta Lake Lab Introduction17s 4 readings 8.1 Using Delta10m 8.2 Managing records10m 8.3 Optimizing Delta10m Delta Lab45m Week 9: SQL Coding Challenges 1 reading SQL coding challenges1h 1 practice exercise Final Exam30m Data Science Fundamentals for Data Analysts Applied Data Science for Data Analysts Data Engineering with MS Azure Synapse Apache Spark Pools \u00b6 Week 1: Big Data Engineering Video: Introduction to the course Reading: Course syllabus Reading: How to be successful in this course Discussion Prompt: Meet and greet Video: What is an Apache Spark pool in Azure Synapse Analytics? Video: How do Apache Spark pools in Azure Synapse Analytics? Reading: When do you use Apache Spark pools in Azure Synapse Analytics? Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to spark notebooks Video: Understand the use-cases for spark notebooks Reading: Create a spark notebook in Azure Synapse Analytics Reading: Discover supported languages in spark notebooks Reading: Develop spark notebooks Reading: Develop spark notebooks Video: Run spark notebooks Reading: Run spark notebooks Reading: Load data in Spark notebooks Reading: Load data in Spark notebooks Reading: Save Spark notebooks Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to DataFrames in Spark pools in Azure Synapse Analytics Video: Load data in a Spark DataFrame Reading: Load data into a Spark DataFrame Reading: Create an Apache Spark table Video: Flatten nested structures and explode arrays with Apache Spark Reading: Flatten nested structures and explode arrays with Apache Spark in synapse Practice Quiz: Knowledge check Video: Lesson summary Week 2: Query pools and manage workloads in Azure Synapse Analytics Video: Describe the integration methods between SQL and Spark pools in Azure Synapse Analytics Video: Understand the use-cases for SQL and Spark pools integration Video: Authenticate in Azure Synapse Analytics Reading: Transfer data between SQL and Spark pool in Azure Synapse Analytics Reading: Authenticate between Spark and SQL pool in Azure Synapse Analytics Reading: Integrate SQL and Spark pools in Azure Synapse Analytics Video: Externalize the use of Spark pools within Azure Synapse Workspace Reading: Transfer data outside the Synapse workspace using the PySpark connector Video: Transfer data outside the Synapse workspace using the PySpark connector Practice Quiz: Knowledge check Video: Lesson summary Video: Monitor Spark pools in Azure Synapse Analytics Reading: Base-line Apache Spark performance with Apache Spark history server in Azure Synapse - Analytics Video: Optimize Apache Spark jobs in Azure Synapse Analytics Reading: Automate scaling of Apache Spark pools in Azure Synapse Analytics Practice Quiz: Knowledge check Video: Lesson summary Week 3: Practice Exam on Perform data engineering with Azure Synapse Apache Spark Pools Reading: About the practice exam Video: Course 6 recap Video: Course wrap up Discussion Prompt: Reflect on learning Reading: Next steps Distributed Computing with Spark SQL \u00b6 Week 1: Introduction to Spark Discussion Prompt: Learning Goals Reading: A Note From UC Davis Video: Course Introduction Video: Why Distributed Computing? Video: Spark DataFrames Video: The Databricks Environment Video: SQL in Notebooks Video: Import Data Reading: Readings and Resources Reading: Assignment #1 - Queries in Spark SQL Week 2: Spark Core Concepts Video: Module Introduction Video: Spark Terminology Video: Caching Video: Shuffle Partitions Video: Spark UI Video: Adaptive Query Execution (AQE) Reading: Readings Reading: Assignment #2 - Spark Internals WeeK 3: Engineering Data Pipelines Video: Module Introduction Video: Spark as a Connector Video: Accessing Data Video: File Formats Video: JSON, Schemas and Types Video: Writing Data Video: Tables and Views Reading: Readings Reading: Assignment #3 - Engineering Data Pipelines Week 4: Data Lakes, Warehouses and Lakehouses Video: Module Introduction Video: Data Lakes vs. Data Warehouses Video: What is a Lakehouse? Video: Delta Lake Video: Delta Lake (Demo) Video: Delta Advanced Features (Demo) Video: Continuing with Spark and Data Science Reading: Readings Reading: Assignment #4 - Lakehouse Video: Course Summary Discussion Prompt: Self-Reflection Data Warehousing with Microsoft Azure Synapse Analytics \u00b6 Tutorials \u00b6 3 Ways To Create Tables With Apache Spark and Download Data Introduction to Microsoft Spark Utilities - mssparkutils Mount in synapse - mssparkutils Python by Example Spark by Examples Note \u00b6 To edit multiple lines Shift + Option + i (Add cursor to end of each lines) Shift + Home (Moves cursor to start of each lines)","title":"\ud83d\udd05 Courses and Tutorials"},{"location":"synapse/pyspark-courses/#courses-and-tutorials","text":"","title":"\ud83d\udd05 Courses and Tutorials"},{"location":"synapse/pyspark-courses/#nosql-big-data-and-spark-foundations-specialization","text":"Introduction to NoSQL Databases Introduction to Big Data with Spark and Hadoop Week 1: What is Big Data? Reading: Course Introduction Video: What is Big Data? Video: Impact of Big Data Video: Parallel Processing, Scaling, and Data Parallelism Video: Big Data Tools and Ecosystem Video: Open Source and Big Data Video: Beyond the Hype Video: Big Data Use Cases Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Big Data Week 2: Introduction to the Hadoop Ecosystem Video: Introduction to Hadoop Video: Intro to MapReduce Video: Hadoop Ecosystem Video: HDFS Video: HIVE Video: HBASE LTI Item: Hands-on Lab: Hadoop MapReduce LTI Item: Hands-on lab : Hadoop Cluster (Optional) Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Hadoop Week 3: Apache Spark Video: Why use Apache Spark? Video: Functional Programming Basics Video: Parallel Programming using Resilient Distributed Datasets Video: Scale out / Data Parallelism in Apache Spark Video: Dataframes and SparkSQL LTI Item: Hands-on Lab: Getting Started with Spark using Python Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Apache Spark Week 4: DataFrames and SparkSQL Video: RDDs in Parallel Programming and Spark Video: Data-frames and Datasets Video: Catalyst and Tungsten Video: ETL with DataFrames LTI Item: Hands-on Lab: Introduction to Data-Frames Video: Real-world usage of SparkSQL LTI Item: Hands-On Lab: Introduction to SparkSQL Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Data-Frames & SparkSQL Week 5: Development and Runtime Environment Options Video: Apache Spark Architecture Video: Overview of Apache Spark Cluster Modes Video: How to Run an Apache Spark Application LTI Item: Hands-on Lab: Submit Apache Spark Applications Reading: Summary & Highlights Practice Quiz: Practice Quiz: Spark Architecture Video: Using Apache Spark on IBM Cloud LTI Item: Activate Trial Account Ungraded Plugin: Hands-on Lab: Getting started with Spark on IBM Cloud Video: Setting Apache Spark Configuration Video: Running Spark on Kubernetes LTI Item: Hands-on Lab: Apache Spark on Kubernetes Reading: Summary & Highlights Practice Quiz: Practice Quiz: Spark Runtime Environments Week 6: Monitoring & Tuning Video: The Apache Spark User Interface Video: Monitoring Application Progress Video: Debugging Apache Spark Application Issues Video: Understanding Memory Resources Video: Understanding Processor Resources LTI Item: Hands-on Lab: Monitoring and Performance Tuning Reading: Summary & Highlights Practice Quiz: Practice Quiz: Introduction to Monitoring & Tuning Reading: Instructions for the Final Exam Reading: Congrats & Next Steps Reading: Team & Acknowledgements Data Engineering and Machine Learning using Spark Week 1: Spark for Data Engineering Reading: Course Introduction Video: Spark Structured Streaming Video: GraphFrames on Apache Spark Video: ETL Workloads Video: Introduction to the pipeline editor in Elyra (Optional) Ungraded Plugin: Reading: Create component oriented data science pipelines using CLAIMED, - Elyra, KubeFlow Pipelines, MLX and Kubernetes LTI Item: Hands-on Lab: ETL using Apache Spark Reading: Summary & Highlights Practice Quiz: Practice Quiz: Spark for Data Engineering Week 2: SparkML Video: SparkML Fundamentals Video: Classification and Regression using Apache Spark Video: SparkML Clustering LTI Item: Obtain an IBM Cloud Feature Code Ungraded Plugin: Jupyter Notebook for Hands-on Lab: Machine Learning with Apache Spark ML LTI Item: Optional: Hands on Lab: Introduction to SparkML Reading: Summary & Highlights Practice Quiz: Practice Quiz: SparkML Week 3: Final Project Reading: Project Overview LTI Item: Hands-on Lab: ETL and Machine Learning Reading: Congratulations & Next Steps Reading: Team & Acknowledgements","title":"NoSQL, Big Data, and Spark Foundations Specialization"},{"location":"synapse/pyspark-courses/#data-engineering-with-ms-azure-synapse-apache-spark-pools","text":"Week 1: Big Data Engineering Video: Introduction to the course Reading: Course syllabus Reading: How to be successful in this course Discussion Prompt: Meet and greet Video: What is an Apache Spark pool in Azure Synapse Analytics? Video: How do Apache Spark pools in Azure Synapse Analytics? Reading: When do you use Apache Spark pools in Azure Synapse Analytics? Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to spark notebooks Video: Understand the use-cases for spark notebooks Reading: Create a spark notebook in Azure Synapse Analytics Reading: Discover supported languages in spark notebooks Reading: Develop spark notebooks Reading: Develop spark notebooks Video: Run spark notebooks Reading: Run spark notebooks Reading: Load data in Spark notebooks Reading: Load data in Spark notebooks Reading: Save Spark notebooks Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to DataFrames in Spark pools in Azure Synapse Analytics Video: Load data in a Spark DataFrame Reading: Load data into a Spark DataFrame Reading: Create an Apache Spark table Video: Flatten nested structures and explode arrays with Apache Spark Reading: Flatten nested structures and explode arrays with Apache Spark in synapse Practice Quiz: Knowledge check Video: Lesson summary Week 2: Query pools and manage workloads in Azure Synapse Analytics Video: Describe the integration methods between SQL and Spark pools in Azure Synapse Analytics Video: Understand the use-cases for SQL and Spark pools integration Video: Authenticate in Azure Synapse Analytics Reading: Transfer data between SQL and Spark pool in Azure Synapse Analytics Reading: Authenticate between Spark and SQL pool in Azure Synapse Analytics Reading: Integrate SQL and Spark pools in Azure Synapse Analytics Video: Externalize the use of Spark pools within Azure Synapse Workspace Reading: Transfer data outside the Synapse workspace using the PySpark connector Video: Transfer data outside the Synapse workspace using the PySpark connector Practice Quiz: Knowledge check Video: Lesson summary Video: Monitor Spark pools in Azure Synapse Analytics Reading: Base-line Apache Spark performance with Apache Spark history server in Azure Synaps- Analytics Video: Optimize Apache Spark jobs in Azure Synapse Analytics Reading: Automate scaling of Apache Spark pools in Azure Synapse Analytics Practice Quiz: Knowledge check Video: Lesson summary Week 3: Practice Exam on Perform data engineering with Azure Synapse Apache Spark Pools Reading: About the practice exam Video: Course 6 recap Video: Course wrap up Discussion Prompt: Reflect on learning Reading: Next steps","title":"Data Engineering with MS Azure Synapse Apache Spark Pools"},{"location":"synapse/pyspark-courses/#data-science-with-databricks-for-data-analysts-specialization","text":"Apache Spark (TM) SQL for Data Analysts Week 1: Welcome to Apache Spark SQL for Data Analysts 1 video Course goals1m 1 reading Before you begin5m Week 2: Spark makes big data easy 6 videos Introduction to module 21m What is big data?6m Common struggles with big data4m Big Data Needs2m Apache Spark Intro3m Spark SQL2m 1 practice exercise Module 2 Concept Review30m Week 3: Using Spark SQL on Databricks 9 videos Introduction to Module 31m Signing up for Databricks Community Edition1m Preparing your workspace2m Working with notebooks3m Using course materials6m Basic queries with Spark SQL reading introduction1m Data Visualization on Databricks reading introduction38s Data visualization tools1m Exploratory Data Analysis lab introduction25s 4 readings Course Materials5m Basic Queries reading activity30m Data Visualization reading activity30m Your turn! Exploratory Data Analysis lab30m Week 4: Spark Under the Hood 7 videos Introduction to module 454s Understanding optimizations7m The physical cluster3m The SparkUI and SQL tab2m Optimizing query logic4m Impact of Caching6m Optimizing with selective data loading6m 1 practice exercise Module 4 Concept Review30m Week 5: Complex Queries 5 videos Introduction to module 559s What is nested data? 2m Introduction to managing nested data1m Introduction to Manipulating Data 21s Introduction to Data Munging57s 3 readings Managing Nested Data reading activity30m Manipulating Data reading activity30m 5.3 Data Munging Lab30m Week 6: Applied Spark SQL 7 videos Introduction to module 61m Complex data - common strategies4m About higher-order functions3m Higher-order functions introduction19s Introducing Aggregating and Summarizing Data25s Partitioning Tables Introduction34s Sharing Insights Lab Introduction21s 4 readings Higher Order Functions reading activity30m Aggregating and Summarizing Data reading activity30m Partitioning Tables10m Sharing Insights10m Week 7: Data Storage and Optimization 5 videos Introduction to module 71m A quick refresher1m Introducing a new data management paradigm39s Introduction to the lesson48s What is Delta Lake5m 4 readings Data Warehouses10m Data Lakes10m Data Lakes vs Data Warehouses10m The Lakehouse15m Week 8: Delta Lake with Spark SQL 5 videos Introduction to the module1m Intro to Using Delta reading49s Managing Records in a Delta table34s Delta Engine Optimization Introduction40s Delta Lake Lab Introduction17s 4 readings 8.1 Using Delta10m 8.2 Managing records10m 8.3 Optimizing Delta10m Delta Lab45m Week 9: SQL Coding Challenges 1 reading SQL coding challenges1h 1 practice exercise Final Exam30m Data Science Fundamentals for Data Analysts Applied Data Science for Data Analysts","title":"Data Science with Databricks for Data Analysts Specialization"},{"location":"synapse/pyspark-courses/#data-engineering-with-ms-azure-synapse-apache-spark-pools_1","text":"Week 1: Big Data Engineering Video: Introduction to the course Reading: Course syllabus Reading: How to be successful in this course Discussion Prompt: Meet and greet Video: What is an Apache Spark pool in Azure Synapse Analytics? Video: How do Apache Spark pools in Azure Synapse Analytics? Reading: When do you use Apache Spark pools in Azure Synapse Analytics? Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to spark notebooks Video: Understand the use-cases for spark notebooks Reading: Create a spark notebook in Azure Synapse Analytics Reading: Discover supported languages in spark notebooks Reading: Develop spark notebooks Reading: Develop spark notebooks Video: Run spark notebooks Reading: Run spark notebooks Reading: Load data in Spark notebooks Reading: Load data in Spark notebooks Reading: Save Spark notebooks Practice Quiz: Knowledge check Video: Lesson summary Video: Introduction to DataFrames in Spark pools in Azure Synapse Analytics Video: Load data in a Spark DataFrame Reading: Load data into a Spark DataFrame Reading: Create an Apache Spark table Video: Flatten nested structures and explode arrays with Apache Spark Reading: Flatten nested structures and explode arrays with Apache Spark in synapse Practice Quiz: Knowledge check Video: Lesson summary Week 2: Query pools and manage workloads in Azure Synapse Analytics Video: Describe the integration methods between SQL and Spark pools in Azure Synapse Analytics Video: Understand the use-cases for SQL and Spark pools integration Video: Authenticate in Azure Synapse Analytics Reading: Transfer data between SQL and Spark pool in Azure Synapse Analytics Reading: Authenticate between Spark and SQL pool in Azure Synapse Analytics Reading: Integrate SQL and Spark pools in Azure Synapse Analytics Video: Externalize the use of Spark pools within Azure Synapse Workspace Reading: Transfer data outside the Synapse workspace using the PySpark connector Video: Transfer data outside the Synapse workspace using the PySpark connector Practice Quiz: Knowledge check Video: Lesson summary Video: Monitor Spark pools in Azure Synapse Analytics Reading: Base-line Apache Spark performance with Apache Spark history server in Azure Synapse - Analytics Video: Optimize Apache Spark jobs in Azure Synapse Analytics Reading: Automate scaling of Apache Spark pools in Azure Synapse Analytics Practice Quiz: Knowledge check Video: Lesson summary Week 3: Practice Exam on Perform data engineering with Azure Synapse Apache Spark Pools Reading: About the practice exam Video: Course 6 recap Video: Course wrap up Discussion Prompt: Reflect on learning Reading: Next steps","title":"Data Engineering with MS Azure Synapse Apache Spark Pools"},{"location":"synapse/pyspark-courses/#distributed-computing-with-spark-sql","text":"Week 1: Introduction to Spark Discussion Prompt: Learning Goals Reading: A Note From UC Davis Video: Course Introduction Video: Why Distributed Computing? Video: Spark DataFrames Video: The Databricks Environment Video: SQL in Notebooks Video: Import Data Reading: Readings and Resources Reading: Assignment #1 - Queries in Spark SQL Week 2: Spark Core Concepts Video: Module Introduction Video: Spark Terminology Video: Caching Video: Shuffle Partitions Video: Spark UI Video: Adaptive Query Execution (AQE) Reading: Readings Reading: Assignment #2 - Spark Internals WeeK 3: Engineering Data Pipelines Video: Module Introduction Video: Spark as a Connector Video: Accessing Data Video: File Formats Video: JSON, Schemas and Types Video: Writing Data Video: Tables and Views Reading: Readings Reading: Assignment #3 - Engineering Data Pipelines Week 4: Data Lakes, Warehouses and Lakehouses Video: Module Introduction Video: Data Lakes vs. Data Warehouses Video: What is a Lakehouse? Video: Delta Lake Video: Delta Lake (Demo) Video: Delta Advanced Features (Demo) Video: Continuing with Spark and Data Science Reading: Readings Reading: Assignment #4 - Lakehouse Video: Course Summary Discussion Prompt: Self-Reflection","title":"Distributed Computing with Spark SQL"},{"location":"synapse/pyspark-courses/#data-warehousing-with-microsoft-azure-synapse-analytics","text":"","title":"Data Warehousing with Microsoft Azure Synapse Analytics"},{"location":"synapse/pyspark-courses/#tutorials","text":"3 Ways To Create Tables With Apache Spark and Download Data Introduction to Microsoft Spark Utilities - mssparkutils Mount in synapse - mssparkutils Python by Example Spark by Examples","title":"Tutorials"},{"location":"synapse/pyspark-courses/#note","text":"To edit multiple lines Shift + Option + i (Add cursor to end of each lines) Shift + Home (Moves cursor to start of each lines)","title":"Note"},{"location":"synapse/pyspark-dataframe-api/","text":"\ud83d\udcdd PySpark Quickstart \u00b6 DataFrame \u00b6 This is a short introduction and quickstart for the PySpark DataFrame API. PySpark DataFrames are lazily evaluated. They are implemented on top of RDDs. When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as collect() are explicitly called, the computation starts. This notebook shows the basic usages of the DataFrame, geared mainly for new users. You can run the latest version of these examples by yourself in \u2018Live Notebook: DataFrame\u2019 at the quickstart page. There is also other useful information in Apache Spark documentation site, see the latest version of Spark SQL and DataFrames, RDD Programming Guide, Structured Streaming Programming Guide, Spark Streaming Programming Guide and Machine Learning Library (MLlib) Guide. PySpark applications start with initializing SparkSession which is the entry point of PySpark as below. In case of running it in PySpark shell via pyspark executable, the shell automatically creates the session in the variable spark for users. Python from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () DataFrame Creation \u00b6 A PySpark DataFrame can be created via pyspark.sql.SparkSession.createDataFrame typically by passing a list of lists, tuples, dictionaries and pyspark.sql.Rows, a pandas DataFrame and an RDD consisting of such a list. pyspark.sql.SparkSession.createDataFrame takes the schema argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data. Firstly, you can create a PySpark DataFrame from a list of rows Python from datetime import datetime , date import pandas as pd from pyspark.sql import Row df = spark . createDataFrame ([ Row ( a = 1 , b = 2. , c = 'string1' , d = date ( 2000 , 1 , 1 ), e = datetime ( 2000 , 1 , 1 , 12 , 0 )), Row ( a = 2 , b = 3. , c = 'string2' , d = date ( 2000 , 2 , 1 ), e = datetime ( 2000 , 1 , 2 , 12 , 0 )), Row ( a = 4 , b = 5. , c = 'string3' , d = date ( 2000 , 3 , 1 ), e = datetime ( 2000 , 1 , 3 , 12 , 0 )) ]) df Create a PySpark DataFrame with an explicit schema. Python df = spark . createDataFrame ([ ( 1 , 2. , 'string1' , date ( 2000 , 1 , 1 ), datetime ( 2000 , 1 , 1 , 12 , 0 )), ( 2 , 3. , 'string2' , date ( 2000 , 2 , 1 ), datetime ( 2000 , 1 , 2 , 12 , 0 )), ( 3 , 4. , 'string3' , date ( 2000 , 3 , 1 ), datetime ( 2000 , 1 , 3 , 12 , 0 )) ], schema = 'a long, b double, c string, d date, e timestamp' ) df Create a PySpark DataFrame from a pandas DataFrame Python pandas_df = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 2. , 3. , 4. ], 'c' : [ 'string1' , 'string2' , 'string3' ], 'd' : [ date ( 2000 , 1 , 1 ), date ( 2000 , 2 , 1 ), date ( 2000 , 3 , 1 )], 'e' : [ datetime ( 2000 , 1 , 1 , 12 , 0 ), datetime ( 2000 , 1 , 2 , 12 , 0 ), datetime ( 2000 , 1 , 3 , 12 , 0 )] }) df = spark . createDataFrame ( pandas_df ) df Create a PySpark DataFrame from an RDD consisting of a list of tuples. Python rdd = spark . sparkContext . parallelize ([ ( 1 , 2. , 'string1' , date ( 2000 , 1 , 1 ), datetime ( 2000 , 1 , 1 , 12 , 0 )), ( 2 , 3. , 'string2' , date ( 2000 , 2 , 1 ), datetime ( 2000 , 1 , 2 , 12 , 0 )), ( 3 , 4. , 'string3' , date ( 2000 , 3 , 1 ), datetime ( 2000 , 1 , 3 , 12 , 0 )) ]) df = spark . createDataFrame ( rdd , schema = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) df The DataFrames created above all have the same results and schema. Python # All DataFrames above result same. df . show () df . printSchema () Viewing Data \u00b6 The top rows of a DataFrame can be displayed using DataFrame.show(). Python df . show ( 1 ) Alternatively, you can enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via spark.sql.repl.eagerEval.maxNumRows configuration. Python spark . conf . set ( 'spark.sql.repl.eagerEval.enabled' , True ) df The rows can also be shown vertically. This is useful when rows are too long to show horizontally. Python df . show ( 1 , vertical = True ) You can see the DataFrame\u2019s schema and column names as follows: Python df . columns Python df . printSchema () Show the summary of the DataFrame Python df . select ( \"a\" , \"b\" , \"c\" ) . describe () . show () DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side. Python df . collect () In order to avoid throwing an out-of-memory exception, use DataFrame.take() or DataFrame.tail(). Python df . take ( 1 ) PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side. Python df . toPandas () Selecting and Accessing Data \u00b6 PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance. Python df . a In fact, most of column-wise operations return Columns. Python from pyspark.sql import Column from pyspark.sql.functions import upper type ( df . c ) == type ( upper ( df . c )) == type ( df . c . isNull ()) These Columns can be used to select the columns from a DataFrame. For example, DataFrame.select() takes the Column instances that returns another DataFrame. Python df . select ( df . c ) . show () Assign new Column instance. Python df . withColumn ( 'upper_c' , upper ( df . c )) . show () To select a subset of rows, use DataFrame.filter(). Python df . filter ( df . a == 1 ) . show () Applying a Function \u00b6 PySpark supports various UDFs and APIs to allow users to execute Python native functions. See also the latest Pandas UDFs and Pandas Function APIs. For instance, the example below allows users to directly use the APIs in a pandas Series within Python native function. Python import pandas as pd from pyspark.sql.functions import pandas_udf @pandas_udf ( 'long' ) def pandas_plus_one ( series : pd . Series ) -> pd . Series : # Simply plus one by using pandas Series. return series + 1 df . select ( pandas_plus_one ( df . a )) . show () Another example is DataFrame.mapInPandas which allows users directly use the APIs in a pandas DataFrame without any restrictions such as the result length. Python def pandas_filter_func ( iterator ): for pandas_df in iterator : yield pandas_df [ pandas_df . a == 1 ] df . mapInPandas ( pandas_filter_func , schema = df . schema ) . show () Grouping Data \u00b6 PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame. Python df = spark . createDataFrame ([ [ 'red' , 'banana' , 1 , 10 ], [ 'blue' , 'banana' , 2 , 20 ], [ 'red' , 'carrot' , 3 , 30 ], [ 'blue' , 'grape' , 4 , 40 ], [ 'red' , 'carrot' , 5 , 50 ], [ 'black' , 'carrot' , 6 , 60 ], [ 'red' , 'banana' , 7 , 70 ], [ 'red' , 'grape' , 8 , 80 ]], schema = [ 'color' , 'fruit' , 'v1' , 'v2' ]) df . show () Grouping and then applying the avg() function to the resulting groups. Python df . groupby ( 'color' ) . avg () . show () You can also apply a Python native function against each group by using pandas API. Python def plus_mean ( pandas_df ): return pandas_df . assign ( v1 = pandas_df . v1 - pandas_df . v1 . mean ()) df . groupby ( 'color' ) . applyInPandas ( plus_mean , schema = df . schema ) . show () Co-grouping and applying a function. Python df1 = spark . createDataFrame ( [( 20000101 , 1 , 1.0 ), ( 20000101 , 2 , 2.0 ), ( 20000102 , 1 , 3.0 ), ( 20000102 , 2 , 4.0 )], ( 'time' , 'id' , 'v1' )) df2 = spark . createDataFrame ( [( 20000101 , 1 , 'x' ), ( 20000101 , 2 , 'y' )], ( 'time' , 'id' , 'v2' )) def asof_join ( l , r ): return pd . merge_asof ( l , r , on = 'time' , by = 'id' ) df1 . groupby ( 'id' ) . cogroup ( df2 . groupby ( 'id' )) . applyInPandas ( asof_join , schema = 'time int, id int, v1 double, v2 string' ) . show () Getting Data in/out \u00b6 CSV is straightforward and easy to use. Parquet and ORC are efficient and compact file formats to read and write faster. There are many other data sources available in PySpark such as JDBC, text, binaryFile, Avro, etc. See also the latest Spark SQL, DataFrames and Datasets Guide in Apache Spark documentation. CSV Python df . write . csv ( 'foo.csv' , header = True ) spark . read . csv ( 'foo.csv' , header = True ) . show () Parquet Python df . write . parquet ( 'bar.parquet' ) spark . read . parquet ( 'bar.parquet' ) . show () Working with SQL \u00b6 DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below: Python df . createOrReplaceTempView ( \"tableA\" ) spark . sql ( \"SELECT count(*) from tableA\" ) . show () In addition, UDFs can be registered and invoked in SQL out of the box: Python @pandas_udf ( \"integer\" ) def add_one ( s : pd . Series ) -> pd . Series : return s + 1 spark . udf . register ( \"add_one\" , add_one ) spark . sql ( \"SELECT add_one(v1) FROM tableA\" ) . show () These SQL expressions can directly be mixed and used as PySpark columns. Python from pyspark.sql.functions import expr df . selectExpr ( 'add_one(v1)' ) . show () df . select ( expr ( 'count(*)' ) > 0 ) . show ()","title":"\ud83d\udcdd PySpark Quickstart"},{"location":"synapse/pyspark-dataframe-api/#pyspark-quickstart","text":"","title":"\ud83d\udcdd PySpark Quickstart"},{"location":"synapse/pyspark-dataframe-api/#dataframe","text":"This is a short introduction and quickstart for the PySpark DataFrame API. PySpark DataFrames are lazily evaluated. They are implemented on top of RDDs. When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as collect() are explicitly called, the computation starts. This notebook shows the basic usages of the DataFrame, geared mainly for new users. You can run the latest version of these examples by yourself in \u2018Live Notebook: DataFrame\u2019 at the quickstart page. There is also other useful information in Apache Spark documentation site, see the latest version of Spark SQL and DataFrames, RDD Programming Guide, Structured Streaming Programming Guide, Spark Streaming Programming Guide and Machine Learning Library (MLlib) Guide. PySpark applications start with initializing SparkSession which is the entry point of PySpark as below. In case of running it in PySpark shell via pyspark executable, the shell automatically creates the session in the variable spark for users. Python from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate ()","title":"DataFrame"},{"location":"synapse/pyspark-dataframe-api/#dataframe-creation","text":"A PySpark DataFrame can be created via pyspark.sql.SparkSession.createDataFrame typically by passing a list of lists, tuples, dictionaries and pyspark.sql.Rows, a pandas DataFrame and an RDD consisting of such a list. pyspark.sql.SparkSession.createDataFrame takes the schema argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data. Firstly, you can create a PySpark DataFrame from a list of rows Python from datetime import datetime , date import pandas as pd from pyspark.sql import Row df = spark . createDataFrame ([ Row ( a = 1 , b = 2. , c = 'string1' , d = date ( 2000 , 1 , 1 ), e = datetime ( 2000 , 1 , 1 , 12 , 0 )), Row ( a = 2 , b = 3. , c = 'string2' , d = date ( 2000 , 2 , 1 ), e = datetime ( 2000 , 1 , 2 , 12 , 0 )), Row ( a = 4 , b = 5. , c = 'string3' , d = date ( 2000 , 3 , 1 ), e = datetime ( 2000 , 1 , 3 , 12 , 0 )) ]) df Create a PySpark DataFrame with an explicit schema. Python df = spark . createDataFrame ([ ( 1 , 2. , 'string1' , date ( 2000 , 1 , 1 ), datetime ( 2000 , 1 , 1 , 12 , 0 )), ( 2 , 3. , 'string2' , date ( 2000 , 2 , 1 ), datetime ( 2000 , 1 , 2 , 12 , 0 )), ( 3 , 4. , 'string3' , date ( 2000 , 3 , 1 ), datetime ( 2000 , 1 , 3 , 12 , 0 )) ], schema = 'a long, b double, c string, d date, e timestamp' ) df Create a PySpark DataFrame from a pandas DataFrame Python pandas_df = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 2. , 3. , 4. ], 'c' : [ 'string1' , 'string2' , 'string3' ], 'd' : [ date ( 2000 , 1 , 1 ), date ( 2000 , 2 , 1 ), date ( 2000 , 3 , 1 )], 'e' : [ datetime ( 2000 , 1 , 1 , 12 , 0 ), datetime ( 2000 , 1 , 2 , 12 , 0 ), datetime ( 2000 , 1 , 3 , 12 , 0 )] }) df = spark . createDataFrame ( pandas_df ) df Create a PySpark DataFrame from an RDD consisting of a list of tuples. Python rdd = spark . sparkContext . parallelize ([ ( 1 , 2. , 'string1' , date ( 2000 , 1 , 1 ), datetime ( 2000 , 1 , 1 , 12 , 0 )), ( 2 , 3. , 'string2' , date ( 2000 , 2 , 1 ), datetime ( 2000 , 1 , 2 , 12 , 0 )), ( 3 , 4. , 'string3' , date ( 2000 , 3 , 1 ), datetime ( 2000 , 1 , 3 , 12 , 0 )) ]) df = spark . createDataFrame ( rdd , schema = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) df The DataFrames created above all have the same results and schema. Python # All DataFrames above result same. df . show () df . printSchema ()","title":"DataFrame Creation"},{"location":"synapse/pyspark-dataframe-api/#viewing-data","text":"The top rows of a DataFrame can be displayed using DataFrame.show(). Python df . show ( 1 ) Alternatively, you can enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via spark.sql.repl.eagerEval.maxNumRows configuration. Python spark . conf . set ( 'spark.sql.repl.eagerEval.enabled' , True ) df The rows can also be shown vertically. This is useful when rows are too long to show horizontally. Python df . show ( 1 , vertical = True ) You can see the DataFrame\u2019s schema and column names as follows: Python df . columns Python df . printSchema () Show the summary of the DataFrame Python df . select ( \"a\" , \"b\" , \"c\" ) . describe () . show () DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side. Python df . collect () In order to avoid throwing an out-of-memory exception, use DataFrame.take() or DataFrame.tail(). Python df . take ( 1 ) PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side. Python df . toPandas ()","title":"Viewing Data"},{"location":"synapse/pyspark-dataframe-api/#selecting-and-accessing-data","text":"PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance. Python df . a In fact, most of column-wise operations return Columns. Python from pyspark.sql import Column from pyspark.sql.functions import upper type ( df . c ) == type ( upper ( df . c )) == type ( df . c . isNull ()) These Columns can be used to select the columns from a DataFrame. For example, DataFrame.select() takes the Column instances that returns another DataFrame. Python df . select ( df . c ) . show () Assign new Column instance. Python df . withColumn ( 'upper_c' , upper ( df . c )) . show () To select a subset of rows, use DataFrame.filter(). Python df . filter ( df . a == 1 ) . show ()","title":"Selecting and Accessing Data"},{"location":"synapse/pyspark-dataframe-api/#applying-a-function","text":"PySpark supports various UDFs and APIs to allow users to execute Python native functions. See also the latest Pandas UDFs and Pandas Function APIs. For instance, the example below allows users to directly use the APIs in a pandas Series within Python native function. Python import pandas as pd from pyspark.sql.functions import pandas_udf @pandas_udf ( 'long' ) def pandas_plus_one ( series : pd . Series ) -> pd . Series : # Simply plus one by using pandas Series. return series + 1 df . select ( pandas_plus_one ( df . a )) . show () Another example is DataFrame.mapInPandas which allows users directly use the APIs in a pandas DataFrame without any restrictions such as the result length. Python def pandas_filter_func ( iterator ): for pandas_df in iterator : yield pandas_df [ pandas_df . a == 1 ] df . mapInPandas ( pandas_filter_func , schema = df . schema ) . show ()","title":"Applying a Function"},{"location":"synapse/pyspark-dataframe-api/#grouping-data","text":"PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame. Python df = spark . createDataFrame ([ [ 'red' , 'banana' , 1 , 10 ], [ 'blue' , 'banana' , 2 , 20 ], [ 'red' , 'carrot' , 3 , 30 ], [ 'blue' , 'grape' , 4 , 40 ], [ 'red' , 'carrot' , 5 , 50 ], [ 'black' , 'carrot' , 6 , 60 ], [ 'red' , 'banana' , 7 , 70 ], [ 'red' , 'grape' , 8 , 80 ]], schema = [ 'color' , 'fruit' , 'v1' , 'v2' ]) df . show () Grouping and then applying the avg() function to the resulting groups. Python df . groupby ( 'color' ) . avg () . show () You can also apply a Python native function against each group by using pandas API. Python def plus_mean ( pandas_df ): return pandas_df . assign ( v1 = pandas_df . v1 - pandas_df . v1 . mean ()) df . groupby ( 'color' ) . applyInPandas ( plus_mean , schema = df . schema ) . show () Co-grouping and applying a function. Python df1 = spark . createDataFrame ( [( 20000101 , 1 , 1.0 ), ( 20000101 , 2 , 2.0 ), ( 20000102 , 1 , 3.0 ), ( 20000102 , 2 , 4.0 )], ( 'time' , 'id' , 'v1' )) df2 = spark . createDataFrame ( [( 20000101 , 1 , 'x' ), ( 20000101 , 2 , 'y' )], ( 'time' , 'id' , 'v2' )) def asof_join ( l , r ): return pd . merge_asof ( l , r , on = 'time' , by = 'id' ) df1 . groupby ( 'id' ) . cogroup ( df2 . groupby ( 'id' )) . applyInPandas ( asof_join , schema = 'time int, id int, v1 double, v2 string' ) . show ()","title":"Grouping Data"},{"location":"synapse/pyspark-dataframe-api/#getting-data-inout","text":"CSV is straightforward and easy to use. Parquet and ORC are efficient and compact file formats to read and write faster. There are many other data sources available in PySpark such as JDBC, text, binaryFile, Avro, etc. See also the latest Spark SQL, DataFrames and Datasets Guide in Apache Spark documentation. CSV Python df . write . csv ( 'foo.csv' , header = True ) spark . read . csv ( 'foo.csv' , header = True ) . show () Parquet Python df . write . parquet ( 'bar.parquet' ) spark . read . parquet ( 'bar.parquet' ) . show ()","title":"Getting Data in/out"},{"location":"synapse/pyspark-dataframe-api/#working-with-sql","text":"DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below: Python df . createOrReplaceTempView ( \"tableA\" ) spark . sql ( \"SELECT count(*) from tableA\" ) . show () In addition, UDFs can be registered and invoked in SQL out of the box: Python @pandas_udf ( \"integer\" ) def add_one ( s : pd . Series ) -> pd . Series : return s + 1 spark . udf . register ( \"add_one\" , add_one ) spark . sql ( \"SELECT add_one(v1) FROM tableA\" ) . show () These SQL expressions can directly be mixed and used as PySpark columns. Python from pyspark.sql.functions import expr df . selectExpr ( 'add_one(v1)' ) . show () df . select ( expr ( 'count(*)' ) > 0 ) . show ()","title":"Working with SQL"},{"location":"synapse/pyspark-docker/","text":"Building spark docker stack - Spark, Livy, Jupyter to work with pyspark on Docker \u00b6 In most generic way we can used pre-built dockers like apache/spark-py and bitnami/spark . And there are a lots of other dockers image ready for working with spark stack but livy won't be compatible with them with arch like arm64/mac. So, we need to build images all the images compatible with spark version. References: \u00b6 Spark Jira Livy Jira Incubator livy github Apache Livy Pylivy Spark release packages Docker livy github and article Building Apache Livy 0.8.0 for Spark 3.x Apache Airflow","title":"Building spark docker stack - Spark, Livy, Jupyter to work with pyspark on Docker"},{"location":"synapse/pyspark-docker/#building-spark-docker-stack-spark-livy-jupyter-to-work-with-pyspark-on-docker","text":"In most generic way we can used pre-built dockers like apache/spark-py and bitnami/spark . And there are a lots of other dockers image ready for working with spark stack but livy won't be compatible with them with arch like arm64/mac. So, we need to build images all the images compatible with spark version.","title":"Building spark docker stack - Spark, Livy, Jupyter to work with pyspark on Docker"},{"location":"synapse/pyspark-docker/#references","text":"Spark Jira Livy Jira Incubator livy github Apache Livy Pylivy Spark release packages Docker livy github and article Building Apache Livy 0.8.0 for Spark 3.x Apache Airflow","title":"References:"},{"location":"synapse/spark-cluster-gpt/","text":"Create PySpark Cluster \u00b6 create docker compose for local pyspark cluster in spark version 3.3.1 with master, two workers, driver in client mode to run python code having jupyter for notebook, delta lake meta store using postgres as hive 1 \u00b6 Here is an example of a docker-compose.yml file for a local PySpark cluster with version 3.3.1: YAML version : '3' services : master : image : spark:3.3.1 hostname : spark-master environment : - SPARK_MASTER=spark://master:7077 - SPARK_MODE=cluster - SPARK_EXECUTOR_MEMORY=1g ports : - \"8080:8080\" - \"7077:7077\" networks : - spark-net worker1 : image : spark:3.3.1 hostname : spark-worker1 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net worker2 : image : spark:3.3.1 hostname : spark-worker2 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net driver : image : jupyter/pyspark-notebook:3.3.1 hostname : spark-driver environment : - SPARK_MASTER=spark://master:7077 - PYSPARK_DRIVER_PYTHON=jupyter - PYSPARK_DRIVER_PYTHON_OPTS='notebook' ports : - \"8888:8888\" depends_on : - master networks : - spark-net delta-lake-meta : image : postgres:10 hostname : delta-lake-meta environment : - POSTGRES_DB=delta - POSTGRES_USER=delta - POSTGRES_PASSWORD=delta ports : - \"5432:5432\" networks : - spark-net networks : spark-net : This file will create four services: a master, two workers, and a driver. The driver service is set up to run Jupyter for notebook, The Delta lake meta store is using postgres as a hive. You can use docker-compose up command to start the cluster. Please note that you will need to have Docker and Docker Compose installed on your machine for this to work. Here are examples of the configuration files you would need to run a PySpark cluster using the docker-compose.yml file I provided earlier: spark-defaults.conf: This file contains configurations for the Spark cluster, such as the amount of memory to allocate to the executor and driver, the number of cores to use, etc. Python spark . master spark : // master : 7077 spark . executor . memory 1 g spark . driver . memory 1 g hive-site.xml: This file contains configurations for the Hive metastore, such as the connection details for the Postgres database. XML <configuration> <property> <name> hive.metastore.uris </name> <value> thrift://delta-lake-meta:9083 </value> </property> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:postgresql://delta-lake-meta:5432/delta </value> </property> <property> <name> javax.jdo.option.ConnectionDriverName </name> <value> org.postgresql.Driver </value> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> delta </value> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> delta </value> </property> </configuration> log4j.properties: This file contains configurations for the logging level of the Spark cluster. Python log4j . rootCategory = ERROR , console log4j . appender . console = org . apache . log4j . ConsoleAppender log4j . appender . console . layout = org . apache . log4j . PatternLayout log4j . appender . console . layout . ConversionPattern = [ % d ] % p % m ( % c ) % n log4j . logger . org . eclipse . jetty = WARN core-site.xml: This file contains configurations for the HDFS connection. Python < configuration > < property > < name > fs . defaultFS </ name > < value > hdfs : // namenode : 9000 </ value > </ property > </ configuration > You can place these files in a directory called config and mount it to the container at /usr/local/spark/conf/ YAML services : master : image : spark:3.3.1 hostname : spark-master volumes : - ./config:/usr/local/spark/conf/ environment : - SPARK_MASTER=spark://master:7077 - SPARK_MODE=cluster - SPARK_EXECUTOR_MEMORY=1g ports : - \"8080:8080\" - \"7077:7077\" networks : - spark-net Here is an example of a docker-compose.yml file that sets up both a PySpark cluster and an HDFS cluster: YAML version : '3' services : master : image : spark:3.3.1 hostname : spark-master environment : - SPARK_MASTER=spark://master:7077 - SPARK_MODE=cluster - SPARK_EXECUTOR_MEMORY=1g ports : - \"8080:8080\" - \"7077:7077\" volumes : - ./config:/usr/local/spark/conf/ networks : - spark-net - hdfs-net worker1 : image : spark:3.3.1 hostname : spark-worker1 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net - hdfs-net worker2 : image : spark:3.3.1 hostname : spark-worker2 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net - hdfs-net driver : image : jupyter/pyspark-notebook:3.3.1 hostname : spark-driver environment : - SPARK_MASTER=spark://master:7077 - PYSPARK_DRIVER_PYTHON=jupyter - PYSPARK_DRIVER_PYTHON_OPTS='notebook' ports : - \"8888:8888\" depends_on : - master networks : - spark-net - hdfs-net delta-lake-meta : image : postgres:10 hostname : delta-lake-meta environment : - POSTGRES_DB=delta - POSTGRES_USER=delta - POSTGRES_PASSWORD=delta ports : - \"5432:5432\" networks : - spark-net - hdfs-net hdfs-namenode : image : hdfs hostname : hdfs-namenode ports : - \"9000:9000\" - \"50070:50070\" volumes : - hdfs-namenode:/hadoop/hdfs/namenode command : hdfs namenode environment : - CLUSTER_NAME=mycluster networks : - hdfs-net hdfs-datanode : image : hdfs hostname : hdfs-datanode ports : - \"50075:50075\" volumes : - hdfs-datanode:/h 2 \u00b6 Here is an example of a docker-compose.yml file that sets up a local PySpark cluster with Spark version 3.3.1, a master, two workers, a driver in client mode, and a Delta Lake meta store using Postgres as a Hive metastore: YAML version : '3.8' services : master : image : spark:3.3.1 command : bin/spark-class org.apache.spark.deploy.master.Master -h master ports : - \"8080:8080\" environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf worker : image : spark:3.3.1 command : bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077 environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf depends_on : - master worker2 : image : spark:3.3.1 command : bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077 environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf depends_on : - master driver : image : spark:3.3.1 command : bin/spark-submit --master spark://master:7077 --deploy-mode client --conf spark.hadoop.fs.s3a.access.key=<access_key> --conf spark.hadoop.fs.s3a.secret.key=<secret_key> <path_to_your_code> environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf depends_on : - master postgres : image : postgres:9.6 environment : POSTGRES_DB : metastore POSTGRES_USER : hive POSTGRES_PASSWORD : hive ports : - \"5432:5432\" volumes : - postgres-data:/var/lib/postgresql/data volumes : postgres-data : You will also need to create a conf directory in the same location as your docker-compose.yml file and include the following configuration files: spark-defaults.conf : This file should contain the configuration for Delta Lake, including the connection details for the Postgres Hive metastore. Python spark . hadoop . hive . metastore . uris jdbc : postgresql : // postgres : 5432 / metastore spark . hadoop . hive . metastore . user hive spark . hadoop . hive . metastore . password hive spark . databricks . delta . retentionDurationCheck . enabled true spark . databricks . delta . retentionDuration 14 d spark . databricks . delta . history . enabled true spark-env.sh : Python version : \"3.3\" services : master : image : \"spark:3.3.1\" command : \"start-master.sh\" ports : - \"8080:8080\" networks : - spark - net worker1 : image : \"spark:3.3.1\" command : \"start-worker.sh spark://master:7077\" networks : - spark - net worker2 : image : \"spark:3.3.1\" command : \"start-worker.sh spark://master:7077\" networks : - spark - net driver : image : \"spark:3.3.1\" command : \"spark-submit --master spark://master:7077 --deploy-mode client --class my.main.class my-app.jar\" networks : - spark - net postgres : image : \"postgres:latest\" environment : - POSTGRES_USER = delta - POSTGRES_PASSWORD = delta - POSTGRES_DB = delta ports : - \"5432:5432\" networks : - spark - net networks : spark - net : driver : bridge volumes : delta - lake - meta : driver : local environment : SPARK_LOCAL_IP = 127.0.0.1 SPARK_PUBLIC_DNS = 127.0.0.1 HIVE_METASTORE_URIS = thrift : // postgres : 9083 HIVE_METASTORE_USERNAME = delta HIVE_METASTORE_PASSWORD = delta HIVE_METASTORE_DBNAME = delta DELTA_LAKE_META_PATH =/ delta / meta SPARK_DELTA_LAKE_ENABLED = true SPARK_CONF_DIR =/ spark / conf SPARK_LOG_DIR =/ spark / logs SPARK_PID_DIR =/ spark / pids","title":"Create PySpark Cluster"},{"location":"synapse/spark-cluster-gpt/#create-pyspark-cluster","text":"create docker compose for local pyspark cluster in spark version 3.3.1 with master, two workers, driver in client mode to run python code having jupyter for notebook, delta lake meta store using postgres as hive","title":"Create PySpark Cluster"},{"location":"synapse/spark-cluster-gpt/#1","text":"Here is an example of a docker-compose.yml file for a local PySpark cluster with version 3.3.1: YAML version : '3' services : master : image : spark:3.3.1 hostname : spark-master environment : - SPARK_MASTER=spark://master:7077 - SPARK_MODE=cluster - SPARK_EXECUTOR_MEMORY=1g ports : - \"8080:8080\" - \"7077:7077\" networks : - spark-net worker1 : image : spark:3.3.1 hostname : spark-worker1 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net worker2 : image : spark:3.3.1 hostname : spark-worker2 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net driver : image : jupyter/pyspark-notebook:3.3.1 hostname : spark-driver environment : - SPARK_MASTER=spark://master:7077 - PYSPARK_DRIVER_PYTHON=jupyter - PYSPARK_DRIVER_PYTHON_OPTS='notebook' ports : - \"8888:8888\" depends_on : - master networks : - spark-net delta-lake-meta : image : postgres:10 hostname : delta-lake-meta environment : - POSTGRES_DB=delta - POSTGRES_USER=delta - POSTGRES_PASSWORD=delta ports : - \"5432:5432\" networks : - spark-net networks : spark-net : This file will create four services: a master, two workers, and a driver. The driver service is set up to run Jupyter for notebook, The Delta lake meta store is using postgres as a hive. You can use docker-compose up command to start the cluster. Please note that you will need to have Docker and Docker Compose installed on your machine for this to work. Here are examples of the configuration files you would need to run a PySpark cluster using the docker-compose.yml file I provided earlier: spark-defaults.conf: This file contains configurations for the Spark cluster, such as the amount of memory to allocate to the executor and driver, the number of cores to use, etc. Python spark . master spark : // master : 7077 spark . executor . memory 1 g spark . driver . memory 1 g hive-site.xml: This file contains configurations for the Hive metastore, such as the connection details for the Postgres database. XML <configuration> <property> <name> hive.metastore.uris </name> <value> thrift://delta-lake-meta:9083 </value> </property> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:postgresql://delta-lake-meta:5432/delta </value> </property> <property> <name> javax.jdo.option.ConnectionDriverName </name> <value> org.postgresql.Driver </value> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> delta </value> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> delta </value> </property> </configuration> log4j.properties: This file contains configurations for the logging level of the Spark cluster. Python log4j . rootCategory = ERROR , console log4j . appender . console = org . apache . log4j . ConsoleAppender log4j . appender . console . layout = org . apache . log4j . PatternLayout log4j . appender . console . layout . ConversionPattern = [ % d ] % p % m ( % c ) % n log4j . logger . org . eclipse . jetty = WARN core-site.xml: This file contains configurations for the HDFS connection. Python < configuration > < property > < name > fs . defaultFS </ name > < value > hdfs : // namenode : 9000 </ value > </ property > </ configuration > You can place these files in a directory called config and mount it to the container at /usr/local/spark/conf/ YAML services : master : image : spark:3.3.1 hostname : spark-master volumes : - ./config:/usr/local/spark/conf/ environment : - SPARK_MASTER=spark://master:7077 - SPARK_MODE=cluster - SPARK_EXECUTOR_MEMORY=1g ports : - \"8080:8080\" - \"7077:7077\" networks : - spark-net Here is an example of a docker-compose.yml file that sets up both a PySpark cluster and an HDFS cluster: YAML version : '3' services : master : image : spark:3.3.1 hostname : spark-master environment : - SPARK_MASTER=spark://master:7077 - SPARK_MODE=cluster - SPARK_EXECUTOR_MEMORY=1g ports : - \"8080:8080\" - \"7077:7077\" volumes : - ./config:/usr/local/spark/conf/ networks : - spark-net - hdfs-net worker1 : image : spark:3.3.1 hostname : spark-worker1 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net - hdfs-net worker2 : image : spark:3.3.1 hostname : spark-worker2 environment : - SPARK_MASTER=spark://master:7077 - SPARK_WORKER_MEMORY=1g depends_on : - master networks : - spark-net - hdfs-net driver : image : jupyter/pyspark-notebook:3.3.1 hostname : spark-driver environment : - SPARK_MASTER=spark://master:7077 - PYSPARK_DRIVER_PYTHON=jupyter - PYSPARK_DRIVER_PYTHON_OPTS='notebook' ports : - \"8888:8888\" depends_on : - master networks : - spark-net - hdfs-net delta-lake-meta : image : postgres:10 hostname : delta-lake-meta environment : - POSTGRES_DB=delta - POSTGRES_USER=delta - POSTGRES_PASSWORD=delta ports : - \"5432:5432\" networks : - spark-net - hdfs-net hdfs-namenode : image : hdfs hostname : hdfs-namenode ports : - \"9000:9000\" - \"50070:50070\" volumes : - hdfs-namenode:/hadoop/hdfs/namenode command : hdfs namenode environment : - CLUSTER_NAME=mycluster networks : - hdfs-net hdfs-datanode : image : hdfs hostname : hdfs-datanode ports : - \"50075:50075\" volumes : - hdfs-datanode:/h","title":"1"},{"location":"synapse/spark-cluster-gpt/#2","text":"Here is an example of a docker-compose.yml file that sets up a local PySpark cluster with Spark version 3.3.1, a master, two workers, a driver in client mode, and a Delta Lake meta store using Postgres as a Hive metastore: YAML version : '3.8' services : master : image : spark:3.3.1 command : bin/spark-class org.apache.spark.deploy.master.Master -h master ports : - \"8080:8080\" environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf worker : image : spark:3.3.1 command : bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077 environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf depends_on : - master worker2 : image : spark:3.3.1 command : bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077 environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf depends_on : - master driver : image : spark:3.3.1 command : bin/spark-submit --master spark://master:7077 --deploy-mode client --conf spark.hadoop.fs.s3a.access.key=<access_key> --conf spark.hadoop.fs.s3a.secret.key=<secret_key> <path_to_your_code> environment : SPARK_CONF_DIR : /conf volumes : - ./conf:/conf depends_on : - master postgres : image : postgres:9.6 environment : POSTGRES_DB : metastore POSTGRES_USER : hive POSTGRES_PASSWORD : hive ports : - \"5432:5432\" volumes : - postgres-data:/var/lib/postgresql/data volumes : postgres-data : You will also need to create a conf directory in the same location as your docker-compose.yml file and include the following configuration files: spark-defaults.conf : This file should contain the configuration for Delta Lake, including the connection details for the Postgres Hive metastore. Python spark . hadoop . hive . metastore . uris jdbc : postgresql : // postgres : 5432 / metastore spark . hadoop . hive . metastore . user hive spark . hadoop . hive . metastore . password hive spark . databricks . delta . retentionDurationCheck . enabled true spark . databricks . delta . retentionDuration 14 d spark . databricks . delta . history . enabled true spark-env.sh : Python version : \"3.3\" services : master : image : \"spark:3.3.1\" command : \"start-master.sh\" ports : - \"8080:8080\" networks : - spark - net worker1 : image : \"spark:3.3.1\" command : \"start-worker.sh spark://master:7077\" networks : - spark - net worker2 : image : \"spark:3.3.1\" command : \"start-worker.sh spark://master:7077\" networks : - spark - net driver : image : \"spark:3.3.1\" command : \"spark-submit --master spark://master:7077 --deploy-mode client --class my.main.class my-app.jar\" networks : - spark - net postgres : image : \"postgres:latest\" environment : - POSTGRES_USER = delta - POSTGRES_PASSWORD = delta - POSTGRES_DB = delta ports : - \"5432:5432\" networks : - spark - net networks : spark - net : driver : bridge volumes : delta - lake - meta : driver : local environment : SPARK_LOCAL_IP = 127.0.0.1 SPARK_PUBLIC_DNS = 127.0.0.1 HIVE_METASTORE_URIS = thrift : // postgres : 9083 HIVE_METASTORE_USERNAME = delta HIVE_METASTORE_PASSWORD = delta HIVE_METASTORE_DBNAME = delta DELTA_LAKE_META_PATH =/ delta / meta SPARK_DELTA_LAKE_ENABLED = true SPARK_CONF_DIR =/ spark / conf SPARK_LOG_DIR =/ spark / logs SPARK_PID_DIR =/ spark / pids","title":"2"},{"location":"tools/boot-os-from-vhd/","text":"Boot OS from Virtual Hard Drive (VHD) \u00b6 Download the Sharepint VHD from http://www.microsoft.com/download/en/details.aspx?displaylang=en&id=27417 ->2010-10a and merge them. Load the VHD using Virtual Box. - Shrink the VHD to 78GB from 130GB (Don't worry about this if you have 130GB space to be used by Sharepoint) Download the VHD Resizer from http://vmtoolkit.com/files/folders/converters/entry87.aspx Make the VHD of 80GB with fixed size (no dynamic) Add to the Boot menu: 1. bcdedit /copy {current} /d \"Sharepoint\" 2. Use Edit >> Mark and Edit >> Copy to copy the GUID so it\u2019s easy to insert into the remaining commands. 3. bcdedit /set {guid} device vhd=[d:]\\2010a.vhd 4. bcdedit /set {guid} osdevice vhd=[d:]\\2010a.vhd 5. bcdedit /set {guid} detecthal on To Remove from Boot menu: bcdedit /delete { guid } /cleanup After boot: Enable the Wifi Service on windows 2008 Server (by defaule, it's disabled) -> Feature Manager -> Add Feature, check the Wifi lan service","title":"Boot OS from Virtual Hard Drive (VHD)"},{"location":"tools/boot-os-from-vhd/#boot-os-from-virtual-hard-drive-vhd","text":"Download the Sharepint VHD from http://www.microsoft.com/download/en/details.aspx?displaylang=en&id=27417 ->2010-10a and merge them. Load the VHD using Virtual Box. - Shrink the VHD to 78GB from 130GB (Don't worry about this if you have 130GB space to be used by Sharepoint) Download the VHD Resizer from http://vmtoolkit.com/files/folders/converters/entry87.aspx Make the VHD of 80GB with fixed size (no dynamic) Add to the Boot menu: 1. bcdedit /copy {current} /d \"Sharepoint\" 2. Use Edit >> Mark and Edit >> Copy to copy the GUID so it\u2019s easy to insert into the remaining commands. 3. bcdedit /set {guid} device vhd=[d:]\\2010a.vhd 4. bcdedit /set {guid} osdevice vhd=[d:]\\2010a.vhd 5. bcdedit /set {guid} detecthal on To Remove from Boot menu: bcdedit /delete { guid } /cleanup After boot: Enable the Wifi Service on windows 2008 Server (by defaule, it's disabled) -> Feature Manager -> Add Feature, check the Wifi lan service","title":"Boot OS from Virtual Hard Drive (VHD)"},{"location":"tools/certificates/","text":"Searching and Installing Certificates \u00b6 Get all installed certs \u00b6 PowerShell Get-ChildItem Cert :\\ -Recurse Find cert by thumbprint \u00b6 PowerShell dir Cert :\\ CurrentUser \\ My | Where-Object { $_ . Thumbprint -in \"THUMBPRINT1\" , \"THUMBPRINT1\" } Tip Text Only Different locations: * CurrentUser * LocalMachine Different folders: * My * addressbook * TrustedPeople * Local NonRemovable Certificates * REQUEST * AuthRoot So sometime it's better to look for all cert to find if there is any as search will look for particular location Search by Friendly Name \u00b6 PowerShell dir cert : -Recurse | Where-Object { $_ . FriendlyName -like \"*Microsoft*\" } Search by Thumbprint anywhere \u00b6 PowerShell dir cert : -Recurse | Where-Object { $_ . Thumbprint -like \"*THUMBPRINT*\" } Search By NotAfter (expiry date) \u00b6 This will list any certificates that isn't valid after the 31 Dec 2020 PowerShell dir cert : -Recurse | Where-Object { $_ . NotAfter -lt ( Get-Date 2018 - 12 - 31 ) } This will list any certificates that will expire the upcomming year, from now and one year ahead PowerShell dir cert : -Recurse | Where-Object { $_ . NotAfter -gt ( Get-Date ) -and $_ . NotAfter -lt ( Get-Date ). AddYears ( 1 ) } Uninstall cert by thumbprint \u00b6 PowerShell Get-ChildItem Cert :\\ CurrentUser \\ My \\ THUMBPRINT | Remove-Item Uninstall by subject/serial number/issuer/whatever \u00b6 PowerShell Get-ChildItem Cert :\\ LocalMachine \\ My | Where-Object { $_ . Subject -match 'foo' } | Remove-Item Install cert as pfx \u00b6 Make sure to select system if for services","title":"Searching and Installing Certificates"},{"location":"tools/certificates/#searching-and-installing-certificates","text":"","title":"Searching and Installing Certificates"},{"location":"tools/certificates/#get-all-installed-certs","text":"PowerShell Get-ChildItem Cert :\\ -Recurse","title":"Get all installed certs"},{"location":"tools/certificates/#find-cert-by-thumbprint","text":"PowerShell dir Cert :\\ CurrentUser \\ My | Where-Object { $_ . Thumbprint -in \"THUMBPRINT1\" , \"THUMBPRINT1\" } Tip Text Only Different locations: * CurrentUser * LocalMachine Different folders: * My * addressbook * TrustedPeople * Local NonRemovable Certificates * REQUEST * AuthRoot So sometime it's better to look for all cert to find if there is any as search will look for particular location","title":"Find cert by thumbprint"},{"location":"tools/certificates/#search-by-friendly-name","text":"PowerShell dir cert : -Recurse | Where-Object { $_ . FriendlyName -like \"*Microsoft*\" }","title":"Search by Friendly Name"},{"location":"tools/certificates/#search-by-thumbprint-anywhere","text":"PowerShell dir cert : -Recurse | Where-Object { $_ . Thumbprint -like \"*THUMBPRINT*\" }","title":"Search by Thumbprint anywhere"},{"location":"tools/certificates/#search-by-notafter-expiry-date","text":"This will list any certificates that isn't valid after the 31 Dec 2020 PowerShell dir cert : -Recurse | Where-Object { $_ . NotAfter -lt ( Get-Date 2018 - 12 - 31 ) } This will list any certificates that will expire the upcomming year, from now and one year ahead PowerShell dir cert : -Recurse | Where-Object { $_ . NotAfter -gt ( Get-Date ) -and $_ . NotAfter -lt ( Get-Date ). AddYears ( 1 ) }","title":"Search By NotAfter (expiry date)"},{"location":"tools/certificates/#uninstall-cert-by-thumbprint","text":"PowerShell Get-ChildItem Cert :\\ CurrentUser \\ My \\ THUMBPRINT | Remove-Item","title":"Uninstall cert by thumbprint"},{"location":"tools/certificates/#uninstall-by-subjectserial-numberissuerwhatever","text":"PowerShell Get-ChildItem Cert :\\ LocalMachine \\ My | Where-Object { $_ . Subject -match 'foo' } | Remove-Item","title":"Uninstall by subject/serial number/issuer/whatever"},{"location":"tools/certificates/#install-cert-as-pfx","text":"Make sure to select system if for services","title":"Install cert as pfx"},{"location":"tools/chocolatey/","text":"Install and manage applications using Chocolatey \u00b6 Run command as admin Batchfile $ @ \" %SystemRoot% \\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \"iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET \"PATH= %PATH% ; %ALLUSERSPROFILE% \\chocolatey\\bin\" Install packages/applications \u00b6 Batchfile choco install -y [app1] [app2] [app3] ... Search application that you are looking for Batchfile choco search [keyword] // chocho search fabric List all installed applications Batchfile choco list -li // with version Export all installed applications as powershell script \u00b6 PowerShell PS > choco list - -local-only - -idonly | % { \"choco install -y \" + $_ } > choco_install . ps1 Don't forget to remove first and last line from the file, one with choco version and other with applications count.Once exported run the file using powershell as admin to reinstall all the application Update all applications \u00b6 Batchfile cup -y all we can install vs code extensions from command as well Batchfile code --install-extension [Unique Identifier] // Eg Angular.ng-template Export vs code extensions : \u00b6 PowerShell PS > code - -list-extensions | % { \"code --install-extension \" + $_ } > vscode_extensions_install . ps1 Run vscode_extensions_install.ps1 to install all exported extensions. similarly, for Visual Studio extensions or packages Batchfile choco install -y visualstudio2017professional --package-parameters= \"'--add Microsoft.VisualStudio.Component.Git'\" But, I prefer to use Settings Sync \ud83d\udc4d Here is better way as config file with application versions \u00b6 PowerShell Write-Output \"<?xml version= `\" 1.0 `\" encoding= `\" utf-8 `\" ?>\" Write-Output \"<packages>\" choco list -lo -r -y | % { \" <package id= `\" $( $_ . SubString ( 0 , $_ . IndexOf ( \"|\" ))) `\" version= `\" $( $_ . SubString ( $_ . IndexOf ( \"|\" ) + 1 )) `\" />\" } Write-Output \"</packages>\" And to use: Export PowerShell choco-export . ps1 > packages . config To import: PowerShell choco install packages . config Tip supported code block extensions Code block types supported diagrams MermaidJS Emoji cheat sheet Emojis Style Examples Warning If problem problem viewing diagrams, change setting > Mermaid Preview Enhanced: Open Mermaid Config Text Only MERMAID_CONFIG = { startOnLoad: true, themeCSS: { a } }","title":"Install and manage applications using Chocolatey"},{"location":"tools/chocolatey/#install-and-manage-applications-using-chocolatey","text":"Run command as admin Batchfile $ @ \" %SystemRoot% \\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \"iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET \"PATH= %PATH% ; %ALLUSERSPROFILE% \\chocolatey\\bin\"","title":"Install and manage applications using Chocolatey"},{"location":"tools/chocolatey/#install-packagesapplications","text":"Batchfile choco install -y [app1] [app2] [app3] ... Search application that you are looking for Batchfile choco search [keyword] // chocho search fabric List all installed applications Batchfile choco list -li // with version","title":"Install packages/applications"},{"location":"tools/chocolatey/#export-all-installed-applications-as-powershell-script","text":"PowerShell PS > choco list - -local-only - -idonly | % { \"choco install -y \" + $_ } > choco_install . ps1 Don't forget to remove first and last line from the file, one with choco version and other with applications count.Once exported run the file using powershell as admin to reinstall all the application","title":"Export all installed applications as powershell script"},{"location":"tools/chocolatey/#update-all-applications","text":"Batchfile cup -y all we can install vs code extensions from command as well Batchfile code --install-extension [Unique Identifier] // Eg Angular.ng-template","title":"Update all applications"},{"location":"tools/chocolatey/#export-vs-code-extensions","text":"PowerShell PS > code - -list-extensions | % { \"code --install-extension \" + $_ } > vscode_extensions_install . ps1 Run vscode_extensions_install.ps1 to install all exported extensions. similarly, for Visual Studio extensions or packages Batchfile choco install -y visualstudio2017professional --package-parameters= \"'--add Microsoft.VisualStudio.Component.Git'\" But, I prefer to use Settings Sync \ud83d\udc4d","title":"Export vs code extensions:"},{"location":"tools/chocolatey/#here-is-better-way-as-config-file-with-application-versions","text":"PowerShell Write-Output \"<?xml version= `\" 1.0 `\" encoding= `\" utf-8 `\" ?>\" Write-Output \"<packages>\" choco list -lo -r -y | % { \" <package id= `\" $( $_ . SubString ( 0 , $_ . IndexOf ( \"|\" ))) `\" version= `\" $( $_ . SubString ( $_ . IndexOf ( \"|\" ) + 1 )) `\" />\" } Write-Output \"</packages>\" And to use: Export PowerShell choco-export . ps1 > packages . config To import: PowerShell choco install packages . config Tip supported code block extensions Code block types supported diagrams MermaidJS Emoji cheat sheet Emojis Style Examples Warning If problem problem viewing diagrams, change setting > Mermaid Preview Enhanced: Open Mermaid Config Text Only MERMAID_CONFIG = { startOnLoad: true, themeCSS: { a } }","title":"Here is better way as config file with application versions"},{"location":"tools/configure-publish-mkdocs-github/","text":"\ud83d\udcda Configure Mkdocs and Auto deploy to Github.io \u00b6 Setup \u00b6 Mkdocs can be setup from https://squidfunk.github.io/mkdocs-material/getting-started/#with-docker either cloning repo or using docker Text Only git clone https://github.com/squidfunk/mkdocs-material.git Note: For mac docker Text Only docker pull ghcr.io/afritzler/mkdocs-material Template mkdocs is published at https://padamshrestha.github.io/mkdocs-material/ Add Access Token to the repo \u00b6 Create a new token with proper access and set it Secrets which will be used by the deployment or ci script Repository can have it's own site at github.io if it's public repo. Enable git actions \u00b6 Enable git actions to auto trigger ci For gh-pages deployment for public repository \u00b6 Add same-gh-deployment.yml ci configuration file at .github/workflows Text Only name: same-gh-deployment on: push: branches: - master - main jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.x - name: Install requirements run: pip install -r requirements.txt - name: Install plugins run: pip install -r requirements-plugins.txt - name: Deploy run: mkdocs gh-deploy --force For gh-pages deployment from private repository to public gh-pages \u00b6 Repository containing code will be private. Create public repository with gh-pages enabled Add remote-gh-deployment.yml ci configuration file at .github/workflows and change public repo path accordingly Text Only name: remote-gh-deployment on: push: branches: - master - main jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - name: Install requirements run: pip install -r requirements.txt - name: Install plugins run: pip install -r requirements-plugins.txt - name: Build, init new repo in site folder and commit generated files run: | mkdocs build cd site git init git add -A git config --local user.name \"padamshrestha\" git config --local user.email \"padamshres@gmail.com\" git commit -m 'deploy' git push -f https://$USERNAME:$ACTION_TOKEN@github.com/padamshrestha/mkdocs-material.git master:gh-pages env: ACTION_TOKEN: ${{ secrets.DEPLOY_ACTION_TOKEN }} USERNAME: padamshrestha With the configuration, when new code gets pushed private repo, compiled mkdocs will be deployed to public gh-pages. The site takes a few mins to reflect new changes. Gitlab pages \u00b6 Gitlab has default private pages for the repo at https://user_name.gitlab.com/repo_name Here is the ci script for gitlab .gitlab-cli.yml image: python:3.11.0 pages: stage: deploy only: - master - main before_script: - pip install -r requirements.txt - pip install -r requirements-plugins.txt script: - pip install mkdocs-material - mkdocs build --site-dir public artifacts: paths: - public Setting for markdown-paste-image \u00b6 JSON \"MarkdownPaste.path\" : \"../assets/images/\" Customize templates \u00b6 Add custom directory to hold the template files so update mkdocs.yml file Markdown theme: name: material custom_dir: overrides Get a copy of html from github repo for modification and add that to Markdown \u251c\u2500 docs \u251c\u2500 overrides/ \u2502 \u2514\u2500 partials/ \u2502 \u2514\u2500 header.html \u2514\u2500 mkdocs.yml New elements can be added to header.html and any custom css or js has to be included in mkdocs.yml Add css and js Markdown \u251c\u2500 docs | \u251c\u2500 assets \u251c\u2500 css extra.css \u251c\u2500 js extra.js \u2514\u2500 mkdocs.yml and provide the references in mkdocs.yml Markdown # css extra_css: - assets/css/custom.css - assets/css/extra.css # js extra_javascript: - assets/js/extra.js Image Paste shortcut \u00b6 Mac \u00b6 Copy - Ctrl + Option + s Paste - Ctrl + Option + v Win VDI with PicPick hot keys remapped and capture default to clipboard \u00b6 Copy - Shift + Alt + s Paste - Ctrl + Option + v Useful links \u00b6 Image copy paste vs code extension File exclude extension Jupytext Github secrets mkdocs setup docs Fonts Disable page search Mkdocs full extensions PyMdown PicPick - Portable snipping tool Supported tips/admonitions Code snippet language supports icons - copy and paste image rather than more icons emoji db - searchable emoji airtable -searchable Convert jupytext to markdown \u00b6 Text Only jupytext --to markdown notebook.ipynb # convert notebook.ipynb to a .md file jupytext --to markdown notebook.py # convert .py to a .md file jupytext --to py notebook.ipynb # convert notebook.ipynb to a .py file","title":"\ud83d\udcda Configure Mkdocs and Auto deploy to Github.io"},{"location":"tools/configure-publish-mkdocs-github/#configure-mkdocs-and-auto-deploy-to-githubio","text":"","title":"\ud83d\udcda Configure Mkdocs and Auto deploy to Github.io"},{"location":"tools/configure-publish-mkdocs-github/#setup","text":"Mkdocs can be setup from https://squidfunk.github.io/mkdocs-material/getting-started/#with-docker either cloning repo or using docker Text Only git clone https://github.com/squidfunk/mkdocs-material.git Note: For mac docker Text Only docker pull ghcr.io/afritzler/mkdocs-material Template mkdocs is published at https://padamshrestha.github.io/mkdocs-material/","title":"Setup"},{"location":"tools/configure-publish-mkdocs-github/#add-access-token-to-the-repo","text":"Create a new token with proper access and set it Secrets which will be used by the deployment or ci script Repository can have it's own site at github.io if it's public repo.","title":"Add Access Token to the repo"},{"location":"tools/configure-publish-mkdocs-github/#enable-git-actions","text":"Enable git actions to auto trigger ci","title":"Enable git actions"},{"location":"tools/configure-publish-mkdocs-github/#for-gh-pages-deployment-for-public-repository","text":"Add same-gh-deployment.yml ci configuration file at .github/workflows Text Only name: same-gh-deployment on: push: branches: - master - main jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.x - name: Install requirements run: pip install -r requirements.txt - name: Install plugins run: pip install -r requirements-plugins.txt - name: Deploy run: mkdocs gh-deploy --force","title":"For gh-pages deployment for public repository"},{"location":"tools/configure-publish-mkdocs-github/#for-gh-pages-deployment-from-private-repository-to-public-gh-pages","text":"Repository containing code will be private. Create public repository with gh-pages enabled Add remote-gh-deployment.yml ci configuration file at .github/workflows and change public repo path accordingly Text Only name: remote-gh-deployment on: push: branches: - master - main jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - name: Install requirements run: pip install -r requirements.txt - name: Install plugins run: pip install -r requirements-plugins.txt - name: Build, init new repo in site folder and commit generated files run: | mkdocs build cd site git init git add -A git config --local user.name \"padamshrestha\" git config --local user.email \"padamshres@gmail.com\" git commit -m 'deploy' git push -f https://$USERNAME:$ACTION_TOKEN@github.com/padamshrestha/mkdocs-material.git master:gh-pages env: ACTION_TOKEN: ${{ secrets.DEPLOY_ACTION_TOKEN }} USERNAME: padamshrestha With the configuration, when new code gets pushed private repo, compiled mkdocs will be deployed to public gh-pages. The site takes a few mins to reflect new changes.","title":"For gh-pages deployment from private repository to public gh-pages"},{"location":"tools/configure-publish-mkdocs-github/#gitlab-pages","text":"Gitlab has default private pages for the repo at https://user_name.gitlab.com/repo_name Here is the ci script for gitlab .gitlab-cli.yml image: python:3.11.0 pages: stage: deploy only: - master - main before_script: - pip install -r requirements.txt - pip install -r requirements-plugins.txt script: - pip install mkdocs-material - mkdocs build --site-dir public artifacts: paths: - public","title":"Gitlab pages"},{"location":"tools/configure-publish-mkdocs-github/#setting-for-markdown-paste-image","text":"JSON \"MarkdownPaste.path\" : \"../assets/images/\"","title":"Setting for markdown-paste-image"},{"location":"tools/configure-publish-mkdocs-github/#customize-templates","text":"Add custom directory to hold the template files so update mkdocs.yml file Markdown theme: name: material custom_dir: overrides Get a copy of html from github repo for modification and add that to Markdown \u251c\u2500 docs \u251c\u2500 overrides/ \u2502 \u2514\u2500 partials/ \u2502 \u2514\u2500 header.html \u2514\u2500 mkdocs.yml New elements can be added to header.html and any custom css or js has to be included in mkdocs.yml Add css and js Markdown \u251c\u2500 docs | \u251c\u2500 assets \u251c\u2500 css extra.css \u251c\u2500 js extra.js \u2514\u2500 mkdocs.yml and provide the references in mkdocs.yml Markdown # css extra_css: - assets/css/custom.css - assets/css/extra.css # js extra_javascript: - assets/js/extra.js","title":"Customize templates"},{"location":"tools/configure-publish-mkdocs-github/#image-paste-shortcut","text":"","title":"Image Paste shortcut"},{"location":"tools/configure-publish-mkdocs-github/#mac","text":"Copy - Ctrl + Option + s Paste - Ctrl + Option + v","title":"Mac"},{"location":"tools/configure-publish-mkdocs-github/#win-vdi-with-picpick-hot-keys-remapped-and-capture-default-to-clipboard","text":"Copy - Shift + Alt + s Paste - Ctrl + Option + v","title":"Win VDI with PicPick hot keys remapped and capture default to clipboard"},{"location":"tools/configure-publish-mkdocs-github/#useful-links","text":"Image copy paste vs code extension File exclude extension Jupytext Github secrets mkdocs setup docs Fonts Disable page search Mkdocs full extensions PyMdown PicPick - Portable snipping tool Supported tips/admonitions Code snippet language supports icons - copy and paste image rather than more icons emoji db - searchable emoji airtable -searchable","title":"Useful links"},{"location":"tools/configure-publish-mkdocs-github/#convert-jupytext-to-markdown","text":"Text Only jupytext --to markdown notebook.ipynb # convert notebook.ipynb to a .md file jupytext --to markdown notebook.py # convert .py to a .md file jupytext --to py notebook.ipynb # convert notebook.ipynb to a .py file","title":"Convert jupytext to markdown"},{"location":"tools/create_tunnel_cloudflare/","text":"Create tunnel in cloudflare for local applications \u00b6 Create tunnel \u00b6 Go to Zero Trust \u00b6 Go to Access -> Tunnel \u00b6 Go to ailgo.org -> DNS -> Add Record \u00b6 Ref: https://tech.aufomm.com/how-to-use-cloudflare-tunnel-to-expose-multiple-local-services Authenticate API with token \u00b6 Create token \u00b6 Add Applications \u00b6 configure \u00b6 When calling http APIs include these in header JavaScript CF - Access - Client - Id : . access CF - Access - Client - Secret : Allow Tradingview Server IPS to call APIs \u00b6 Allow list of IPs JavaScript 52.89.214.238 34.212.75.30 54.218.53.128 52.32.178.7 Ref: https://www.tradingview.com/support/solutions/43000529348-about-webhooks/ Map new application to cloudflare tunnel \u00b6 Adding email for access \u00b6","title":"Create tunnel in cloudflare for local applications"},{"location":"tools/create_tunnel_cloudflare/#create-tunnel-in-cloudflare-for-local-applications","text":"","title":"Create tunnel in cloudflare for local applications"},{"location":"tools/create_tunnel_cloudflare/#create-tunnel","text":"","title":"Create tunnel"},{"location":"tools/create_tunnel_cloudflare/#go-to-zero-trust","text":"","title":"Go to Zero Trust"},{"location":"tools/create_tunnel_cloudflare/#go-to-access-tunnel","text":"","title":"Go to Access -&gt; Tunnel"},{"location":"tools/create_tunnel_cloudflare/#go-to-ailgoorg-dns-add-record","text":"Ref: https://tech.aufomm.com/how-to-use-cloudflare-tunnel-to-expose-multiple-local-services","title":"Go to ailgo.org -&gt; DNS -&gt; Add Record"},{"location":"tools/create_tunnel_cloudflare/#authenticate-api-with-token","text":"","title":"Authenticate API with token"},{"location":"tools/create_tunnel_cloudflare/#create-token","text":"","title":"Create token"},{"location":"tools/create_tunnel_cloudflare/#add-applications","text":"","title":"Add Applications"},{"location":"tools/create_tunnel_cloudflare/#configure","text":"When calling http APIs include these in header JavaScript CF - Access - Client - Id : . access CF - Access - Client - Secret :","title":"configure"},{"location":"tools/create_tunnel_cloudflare/#allow-tradingview-server-ips-to-call-apis","text":"Allow list of IPs JavaScript 52.89.214.238 34.212.75.30 54.218.53.128 52.32.178.7 Ref: https://www.tradingview.com/support/solutions/43000529348-about-webhooks/","title":"Allow Tradingview Server IPS to call APIs"},{"location":"tools/create_tunnel_cloudflare/#map-new-application-to-cloudflare-tunnel","text":"","title":"Map new application to cloudflare tunnel"},{"location":"tools/create_tunnel_cloudflare/#adding-email-for-access","text":"","title":"Adding email for access"},{"location":"tools/docker/","text":"\ud83d\udc33 Installing and Running Docker in WSL with VS Code \u00b6 WSL is much more integration and user friendly. Install Docker Docker Settings: \u00b6 Run docker from WSL Linux \u00b6 Connect to container via VS Code \u00b6 Commands \u00b6 Command Description docker ps List all containers docker ps -a docker stop id for id first num or char should be enough if no duplicate docker rm id docker images docker rmi id docker inspect containerName Give all the details about the container docker rm \u2013v id Remove container and volume Command Description docker-compose build Build or rebuild services defined in docker-compose.yml One off command (only mongo) docker-compose build mongo docker-compose up Create and start the containers docker-compose up \u2013no-deps node --no-deps: Do not recreate services that node depends on node: Rebuild node image and stop, destroy and recreate only node docker-compose up \u2013d : in daemon mode, so it won\u2019t block the console docker-compose down down: Take all of the containers down (stop and remove) docker-compose down \u2013rmi all \u2013volumes --rmi :Remove all images --volumes: Remove all volumes docker-compose logs docker-compose ps docker-compose stop docker-compose start docker-compose rm Command Description docker rm $(docker ps -a -q) Delete all containers docker rmi $(docker images -q) Delete all images docker rm \u2013f $(docker ps \u2013a -q) Remove all containers -f: force -q: quite docker rmi -f $( docker images -q dangling=true) docker rmi -f $( docker images -q) Example Text Only `docker pull kitematic/hello-world-nginx` `docker run \u2013p 80:80 kitematic/hello-world-nginx` With part mapping - 80 image port to 80 container port `docker run -p 8080:3000 \u2013v /var/www node` where \u2013v creates volume and /var/www is container volume on host. We can check about the location of container volume using command We can customize the volume location rather than default. docker run -p 8080:3000 \u2013v $(pwd) /var/www node where $(pwd) is from current working directory used as host location (or host mount) 8080: external or image port 3000: default port the express generated so we need to use that as internal port -v: create volume $(pwd): Create volume that point source code in current directory /var/www: volume inside the container that points to current directory -w: make \u201c/var/www\u201d as working directory or context to run commands Tip Text Only Set timezone in ubuntu console RUN apt-get update && \\ apt-get install -yq tzdata && \\ ln -fs /usr/share/zoneinfo/America/New_York /etc/localtime Tip Text Only Get all containers and it's ports `docker container ls --format \"table {{.ID}}\\t{{.Names}}\\t{{.Ports}}\" -a`","title":"\ud83d\udc33  Installing and Running Docker in WSL with VS Code"},{"location":"tools/docker/#installing-and-running-docker-in-wsl-with-vs-code","text":"WSL is much more integration and user friendly. Install Docker","title":"\ud83d\udc33  Installing and Running Docker in WSL with VS Code"},{"location":"tools/docker/#docker-settings","text":"","title":"Docker Settings:"},{"location":"tools/docker/#run-docker-from-wsl-linux","text":"","title":"Run docker from WSL Linux"},{"location":"tools/docker/#connect-to-container-via-vs-code","text":"","title":"Connect to container via VS Code"},{"location":"tools/docker/#commands","text":"Command Description docker ps List all containers docker ps -a docker stop id for id first num or char should be enough if no duplicate docker rm id docker images docker rmi id docker inspect containerName Give all the details about the container docker rm \u2013v id Remove container and volume Command Description docker-compose build Build or rebuild services defined in docker-compose.yml One off command (only mongo) docker-compose build mongo docker-compose up Create and start the containers docker-compose up \u2013no-deps node --no-deps: Do not recreate services that node depends on node: Rebuild node image and stop, destroy and recreate only node docker-compose up \u2013d : in daemon mode, so it won\u2019t block the console docker-compose down down: Take all of the containers down (stop and remove) docker-compose down \u2013rmi all \u2013volumes --rmi :Remove all images --volumes: Remove all volumes docker-compose logs docker-compose ps docker-compose stop docker-compose start docker-compose rm Command Description docker rm $(docker ps -a -q) Delete all containers docker rmi $(docker images -q) Delete all images docker rm \u2013f $(docker ps \u2013a -q) Remove all containers -f: force -q: quite docker rmi -f $( docker images -q dangling=true) docker rmi -f $( docker images -q) Example Text Only `docker pull kitematic/hello-world-nginx` `docker run \u2013p 80:80 kitematic/hello-world-nginx` With part mapping - 80 image port to 80 container port `docker run -p 8080:3000 \u2013v /var/www node` where \u2013v creates volume and /var/www is container volume on host. We can check about the location of container volume using command We can customize the volume location rather than default. docker run -p 8080:3000 \u2013v $(pwd) /var/www node where $(pwd) is from current working directory used as host location (or host mount) 8080: external or image port 3000: default port the express generated so we need to use that as internal port -v: create volume $(pwd): Create volume that point source code in current directory /var/www: volume inside the container that points to current directory -w: make \u201c/var/www\u201d as working directory or context to run commands Tip Text Only Set timezone in ubuntu console RUN apt-get update && \\ apt-get install -yq tzdata && \\ ln -fs /usr/share/zoneinfo/America/New_York /etc/localtime Tip Text Only Get all containers and it's ports `docker container ls --format \"table {{.ID}}\\t{{.Names}}\\t{{.Ports}}\" -a`","title":"Commands"},{"location":"tools/dotnetcore-azurefunction-wsl/","text":"Install dotnet core 3.1 and azure-functions-core-tools on WSL \u00b6 Install dotnet core 3.1 \u00b6 Text Only wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-3.1 However, might need to clean up previous attempts. Text Only sudo dpkg --purge packages-microsoft-prod && sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update Install azure-functions-core-tools \u00b6 Text Only wget -q https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install azure-functions-core-tools-3","title":"Install dotnet core 3.1 and azure-functions-core-tools on WSL"},{"location":"tools/dotnetcore-azurefunction-wsl/#install-dotnet-core-31-and-azure-functions-core-tools-on-wsl","text":"","title":"Install dotnet core 3.1 and azure-functions-core-tools on WSL"},{"location":"tools/dotnetcore-azurefunction-wsl/#install-dotnet-core-31","text":"Text Only wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-3.1 However, might need to clean up previous attempts. Text Only sudo dpkg --purge packages-microsoft-prod && sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update","title":"Install dotnet core 3.1"},{"location":"tools/dotnetcore-azurefunction-wsl/#install-azure-functions-core-tools","text":"Text Only wget -q https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get update sudo apt-get install azure-functions-core-tools-3","title":"Install azure-functions-core-tools"},{"location":"tools/dotnetcore/","text":"Command to create project and solutions on Dotnetcore \u00b6 Create a folder for a project or clone empty project from the git \u00b6 Navigate to the folder from WSL console \u00b6 Text Only dotnet new sln func init process --dotnet --docker dotnet sln azure-function-dapr.sln add ./process/process.csproj cd process/ func new --name GetMessage --template \"HTTP Trigger\" --authlevel \"anonymous\" ps@Dev-WD MINGW64 /c/Projects/azure-function-dapr/process (main) $ func extensions install -p Dapr.AzureFunctions.Extension -v 0.12.0-p review01 dotnet build process.csproj -o bin --no-incremental Run Azure Function locally \u00b6 Text Only dapr run -a functionapp -p 3001 --components-path ./components/ --config ./config.yaml -- func host start Binding c# function with Dapr \u00b6 https://github.com/dapr/azure-functions-extension/blob/master/docs/triggers.md Reference: https://adamstorr.azurewebsites.net/blog/using-dotnet-cli-to-setup-aspnetcore-solution-and-run-with-tye https://www.skylinetechnologies.com/Blog/Skyline-Blog/February_2018/how-to-use-dot-net-core-cli-create-multi-project https://github.com/padamshrestha/tye","title":"Command to create project and solutions on Dotnetcore"},{"location":"tools/dotnetcore/#command-to-create-project-and-solutions-on-dotnetcore","text":"","title":"Command to create project and solutions on Dotnetcore"},{"location":"tools/dotnetcore/#create-a-folder-for-a-project-or-clone-empty-project-from-the-git","text":"","title":"Create a folder for a project or clone empty project from the git"},{"location":"tools/dotnetcore/#navigate-to-the-folder-from-wsl-console","text":"Text Only dotnet new sln func init process --dotnet --docker dotnet sln azure-function-dapr.sln add ./process/process.csproj cd process/ func new --name GetMessage --template \"HTTP Trigger\" --authlevel \"anonymous\" ps@Dev-WD MINGW64 /c/Projects/azure-function-dapr/process (main) $ func extensions install -p Dapr.AzureFunctions.Extension -v 0.12.0-p review01 dotnet build process.csproj -o bin --no-incremental","title":"Navigate to the folder from WSL console"},{"location":"tools/dotnetcore/#run-azure-function-locally","text":"Text Only dapr run -a functionapp -p 3001 --components-path ./components/ --config ./config.yaml -- func host start","title":"Run Azure Function locally"},{"location":"tools/dotnetcore/#binding-c-function-with-dapr","text":"https://github.com/dapr/azure-functions-extension/blob/master/docs/triggers.md Reference: https://adamstorr.azurewebsites.net/blog/using-dotnet-cli-to-setup-aspnetcore-solution-and-run-with-tye https://www.skylinetechnologies.com/Blog/Skyline-Blog/February_2018/how-to-use-dot-net-core-cli-create-multi-project https://github.com/padamshrestha/tye","title":"Binding c# function with Dapr"},{"location":"tools/error-solutions/","text":"Everyday error and fixes \u00b6 while yarn or npm install if you see python error \u00b6 Install python version 2.7 and set environment path c:\\python For multiple node version try using \u00b6 nvm For errors \u00b6 gyp verb check python checking for Python executable \"python2\" in the PATH gyp verb which failed Error: not found: python2 Text Only npm --add-python-to-path='true' --debug install --global windows-build-tools npm install --global node-gyp ReturnsAsync(null) creates a build error when using Moq for unit test \u00b6 There are two ReturnsAsync extension methods in Moq ReturnsExtensions class.They have following parameters: (this IReturns<TMock, Task<TResult>> mock, TResult value) (this IReturns<TMock, Task<TResult>> mock, Func<TResult> valueFunction) To fix this error you need to help compiler choose correct method overload - cast null to type of task's result (e.g. string): ReturnsAsync((string)null) Or you can pass value which is null string s = null; ... ReturnsAsync(s); Alternatively, ReturnsAsync(default(string)) Or user anonymous function ReturnsAsync(() => null) docker: Got permission denied while trying to connect to the Docker daemon socket at unix \u00b6 Bash sudo chmod 666 /var/run/docker.sock make usre you configure you 'user.name' and 'user.email' in git Ubuntu \u00b6 Bash git config --global user.email \"ps@gmail.com\" git config --global user.name \"PS\" If you see error while running python in docker try \u00b6 https://code.visualstudio.com/updates/v1_36#_launch-terminals-with-clean-environments . set terminal.integrated.inheritEnv: false Error: FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory \u00b6 Solution: export NODE_OPTIONS=\"--max-old-space-size=8192\" Error: Evaluated project metadata file could not be found for service ailgo.web \u00b6 Solution: Delete /obj folder from cs projects If it doesn't work, sometime auto update in mac install dotnet preview version. So uninstall them or use the right version. To uninstall Bash version = \"7.0.100-rc.2.22477.23\" sudo rm -rf /usr/local/share/dotnet/sdk/ $version sudo rm -rf /usr/local/share/dotnet/shared/Microsoft.NETCore.App/ $version sudo rm -rf /usr/local/share/dotnet/shared/Microsoft.AspNetCore.All/ $version sudo rm -rf /usr/local/share/dotnet/shared/Microsoft.AspNetCore.App/ $version sudo rm -rf /usr/local/share/dotnet/host/fxr/ $version Error ERROR [internal] load metadata for docker.io/library Example when building image on mac with image built on Windows OpenJDK maintained by Microsoft 8, 11, 17 Might need to specify the arch Bash Session docker build -t image_name --platform=linux/arm64 . !!!error ubuntu stuck at Default Kerberos version 5 realm: Here are the two possible ways to set DEBIAN_FRONTEND: Text Only 1 .Setting it via ARG as this is only available during build: ARG DEBIAN_FRONTEND=noninteractive RUN apt-get -qq install {your-package} 2. When required, setting it on-the-fly. RUN apt-get update && \\ DEBIAN_FRONTEND=noninteractive apt-get -qq install {your-package} Use DEBIAN_FRONTEND=noninteractive on install command ```console RUN apt-get update && \\ DEBIAN_FRONTEND=noninteractive apt-get install -y python3 python3-pip curl wget gcc krb5-config libkrb5-dev git ``` !!!error ModuleNotFoundError: No module named: If encounter ModuleNotFoundError from container despite package is installed Text Only Add to Dockerfile ```console ln -s /usr/bin/python3 /usr/bin/python && \\ rm -rf /var/lib/apt/lists/* ``` To check if he package is installed ```pip list``` !!!error PSQLException : The connection attempt failed. postgres Solution: somehow hive requires posgres user and user role despite env variables and it keeps restarting during schema initialization. So, Create user and schema manually Text Only SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Metastore connection URL: jdbc:postgresql://postgres_db:5432/metastore Metastore Connection Driver : org.postgresql.Driver Metastore connection User: postgres org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version. Underlying cause: org.postgresql.util.PSQLException : The connection attempt failed. SQL Error code: 0 org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version. at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:94) at org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(HiveSchemaTool.java:169) at org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore(HiveSchemaTool.java:475) at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:581) at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:567) at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1517) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:323) at org.apache.hadoop.util.RunJar.main(RunJar.java:236) Caused by: org.postgresql.util.PSQLException: The connection attempt failed. at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262) at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:66) at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:215) at org.postgresql.Driver.makeConnection(Driver.java:406) at org.postgresql.Driver.connect(Driver.java:274) at java.sql.DriverManager.getConnection(DriverManager.java:664) at java.sql.DriverManager.getConnection(DriverManager.java:247) at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:88) ... 11 more Caused by: java.net.UnknownHostException: postgres_db at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:607) at org.postgresql.core.PGStream.<init>(PGStream.java:64) at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:133) ... 18 more","title":"Everyday error and fixes"},{"location":"tools/error-solutions/#everyday-error-and-fixes","text":"","title":"Everyday error and fixes"},{"location":"tools/error-solutions/#while-yarn-or-npm-install-if-you-see-python-error","text":"Install python version 2.7 and set environment path c:\\python","title":"while yarn or npm install if you see python error"},{"location":"tools/error-solutions/#for-multiple-node-version-try-using","text":"nvm","title":"For multiple node version try using"},{"location":"tools/error-solutions/#for-errors","text":"gyp verb check python checking for Python executable \"python2\" in the PATH gyp verb which failed Error: not found: python2 Text Only npm --add-python-to-path='true' --debug install --global windows-build-tools npm install --global node-gyp","title":"For errors"},{"location":"tools/error-solutions/#returnsasyncnull-creates-a-build-error-when-using-moq-for-unit-test","text":"There are two ReturnsAsync extension methods in Moq ReturnsExtensions class.They have following parameters: (this IReturns<TMock, Task<TResult>> mock, TResult value) (this IReturns<TMock, Task<TResult>> mock, Func<TResult> valueFunction) To fix this error you need to help compiler choose correct method overload - cast null to type of task's result (e.g. string): ReturnsAsync((string)null) Or you can pass value which is null string s = null; ... ReturnsAsync(s); Alternatively, ReturnsAsync(default(string)) Or user anonymous function ReturnsAsync(() => null)","title":"ReturnsAsync(null) creates a build error when using Moq for unit test"},{"location":"tools/error-solutions/#docker-got-permission-denied-while-trying-to-connect-to-the-docker-daemon-socket-at-unix","text":"Bash sudo chmod 666 /var/run/docker.sock","title":"docker: Got permission denied while trying to connect to the Docker daemon socket at unix"},{"location":"tools/error-solutions/#make-usre-you-configure-you-username-and-useremail-in-git-ubuntu","text":"Bash git config --global user.email \"ps@gmail.com\" git config --global user.name \"PS\"","title":"make usre you configure you 'user.name' and 'user.email' in git Ubuntu"},{"location":"tools/error-solutions/#if-you-see-error-while-running-python-in-docker-try","text":"https://code.visualstudio.com/updates/v1_36#_launch-terminals-with-clean-environments . set terminal.integrated.inheritEnv: false","title":"If you see error while running python in docker try"},{"location":"tools/error-solutions/#error-fatal-error-ineffective-mark-compacts-near-heap-limit-allocation-failed-javascript-heap-out-of-memory","text":"Solution: export NODE_OPTIONS=\"--max-old-space-size=8192\"","title":"Error: FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory"},{"location":"tools/error-solutions/#error-evaluated-project-metadata-file-could-not-be-found-for-service-ailgoweb","text":"Solution: Delete /obj folder from cs projects If it doesn't work, sometime auto update in mac install dotnet preview version. So uninstall them or use the right version. To uninstall Bash version = \"7.0.100-rc.2.22477.23\" sudo rm -rf /usr/local/share/dotnet/sdk/ $version sudo rm -rf /usr/local/share/dotnet/shared/Microsoft.NETCore.App/ $version sudo rm -rf /usr/local/share/dotnet/shared/Microsoft.AspNetCore.All/ $version sudo rm -rf /usr/local/share/dotnet/shared/Microsoft.AspNetCore.App/ $version sudo rm -rf /usr/local/share/dotnet/host/fxr/ $version Error ERROR [internal] load metadata for docker.io/library Example when building image on mac with image built on Windows OpenJDK maintained by Microsoft 8, 11, 17 Might need to specify the arch Bash Session docker build -t image_name --platform=linux/arm64 . !!!error ubuntu stuck at Default Kerberos version 5 realm: Here are the two possible ways to set DEBIAN_FRONTEND: Text Only 1 .Setting it via ARG as this is only available during build: ARG DEBIAN_FRONTEND=noninteractive RUN apt-get -qq install {your-package} 2. When required, setting it on-the-fly. RUN apt-get update && \\ DEBIAN_FRONTEND=noninteractive apt-get -qq install {your-package} Use DEBIAN_FRONTEND=noninteractive on install command ```console RUN apt-get update && \\ DEBIAN_FRONTEND=noninteractive apt-get install -y python3 python3-pip curl wget gcc krb5-config libkrb5-dev git ``` !!!error ModuleNotFoundError: No module named: If encounter ModuleNotFoundError from container despite package is installed Text Only Add to Dockerfile ```console ln -s /usr/bin/python3 /usr/bin/python && \\ rm -rf /var/lib/apt/lists/* ``` To check if he package is installed ```pip list``` !!!error PSQLException : The connection attempt failed. postgres Solution: somehow hive requires posgres user and user role despite env variables and it keeps restarting during schema initialization. So, Create user and schema manually Text Only SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Metastore connection URL: jdbc:postgresql://postgres_db:5432/metastore Metastore Connection Driver : org.postgresql.Driver Metastore connection User: postgres org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version. Underlying cause: org.postgresql.util.PSQLException : The connection attempt failed. SQL Error code: 0 org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version. at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:94) at org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(HiveSchemaTool.java:169) at org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore(HiveSchemaTool.java:475) at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:581) at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:567) at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1517) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:323) at org.apache.hadoop.util.RunJar.main(RunJar.java:236) Caused by: org.postgresql.util.PSQLException: The connection attempt failed. at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262) at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:66) at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:215) at org.postgresql.Driver.makeConnection(Driver.java:406) at org.postgresql.Driver.connect(Driver.java:274) at java.sql.DriverManager.getConnection(DriverManager.java:664) at java.sql.DriverManager.getConnection(DriverManager.java:247) at org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:88) ... 11 more Caused by: java.net.UnknownHostException: postgres_db at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:607) at org.postgresql.core.PGStream.<init>(PGStream.java:64) at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:133) ... 18 more","title":"Error: Evaluated project metadata file could not be found for service ailgo.web"},{"location":"tools/git-help/","text":"Git \u00b6 when folder name is renamed only changing case eg algorithms to Algorithms, git won't see any change. Change the config to make git case sensitive \u00b6 PowerShell git config core . ignorecase false Remove stored password in Mac \u00b6 Use Keychain Access to delete the stored/saved password Remove stored password in Windows \u00b6 Show git stored git config \u00b6 Text Only git config --list --show-origin","title":"Git"},{"location":"tools/git-help/#git","text":"","title":"Git"},{"location":"tools/git-help/#when-folder-name-is-renamed-only-changing-case-eg-algorithms-to-algorithms-git-wont-see-any-change-change-the-config-to-make-git-case-sensitive","text":"PowerShell git config core . ignorecase false","title":"when folder name is renamed only changing case eg algorithms to Algorithms, git won't see any change. Change the config to make git case sensitive"},{"location":"tools/git-help/#remove-stored-password-in-mac","text":"Use Keychain Access to delete the stored/saved password","title":"Remove stored password in Mac"},{"location":"tools/git-help/#remove-stored-password-in-windows","text":"","title":"Remove stored password in Windows"},{"location":"tools/git-help/#show-git-stored-git-config","text":"Text Only git config --list --show-origin","title":"Show git stored git config"},{"location":"tools/helpful-links/","text":"\ud83d\udd17 Helpful links \u00b6 Mirroring a github repository VS Code Jupyter Docker Working with Jupyter VS Code Remote Docker Container VS Code Repo to Docker Angular course \u00b6 Fundamentals of Angular - Getting Started : Joe Eames Grafana Data Manipulation plugin \u00b6 https://volkovlabs.io/blog/data-manipulation-panel-plugin-for-grafana-97f9af2c67e0/ https://github.com/volkovlabs/volkovlabs-form-panel https://www.youtube.com/watch?v=DXALVG8GijM https://grafana.com/grafana/plugins/volkovlabs-form-panel/ http://localhost:3000/plugins/volkovlabs-form-panel","title":"\ud83d\udd17 Helpful links"},{"location":"tools/helpful-links/#helpful-links","text":"Mirroring a github repository VS Code Jupyter Docker Working with Jupyter VS Code Remote Docker Container VS Code Repo to Docker","title":"\ud83d\udd17 Helpful links"},{"location":"tools/helpful-links/#angular-course","text":"Fundamentals of Angular - Getting Started : Joe Eames","title":"Angular course"},{"location":"tools/helpful-links/#grafana-data-manipulation-plugin","text":"https://volkovlabs.io/blog/data-manipulation-panel-plugin-for-grafana-97f9af2c67e0/ https://github.com/volkovlabs/volkovlabs-form-panel https://www.youtube.com/watch?v=DXALVG8GijM https://grafana.com/grafana/plugins/volkovlabs-form-panel/ http://localhost:3000/plugins/volkovlabs-form-panel","title":"Grafana Data Manipulation plugin"},{"location":"tools/install-brave-ubuntu2004/","text":"Install Brave browser on Ubuntu 20.04 LTS \u00b6 Do system update of package index. \u00b6 Bash sudo apt update sudo apt -y install curl software-properties-common apt-transport-https Import repository GPG key: \u00b6 Bash curl -s https://brave-browser-apt-release.s3.brave.com/brave-core.asc | sudo apt-key --keyring /etc/apt/trusted.gpg.d/brave-browser-release.gpg add - Then add the Brave browser repository to the system. \u00b6 Bash echo \"deb [arch=amd64] https://brave-browser-apt-release.s3.brave.com/ stable main\" | sudo tee /etc/apt/sources.list.d/brave-browser-release.list Then update your package index and install Brave web browser on Ubuntu / Linux Mint. \u00b6 Bash sudo apt update sudo apt install -y brave-browser Launch browser from menu \u00b6","title":"Install Brave browser on Ubuntu 20.04 LTS"},{"location":"tools/install-brave-ubuntu2004/#install-brave-browser-on-ubuntu-2004-lts","text":"","title":"Install Brave browser on Ubuntu 20.04 LTS"},{"location":"tools/install-brave-ubuntu2004/#do-system-update-of-package-index","text":"Bash sudo apt update sudo apt -y install curl software-properties-common apt-transport-https","title":"Do system update of package index."},{"location":"tools/install-brave-ubuntu2004/#import-repository-gpg-key","text":"Bash curl -s https://brave-browser-apt-release.s3.brave.com/brave-core.asc | sudo apt-key --keyring /etc/apt/trusted.gpg.d/brave-browser-release.gpg add -","title":"Import repository GPG key:"},{"location":"tools/install-brave-ubuntu2004/#then-add-the-brave-browser-repository-to-the-system","text":"Bash echo \"deb [arch=amd64] https://brave-browser-apt-release.s3.brave.com/ stable main\" | sudo tee /etc/apt/sources.list.d/brave-browser-release.list","title":"Then add the Brave browser repository to the system."},{"location":"tools/install-brave-ubuntu2004/#then-update-your-package-index-and-install-brave-web-browser-on-ubuntu-linux-mint","text":"Bash sudo apt update sudo apt install -y brave-browser","title":"Then update your package index and install Brave web browser on Ubuntu / Linux Mint."},{"location":"tools/install-brave-ubuntu2004/#launch-browser-from-menu","text":"","title":"Launch browser from menu"},{"location":"tools/install-dotnet-sdk-armx64/","text":"Install dotnet skd on Mac m1 armx64 \u00b6 Follow the github link comment from robertsundstrom and chunghakngor Remove installed runtimes and SDKs: Bash Session sudo rm -r /usr/local/share/dotnet sudo rm -r /etc/dotnet Then download the latest SDK installers: .NET 6 (ARM64) .NET 6 (X64) .NET 5 (X64) .NET Core 3.1 (X64) The previous versions have been doing updated with support for multi-architecture targeting. Run the instwllers in the order of the list. From top to bottom. Check /usr/local/share/dotnet/x64/sdk to see that the SDKs are there. And also run: Bash Session cat /etc/dotnet The directory should contain a file for each architecture containing the SDK path of each. Keep in mind: Bash Session dotnet --list-sdks Will only show .NET 6 on ARM64. The reason is that the dotnet command belongs to the Silicon/ARM64 SDK. When run Bash Session dotnet --info If it running on x64, then the RID is shown as osx-x64 and if it's arm64 it should be osx-arm64 If you change the symbolic link by removing the current one and changing it to the arm64. You can run the following commands: Bash Session rm /usr/local/bin/dotnet ln -s /usr/local/share/dotnet/dotnet /usr/local/bin/dotnet While compiling the Project from Visual Studio if there is and error Sharing violation on path /Volumes Make sure to have all the project files in Apple formatted disk. There are some issues with ExFAT. Note Text Only Make sure tye and dapr are installed and initialized again.","title":"Install dotnet skd on Mac m1 armx64"},{"location":"tools/install-dotnet-sdk-armx64/#install-dotnet-skd-on-mac-m1-armx64","text":"Follow the github link comment from robertsundstrom and chunghakngor Remove installed runtimes and SDKs: Bash Session sudo rm -r /usr/local/share/dotnet sudo rm -r /etc/dotnet Then download the latest SDK installers: .NET 6 (ARM64) .NET 6 (X64) .NET 5 (X64) .NET Core 3.1 (X64) The previous versions have been doing updated with support for multi-architecture targeting. Run the instwllers in the order of the list. From top to bottom. Check /usr/local/share/dotnet/x64/sdk to see that the SDKs are there. And also run: Bash Session cat /etc/dotnet The directory should contain a file for each architecture containing the SDK path of each. Keep in mind: Bash Session dotnet --list-sdks Will only show .NET 6 on ARM64. The reason is that the dotnet command belongs to the Silicon/ARM64 SDK. When run Bash Session dotnet --info If it running on x64, then the RID is shown as osx-x64 and if it's arm64 it should be osx-arm64 If you change the symbolic link by removing the current one and changing it to the arm64. You can run the following commands: Bash Session rm /usr/local/bin/dotnet ln -s /usr/local/share/dotnet/dotnet /usr/local/bin/dotnet While compiling the Project from Visual Studio if there is and error Sharing violation on path /Volumes Make sure to have all the project files in Apple formatted disk. There are some issues with ExFAT. Note Text Only Make sure tye and dapr are installed and initialized again.","title":"Install dotnet skd on Mac m1 armx64"},{"location":"tools/install-vscode-ubuntu/","text":"Install VS Code on Ubuntu \u00b6 First, update the packages index and install the dependencies by typing: \u00b6 Bash sudo apt update sudo apt install software-properties-common apt-transport-https wget Next, import the Microsoft GPG key using the following wget command: \u00b6 Bash wget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add - And enable the Visual Studio Code repository by typing: Bash sudo add-apt-repository \"deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\" Once the apt repository is enabled, install the latest version of Visual Studio Code with: \u00b6 Bash sudo apt update sudo apt install code Starting VS Code \u00b6 Bash code .","title":"Install VS Code on Ubuntu"},{"location":"tools/install-vscode-ubuntu/#install-vs-code-on-ubuntu","text":"","title":"Install VS Code on Ubuntu"},{"location":"tools/install-vscode-ubuntu/#first-update-the-packages-index-and-install-the-dependencies-by-typing","text":"Bash sudo apt update sudo apt install software-properties-common apt-transport-https wget","title":"First, update the packages index and install the dependencies by typing:"},{"location":"tools/install-vscode-ubuntu/#next-import-the-microsoft-gpg-key-using-the-following-wget-command","text":"Bash wget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add - And enable the Visual Studio Code repository by typing: Bash sudo add-apt-repository \"deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\"","title":"Next, import the Microsoft GPG key using the following wget command:"},{"location":"tools/install-vscode-ubuntu/#once-the-apt-repository-is-enabled-install-the-latest-version-of-visual-studio-code-with","text":"Bash sudo apt update sudo apt install code","title":"Once the apt repository is enabled, install the latest version of Visual Studio Code with:"},{"location":"tools/install-vscode-ubuntu/#starting-vs-code","text":"Bash code .","title":"Starting VS Code"},{"location":"tools/keyboard-shortcuts/","text":"Keyboard shortcuts for everyday tasks \u00b6 ConEmu \u00b6 Command Description Win + Shift + w open new tab with dialog Win + w new console tab ConEmu shortcut reference VS Code \u00b6 Command Description Ctrl + p open file by name Ctrl + Shift + p run task Ctrl + b toggle left bar Ctrl + j toggle terminal VS Code shortcut reference Visual Studio \u00b6 Command Description Ctrl + q Ctrl + t quick search in code Ctrl + q quick launch Ctrl + k Ctrl + d format document Ctrl + k Ctrl + k format selection Ctrl + + autocomplete prop + Tab + Tab add property to class Ctrl + m + o collapse all Ctrl + m + l un-collapse all Ctrl + m + m collapse function Ctrl + k + k bookmark line Ctrl + k + n next bookmark Ctrl + k + p previous bookmark Ctrl + k + c bulk comment Ctrl + k + u bulk uncomment Ctrl + - Navigate backward Ctrl + t x where x: f - file r - recent file m - method Visual Studio shortcut reference Microsoft Windows Terminal \u00b6 Command Description Ctrl + Alt + t open terminal after hotkey setup Shift + Alt + d split screen to right Shift + Alt + = split screen to right Shift + Alt + - split screen to bottom Alt + arrow Switch pane left, right, top, bottom Alt + Shift + arrow to resize in the direction Ctrl + Shift + w or type exit to close the pane Ctrl + Alt + num switch to tab number Ctrl + Shift + mouse-scroll change opacity Ctrl + Shift + p command bar (requires preview version: 1.2.2022.0) Reference Hotkeys manual settings.json JSON \"profiles\" : { \"defaults\" : { // Put settings here that you want to apply to all profiles. \"colorScheme\" : \"Campbell Powershell\" } } Tip use windows terminal in vs code, replace in settings \"terminal.integrated.shell.windows\": \"C:\\Users\\ \\AppData\\Local\\Microsoft\\WindowsApps\\wt.exe\",","title":"Keyboard shortcuts for everyday tasks "},{"location":"tools/keyboard-shortcuts/#keyboard-shortcuts-for-everyday-tasks","text":"","title":"Keyboard shortcuts for everyday tasks"},{"location":"tools/keyboard-shortcuts/#conemu","text":"Command Description Win + Shift + w open new tab with dialog Win + w new console tab ConEmu shortcut reference","title":"ConEmu"},{"location":"tools/keyboard-shortcuts/#vs-code","text":"Command Description Ctrl + p open file by name Ctrl + Shift + p run task Ctrl + b toggle left bar Ctrl + j toggle terminal VS Code shortcut reference","title":"VS Code"},{"location":"tools/keyboard-shortcuts/#visual-studio","text":"Command Description Ctrl + q Ctrl + t quick search in code Ctrl + q quick launch Ctrl + k Ctrl + d format document Ctrl + k Ctrl + k format selection Ctrl + + autocomplete prop + Tab + Tab add property to class Ctrl + m + o collapse all Ctrl + m + l un-collapse all Ctrl + m + m collapse function Ctrl + k + k bookmark line Ctrl + k + n next bookmark Ctrl + k + p previous bookmark Ctrl + k + c bulk comment Ctrl + k + u bulk uncomment Ctrl + - Navigate backward Ctrl + t x where x: f - file r - recent file m - method Visual Studio shortcut reference","title":"Visual Studio"},{"location":"tools/keyboard-shortcuts/#microsoft-windows-terminal","text":"Command Description Ctrl + Alt + t open terminal after hotkey setup Shift + Alt + d split screen to right Shift + Alt + = split screen to right Shift + Alt + - split screen to bottom Alt + arrow Switch pane left, right, top, bottom Alt + Shift + arrow to resize in the direction Ctrl + Shift + w or type exit to close the pane Ctrl + Alt + num switch to tab number Ctrl + Shift + mouse-scroll change opacity Ctrl + Shift + p command bar (requires preview version: 1.2.2022.0) Reference Hotkeys manual settings.json JSON \"profiles\" : { \"defaults\" : { // Put settings here that you want to apply to all profiles. \"colorScheme\" : \"Campbell Powershell\" } } Tip use windows terminal in vs code, replace in settings \"terminal.integrated.shell.windows\": \"C:\\Users\\ \\AppData\\Local\\Microsoft\\WindowsApps\\wt.exe\",","title":"Microsoft Windows Terminal"},{"location":"tools/limited_access_pat_token/","tags":["git"],"text":"Create Git PAT token with limited access. \u00b6 With newly introduced feature from Github, it is now possible to create PAT token with fine grained access to repositories with expiration date on them. For Repository pull and push actions, w/r access need to be granted on \"Content\" section The PAT token can be used to access repositories just using urls url: https://PAT_TOKEN@github.com/user_name/repo_name.git","title":"Create Git PAT token with limited access."},{"location":"tools/limited_access_pat_token/#create-git-pat-token-with-limited-access","text":"With newly introduced feature from Github, it is now possible to create PAT token with fine grained access to repositories with expiration date on them. For Repository pull and push actions, w/r access need to be granted on \"Content\" section The PAT token can be used to access repositories just using urls url: https://PAT_TOKEN@github.com/user_name/repo_name.git","title":"Create Git PAT token with limited access."},{"location":"tools/linux-commands/","text":"Useful linux commands \u00b6 ping \u00b6 Install apt install iputils-ping or apt-get update && apt-get install -y iputils-ping Use ping ip_address netstat \u00b6 Install apt install net-tools View routing table netstat -nr Display network interface netstat -ai Show network connections netstat -ant Show network services netstat -pnltu","title":"Useful linux commands"},{"location":"tools/linux-commands/#useful-linux-commands","text":"","title":"Useful linux commands"},{"location":"tools/linux-commands/#ping","text":"Install apt install iputils-ping or apt-get update && apt-get install -y iputils-ping Use ping ip_address","title":"ping"},{"location":"tools/linux-commands/#netstat","text":"Install apt install net-tools View routing table netstat -nr Display network interface netstat -ai Show network connections netstat -ant Show network services netstat -pnltu","title":"netstat"},{"location":"tools/multiple-remote-git/","tags":["git"],"text":"Multiple remote git configuration for repository \u00b6 Sometimes there could be need to maintain the code in multiple repositories. If you are looking for that please proceed. The repository can have multiple remote git repositories - Github - Gitlab Setup \u00b6 Create or clone repository from Github Open the repository with SourceTree Create new repository and init in Gitlab Add remote repository url of newly crated repository Note: Make sure Gitlab might have default branch as master and might be protected. Once the code is pushed change main as default branch and delete master. Update code \u00b6 Update code, commit and push. This will push to default repo gitlab To update secondary repository or github in this case, push to github origin If you are using secondary machine, you will have to follow the same procedure","title":"Multiple remote git configuration for repository"},{"location":"tools/multiple-remote-git/#multiple-remote-git-configuration-for-repository","text":"Sometimes there could be need to maintain the code in multiple repositories. If you are looking for that please proceed. The repository can have multiple remote git repositories - Github - Gitlab","title":"Multiple remote git configuration for repository"},{"location":"tools/multiple-remote-git/#setup","text":"Create or clone repository from Github Open the repository with SourceTree Create new repository and init in Gitlab Add remote repository url of newly crated repository Note: Make sure Gitlab might have default branch as master and might be protected. Once the code is pushed change main as default branch and delete master.","title":"Setup"},{"location":"tools/multiple-remote-git/#update-code","text":"Update code, commit and push. This will push to default repo gitlab To update secondary repository or github in this case, push to github origin If you are using secondary machine, you will have to follow the same procedure","title":"Update code"},{"location":"tools/nuget/","text":"Nuget Package Manger \u00b6 Add nuget source in MS Azure \u00b6 PowerShell nuget sources Add -Name \"ps-feed\" -Source \"https://msazure.pkgs.visualstudio.com/D365/_packaging/ps-feed1/nuget/v3/index.json\" -username v-padshr @microsft . com -password pswd where pswd is PAT token Helpful tool to modify nuget packages \u00b6 Nuget Explorer It requires PAT token as well to publish the packages to MS Azure We can have local Nuget Repo and install specific version from there \u00b6","title":"Nuget Package Manger"},{"location":"tools/nuget/#nuget-package-manger","text":"","title":"Nuget Package Manger"},{"location":"tools/nuget/#add-nuget-source-in-ms-azure","text":"PowerShell nuget sources Add -Name \"ps-feed\" -Source \"https://msazure.pkgs.visualstudio.com/D365/_packaging/ps-feed1/nuget/v3/index.json\" -username v-padshr @microsft . com -password pswd where pswd is PAT token","title":"Add nuget source in MS Azure"},{"location":"tools/nuget/#helpful-tool-to-modify-nuget-packages","text":"Nuget Explorer It requires PAT token as well to publish the packages to MS Azure","title":"Helpful tool to modify nuget packages"},{"location":"tools/nuget/#we-can-have-local-nuget-repo-and-install-specific-version-from-there","text":"","title":"We can have local Nuget Repo and install specific version from there"},{"location":"tools/rename-dotnet-3-6/","text":"Migrate Dotnet Core 3.1 to DotNet 6.0 \u00b6 If used global.json, update the version \u00b6 JSON { \"sdk\" : { - \"version\" : \"3.1.200\" + \"version\" : \"6.0.100\" } } Update csproj file with \u00b6 Text Only <Project Sdk=\"Microsoft.NET.Sdk.Web\"> <PropertyGroup> - <TargetFramework>netcoreapp3.1</TargetFramework> + <TargetFramework>net6.0</TargetFramework> </PropertyGroup> </Project> Update all nuget package \u00b6 Install dotnet-outdated-tools from nuget repo Python dotnet tool install -- global dotnet - outdated - tool -- version 4.2.0 Run update command to update all the packages Python dotnet - outdated -- upgrade The command will automatically update all the package.","title":"Migrate Dotnet Core 3.1 to DotNet 6.0"},{"location":"tools/rename-dotnet-3-6/#migrate-dotnet-core-31-to-dotnet-60","text":"","title":"Migrate Dotnet Core 3.1 to DotNet 6.0"},{"location":"tools/rename-dotnet-3-6/#if-used-globaljson-update-the-version","text":"JSON { \"sdk\" : { - \"version\" : \"3.1.200\" + \"version\" : \"6.0.100\" } }","title":"If used global.json, update the version"},{"location":"tools/rename-dotnet-3-6/#update-csproj-file-with","text":"Text Only <Project Sdk=\"Microsoft.NET.Sdk.Web\"> <PropertyGroup> - <TargetFramework>netcoreapp3.1</TargetFramework> + <TargetFramework>net6.0</TargetFramework> </PropertyGroup> </Project>","title":"Update csproj file with"},{"location":"tools/rename-dotnet-3-6/#update-all-nuget-package","text":"Install dotnet-outdated-tools from nuget repo Python dotnet tool install -- global dotnet - outdated - tool -- version 4.2.0 Run update command to update all the packages Python dotnet - outdated -- upgrade The command will automatically update all the package.","title":"Update all nuget package"},{"location":"tools/temp-notes/","text":"Temporary Notes \u00b6 sudo apt update sudo apt -y upgrade python3 -V apt-get install python3-pip pip3 install jupyter-repo2docker jupyter-repo2docker https://github.com/padamshrestha/vscode-jupyter-docker \u00b6 docker login docker images docker tag r2dhttps-3a-2f-2fgithub-2ecom-2fpadamshrestha-2fvscode-2djupyter-2ddocker78a2383 padamshrestha/vsjd-generated docker push padamshrestha/vsjd-generated \u00b6 docker run -it --rm -p 8888:8888 -v \\((pwd)/work2:/home/jovyan/work -e NB_UID=1234 -e NB_GID=1234 --user root jupyter/pyspark-notebook start.sh Set username to: jovyan usermod: no changes Set jovyan UID to: 1234 Add jovyan to group: 1234 Executing the command: bash jovyan@f6ca893feb55:~\\) echo \"foo\" > work/foo bash: work/foo: Permission denied jovyan@f6ca893feb55:~$ cd work/ jovyan@f6ca893feb55:~/work$ ll total 4 drwxr-xr-x 2 root root 40 May 4 15:45 ./ drwsrwsr-x 1 jovyan jovyan 4096 Apr 25 18:21 ../ docker run -it --rm -p 8888:8888 -v /mnt/c/Projects/PS/pyspark-notebook:/home/jovyan/work -e NB_UID=1234 -e NB_GID=1234 --user root jupyter/pyspark-notebook start.sh C:\\Projects\\PS\\pyspark-notebook Run command from ubuntu or wsl \u00b6 docker run -p 8888:8888 -p 4040:4040 -v /mnt/c/Projects/PS/pyspark-notebook:/home/jovyan/work --name spark jupyter/pyspark-notebook docker run -p 8888:8888 -p 4040:4040 -v /mnt/c/Projects/pyspark-practice-notebook/:/home/jovyan/work --name spark jupyter/pyspark-notebook docker run -p 8888:8888 -p 4040:4040 -v /mnt/c/Projects/PS/pyspark-practice-notebook/:/home/jovyan/work --user root -e GRANT_SUDO=yes --name spark jupyter/pyspark-notebook docker exec -it spark /bin/bash TD Ameritrade \u00b6 https://developer.tdameritrade.com/guides Sigma Coding Sigma Coding - Github How to use the TD Ameritrade APi | Part 1 How to use TD Ameritrade Streaming API Jupyter for https://github.com/padamshrestha/finance-news-aggregator \u00b6 pip install -e . pip install fake-useragent from pprint import pprint from finnews.client import News Note from some post \u00b6 I found out that I can override the jupyter_notebook_config.py in jupyter/base-notebook, and move it to the /home/jovyan/.jupyter/ to get executed on load So I added these lines in jupyter_notebook_config.py if 'PASSWORD' in os.environ: password = os.environ['PASSWORD'] if password: print(\"Password updated\") c.NotebookApp.password = passwd(password) del os.environ['PASSWORD'] And move them when container starts up COPY jupyter_notebook_config.py /home/jovyan/.jupyter/jupyter_notebook_config.py Install packages from Url \u00b6 Bash $ sudo apt-get update $ wget https://dl.winehq.org/wine/source/1.9/wine-1.9.19.tar.bz2 $ sudo tar xzvf wine-1.9.19.tar.bz2 $ cd wine-1.9.19 $ ./configure $ make $ sudo make install $ sudo reboot Or as a user Bash $ untar and cd $ ./configure --prefix = /usr $ make $ sudo make install Delete folder in ubuntu \u00b6 rm -rfv folder_name For TA TA using Panda https://github.com/twopirllc/pandas-ta/blob/master/examples/example.ipynb https://github.com/bukosabino/ta https://towardsdatascience.com/technical-analysis-library-to-financial-datasets-with-pandas-python-4b2b390d3543 Detecting Technical Chart https://medium.com/automation-generation/algorithmically-detecting-and-trading-technical-chart-patterns-with-python-c577b3a396ed https://alpaca.markets/learn/algorithmic-trading-chart-pattern-python/ Prediction on streaming data https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/ Prediction https://www.kdnuggets.com/2020/01/stock-market-forecasting-time-series-analysis.html Financial charting: https://github.com/matplotlib/mplfinance Pattern Recognizer https://github.com/RiccardoM/Forex-and-Stock-Python-Pattern-Recognizer For AlphaVantage Stock Data https://github.com/RomelTorres/alpha_vantage https://github.com/twopirllc/AlphaVantageAPI For ACID transactions https://docs.delta.io/latest/delta-intro.html For Stock information https://www.investopedia.com/terms/p/protective-put.asp Awesome opensource project Awesome opensource Trade Analysis App https://www.macroaxis.com/invest/predict/MSFT/Microsoft Top 50 matplotlib visualization https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/ Arima model https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/ Plotting with Python https://www.kaggle.com/python10pm/plotting-with-python-learn-80-plots-step-by-step python3 /mnt/c/Projects/PS/financial-analysis-stack/scripts/merge_stock_histories.py /mnt/c/Users/v-padshr/Downloads/75752_1304789_bundle_archive/ /mnt/c/Projects/PS/financial-analysis-stack/data/stock_histories.csv For Data https://www.kaggle.com/qks1lver/amex-nyse-nasdaq-stock-histories gzip -c filename > filename.gz docker stop \u2013f $(docker ps \u2013a -q) docker rm \u2013f $(docker ps \u2013a -q)","title":"Temporary Notes"},{"location":"tools/temp-notes/#temporary-notes","text":"sudo apt update sudo apt -y upgrade python3 -V apt-get install python3-pip pip3 install jupyter-repo2docker jupyter-repo2docker https://github.com/padamshrestha/vscode-jupyter-docker","title":"Temporary Notes"},{"location":"tools/temp-notes/#_1","text":"docker login docker images docker tag r2dhttps-3a-2f-2fgithub-2ecom-2fpadamshrestha-2fvscode-2djupyter-2ddocker78a2383 padamshrestha/vsjd-generated docker push padamshrestha/vsjd-generated","title":""},{"location":"tools/temp-notes/#_2","text":"docker run -it --rm -p 8888:8888 -v \\((pwd)/work2:/home/jovyan/work -e NB_UID=1234 -e NB_GID=1234 --user root jupyter/pyspark-notebook start.sh Set username to: jovyan usermod: no changes Set jovyan UID to: 1234 Add jovyan to group: 1234 Executing the command: bash jovyan@f6ca893feb55:~\\) echo \"foo\" > work/foo bash: work/foo: Permission denied jovyan@f6ca893feb55:~$ cd work/ jovyan@f6ca893feb55:~/work$ ll total 4 drwxr-xr-x 2 root root 40 May 4 15:45 ./ drwsrwsr-x 1 jovyan jovyan 4096 Apr 25 18:21 ../ docker run -it --rm -p 8888:8888 -v /mnt/c/Projects/PS/pyspark-notebook:/home/jovyan/work -e NB_UID=1234 -e NB_GID=1234 --user root jupyter/pyspark-notebook start.sh C:\\Projects\\PS\\pyspark-notebook","title":""},{"location":"tools/temp-notes/#run-command-from-ubuntu-or-wsl","text":"docker run -p 8888:8888 -p 4040:4040 -v /mnt/c/Projects/PS/pyspark-notebook:/home/jovyan/work --name spark jupyter/pyspark-notebook docker run -p 8888:8888 -p 4040:4040 -v /mnt/c/Projects/pyspark-practice-notebook/:/home/jovyan/work --name spark jupyter/pyspark-notebook docker run -p 8888:8888 -p 4040:4040 -v /mnt/c/Projects/PS/pyspark-practice-notebook/:/home/jovyan/work --user root -e GRANT_SUDO=yes --name spark jupyter/pyspark-notebook docker exec -it spark /bin/bash","title":"Run command from ubuntu or wsl"},{"location":"tools/temp-notes/#td-ameritrade","text":"https://developer.tdameritrade.com/guides Sigma Coding Sigma Coding - Github How to use the TD Ameritrade APi | Part 1 How to use TD Ameritrade Streaming API","title":"TD Ameritrade"},{"location":"tools/temp-notes/#jupyter-for-httpsgithubcompadamshresthafinance-news-aggregator","text":"pip install -e . pip install fake-useragent from pprint import pprint from finnews.client import News","title":"Jupyter for https://github.com/padamshrestha/finance-news-aggregator"},{"location":"tools/temp-notes/#note-from-some-post","text":"I found out that I can override the jupyter_notebook_config.py in jupyter/base-notebook, and move it to the /home/jovyan/.jupyter/ to get executed on load So I added these lines in jupyter_notebook_config.py if 'PASSWORD' in os.environ: password = os.environ['PASSWORD'] if password: print(\"Password updated\") c.NotebookApp.password = passwd(password) del os.environ['PASSWORD'] And move them when container starts up COPY jupyter_notebook_config.py /home/jovyan/.jupyter/jupyter_notebook_config.py","title":"Note from some post"},{"location":"tools/temp-notes/#install-packages-from-url","text":"Bash $ sudo apt-get update $ wget https://dl.winehq.org/wine/source/1.9/wine-1.9.19.tar.bz2 $ sudo tar xzvf wine-1.9.19.tar.bz2 $ cd wine-1.9.19 $ ./configure $ make $ sudo make install $ sudo reboot Or as a user Bash $ untar and cd $ ./configure --prefix = /usr $ make $ sudo make install","title":"Install packages from Url"},{"location":"tools/temp-notes/#delete-folder-in-ubuntu","text":"rm -rfv folder_name For TA TA using Panda https://github.com/twopirllc/pandas-ta/blob/master/examples/example.ipynb https://github.com/bukosabino/ta https://towardsdatascience.com/technical-analysis-library-to-financial-datasets-with-pandas-python-4b2b390d3543 Detecting Technical Chart https://medium.com/automation-generation/algorithmically-detecting-and-trading-technical-chart-patterns-with-python-c577b3a396ed https://alpaca.markets/learn/algorithmic-trading-chart-pattern-python/ Prediction on streaming data https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/ Prediction https://www.kdnuggets.com/2020/01/stock-market-forecasting-time-series-analysis.html Financial charting: https://github.com/matplotlib/mplfinance Pattern Recognizer https://github.com/RiccardoM/Forex-and-Stock-Python-Pattern-Recognizer For AlphaVantage Stock Data https://github.com/RomelTorres/alpha_vantage https://github.com/twopirllc/AlphaVantageAPI For ACID transactions https://docs.delta.io/latest/delta-intro.html For Stock information https://www.investopedia.com/terms/p/protective-put.asp Awesome opensource project Awesome opensource Trade Analysis App https://www.macroaxis.com/invest/predict/MSFT/Microsoft Top 50 matplotlib visualization https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/ Arima model https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/ Plotting with Python https://www.kaggle.com/python10pm/plotting-with-python-learn-80-plots-step-by-step python3 /mnt/c/Projects/PS/financial-analysis-stack/scripts/merge_stock_histories.py /mnt/c/Users/v-padshr/Downloads/75752_1304789_bundle_archive/ /mnt/c/Projects/PS/financial-analysis-stack/data/stock_histories.csv For Data https://www.kaggle.com/qks1lver/amex-nyse-nasdaq-stock-histories gzip -c filename > filename.gz docker stop \u2013f $(docker ps \u2013a -q) docker rm \u2013f $(docker ps \u2013a -q)","title":"Delete folder in ubuntu"},{"location":"tools/useful-online-tools/","text":"\u2692 Useful online tools \u00b6 Converters (Docker run to Compose) \u00b6 Composerize https://8gwifi.org/dc1.jsp Online Terminals \u00b6 https://0cloud0.com/playground Online Flowchart Diagrams \u00b6 [Online Diagrams] - https://whimsical.com - draw.io PDF to MD \u00b6 PDF to MD Cron config \u00b6 Crontab Az Function Cron cheat sheet Get proper command for linux distros \u00b6 Command not found Git icons \u00b6 Git profile status generator Git icons Online AI icon generator \u00b6 dall-e Online Youtube video download \u00b6 Video Video to MP3 Subtitle Convert Data Sets \u00b6 Google dateset Tip Search in github by filename filename:spark-defaults.conf Tip If vs code drawio doesn't have image, it can be copied from https://app.diagrams.net/","title":"\u2692 Useful online tools"},{"location":"tools/useful-online-tools/#useful-online-tools","text":"","title":"\u2692 Useful online tools"},{"location":"tools/useful-online-tools/#converters-docker-run-to-compose","text":"Composerize https://8gwifi.org/dc1.jsp","title":"Converters (Docker run to Compose)"},{"location":"tools/useful-online-tools/#online-terminals","text":"https://0cloud0.com/playground","title":"Online Terminals"},{"location":"tools/useful-online-tools/#online-flowchart-diagrams","text":"[Online Diagrams] - https://whimsical.com - draw.io","title":"Online Flowchart Diagrams"},{"location":"tools/useful-online-tools/#pdf-to-md","text":"PDF to MD","title":"PDF to MD"},{"location":"tools/useful-online-tools/#cron-config","text":"Crontab Az Function Cron cheat sheet","title":"Cron config"},{"location":"tools/useful-online-tools/#get-proper-command-for-linux-distros","text":"Command not found","title":"Get proper command for linux distros"},{"location":"tools/useful-online-tools/#git-icons","text":"Git profile status generator Git icons","title":"Git icons"},{"location":"tools/useful-online-tools/#online-ai-icon-generator","text":"dall-e","title":"Online AI icon generator"},{"location":"tools/useful-online-tools/#online-youtube-video-download","text":"Video Video to MP3 Subtitle Convert","title":"Online Youtube video download"},{"location":"tools/useful-online-tools/#data-sets","text":"Google dateset Tip Search in github by filename filename:spark-defaults.conf Tip If vs code drawio doesn't have image, it can be copied from https://app.diagrams.net/","title":"Data Sets"},{"location":"tools/windows-tricks/","text":"Windows tricks \u00b6 Edit environment variables without admin access \u00b6 Execute command in run window PowerShell rundll32 sysdm . cpl , EditEnvironmentVariables Alternatively environment variables can be set using powershell \u00b6 PowerShell [Environment] :: SetEnvironmentVariable ( \"HOME\" , \"C:\\path\\to\\home\" , \"User\" ) List all the variables PowerShell Get-ChildItem Env : Or, show particular environment variable PowerShell Get-ChildItem Env : HOME","title":"Windows tricks"},{"location":"tools/windows-tricks/#windows-tricks","text":"","title":"Windows tricks"},{"location":"tools/windows-tricks/#edit-environment-variables-without-admin-access","text":"Execute command in run window PowerShell rundll32 sysdm . cpl , EditEnvironmentVariables","title":"Edit environment variables without admin access"},{"location":"tools/windows-tricks/#alternatively-environment-variables-can-be-set-using-powershell","text":"PowerShell [Environment] :: SetEnvironmentVariable ( \"HOME\" , \"C:\\path\\to\\home\" , \"User\" ) List all the variables PowerShell Get-ChildItem Env : Or, show particular environment variable PowerShell Get-ChildItem Env : HOME","title":"Alternatively environment variables can be set using powershell"},{"location":"trading/ichimoku_cloud/","text":"Ichimoku Cloud Master \u00b6 Ichimoku Cloud Master aims to provide the ichimoku trader with easy alert functionality to not miss out on valuable trade setups. The key purpose of this script is to better visualise crucial moments in Ichimoku trading. These alerts should not be used for botting in my opinion as they always need a human to confirm the ichimoku market structure. For example, is the Kijun-Sen flat and too far away from price? A good ichimoku trader will not enter at such a point in time. Explanation of script: \u00b6 Chikou(lagging span): pink line, this is price plotted 26 bars ago. People ignore the power of this it is crucial to see how chikou behaves towards past price action as seen in the chart below where we got an entry at red arrow because chikou bounced from past fractal bottom. Kijun-Sen(base line): Black line or color coded line. This is the equilibrium of last 26 candles. To me this is the most important line in the system as it attracts price. Kijun = (Highest high of 26 periods + Lowest low of 26 periods) \u00f7 2 Tenkan-Sen(conversion line): Blue line. This is the equilibrium of last 9 candles. In a strong uptrend price stays above this line. Tenkan = (Highest high of 9 periods + Lowest low of 9 periods) \u00f7 2 Senkou A (Leading span A)= Pink cloud line, this is the average of the 2 components projected 26 bars in the future. Senkou A = (Tenkan + Kijun) \u00f7 2 Senkou B (Leading span B) = Green cloud line, this is the 52 day equilibrium projected 26 bars in the future. Senkou B = (Highest high of prior 52 periods + Lowest low of prior 52 periods) \u00f7 2 Notice how the distance between Chikou and the cloud is also 52 bars. This is all part of Hosoda's numbers which I am not going to explain here. Fractals: These are the black triangles you find at key turning point. If you want to know how they work reseach williams fractals. I've used fractals with a period of 9 as it is an ichimoku number. These fractals are useful when working with ichimoku wave theory. Again I will not explain that here but in further education Fractal Support: Ability to extend lines from the fractals which can be used as an entry/exit mechanism in your trading. For example wait for tenkan to cross kijun and then enter on fractal breakout. Signals: \u00b6 Crossing of Chikou (lagging span) with past Kijun-Sen: this will color code the Bars / Kijun-Sen (you can turn this off in options) The script also has a signal for this, this will be the green and purple diamonds. Where green is bullish and purple is bearish. wy is this important? When current price plotted 26 candles back (chikou) crosses over the past equilibrium (kijun-sen) this usualy means price has moved past resistance levels where sellers come in. This indicates a switch in market structure and price is bullish from this point, this is the same in the other direction. Kumo Twist: when the kumo cloud (future) has a crossover from for example green to red (bull to bear). The script plots these using the colored cross symbols as seen in the picture above. A chikou cross + a Kumo twist at same bar of next to eachother below the cloud can be a great entry sign: this would be an entry after cross in the chart above. Kijun Bounce: when in an uptrend the price retraces back to Kijun-Sen and starts to go back up. These are marked by the yellow circles as seen in chart below: low below Kijun-Sen and close above it trong Trend: when Tenkan is above Kijun, price above cloud, future cloud green, chikou above close, chikou above Kijun we establish a strong bullish trend. For bearish the exact opposite. The script has a function to send an alert at the start of such trends and to plot them with small colored circles above the bars. Customisation: \u00b6 I've added options to disable specific aspects of the indicator for those traders who do not want to use all aspects of the indicator. In the customisation tab I've given each part a clear title so you can use your own colors/shapes. The perfect entry? \u00b6 Further info: Look into my education pane, I will be adding education in the future. The chance of me making a more advanced version of the script including line forecasting etc is rather high so watch out for that. For those who want to master this system I recommend reading the book: How to make money with the ichimoku system by Balkrishna M. Sadekar Or the originals books by Hosoda the inventor of Ichimoku if you can get your hands on them and can read Japanese. Almost all info about the ichimoku system you find on the internet will lose you money because they reduce the system to simple signals that do not generate money. I will be providing educational material on tradingview using this indicator. JavaScript //@version=5 indicator ( title = 'Ichimoku Cloud Master' , shorttitle = 'Ichimoku Master' , overlay = true , timeframe = \"\" , timeframe_gaps = false ) //----------[ Ichimoku Basic Script //inputs tenkanPeriods = input . int ( 9 , minval = 1 , title = 'Tenkan-Sen Length' , group = 'Ichimoku' ) kijunPeriods = input . int ( 26 , minval = 1 , title = 'Kijun-Sen Length' , group = 'Ichimoku' ) laggingSpan2Periods = input . int ( 52 , minval = 1 , title = 'Senkou Span B Length' , group = 'Ichimoku' ) displacement = input . int ( 26 , minval = 1 , title = 'Displacement' , group = 'Ichimoku' ) useBarColor = input . bool ( true , title = 'Bar Colored by Chikou' , group = 'Ichimoku' ) usekijunChikouColors = input . bool ( false , title = \"Kijun Colored by Chikou\" , group = 'Ichimoku' ) useKumoMid = input . bool ( false , title = 'Kumo Middle' , group = 'Ichimoku' ) useCloseInstead = input . bool ( false , title = 'Use close Instead of Kijun for Chikou Crossover' , group = 'Ichimoku' ) useChikouCross = input . bool ( true , title = 'Chikou/Kijun Crossover' , group = 'Conditions' ) useKumoTwist = input . bool ( true , title = 'Kumo Twists' , group = 'Conditions' ) useKijunBounce = input . bool ( true , title = 'Kijun Bounce' , group = 'Conditions' ) useTrendDetector = input . bool ( false , title = 'Strong Trend' , group = 'Conditions' ) //Calculations donchian ( len ) => math . avg ( ta . lowest ( len ), ta . highest ( len )) tenkanSen = donchian ( tenkanPeriods ) kijunSen = donchian ( kijunPeriods ) senkouSpanA = math . avg ( tenkanSen , kijunSen ) senkouSpanB = donchian ( laggingSpan2Periods ) isChikouAboveKijun = useCloseInstead ? close > close [ 26 ] : close > kijunSen [ 26 ] chikouColor = isChikouAboveKijun and close > open ? color . rgb ( 0 , 255 , 0 ) : isChikouAboveKijun and close < open ? color . rgb ( 0 , 100 , 0 ) : ( not isChikouAboveKijun ) and close < open ? color . rgb ( 238 , 130 , 238 ) : ( not isChikouAboveKijun ) and close > open ? color . rgb ( 75 , 0 , 130 ) : na chikouColor2 = isChikouAboveKijun ? color . rgb ( 0 , 0 , 255 ) : color . rgb ( 255 , 0 , 0 ) //Plotting plot ( tenkanSen , linewidth = 1 , color = color . rgb ( 0 , 0 , 255 , 0 ), title = 'Tenkan-Sen' ) plot ( kijunSen , linewidth = 3 , color = usekijunChikouColors ? chikouColor2 : color . rgb ( 0 , 0 , 0 , 0 ), title = 'Kijun-Sen' ) plot ( close , linewidth = 3 , color = color . rgb ( 238 , 130 , 238 , 20 ), offset =- displacement + 1 , title = 'Chikou Span' ) p1 = plot ( senkouSpanA , linewidth = 3 , color = color . rgb ( 238 , 130 , 238 ), offset = displacement - 1 , title = 'Senkou Span A' ) p2 = plot ( senkouSpanB , linewidth = 3 , color = color . rgb ( 0 , 100 , 0 ), offset = displacement - 1 , title = 'Senkou Span B' ) kumoCloudFillUpColor = input . color ( color . rgb ( 189 , 209 , 186 , 70 ), title = \"Kumo Fill Up Color\" ) kumoCloudFillDownColor = input . color ( color . rgb ( 230 , 187 , 189 , 70 ), title = \"Kumo Fill Dowan Color\" ) fill ( p1 , p2 , color = senkouSpanA > senkouSpanB ? kumoCloudFillUpColor : kumoCloudFillDownColor ) //----------] End of Ichimoku Basic Script //----------[ Start of Williams Fractals // Define \"n\" as the number of periods and keep a minimum value of 2 for error handling. n = input . int ( defval = 9 , minval = 2 , title = 'Fractal Periods' , group = 'Williams Fractals' ) useFractals = input . bool ( true , title = \"Fractals\" , group = 'Williams Fractals' ) showFractalSupport = input ( false , title = 'show Fractal Support?' , group = 'Williams Fractals' ) // UpFractal bool upflagDownFrontier = true bool upflagUpFrontier0 = true bool upflagUpFrontier1 = true bool upflagUpFrontier2 = true bool upflagUpFrontier3 = true bool upflagUpFrontier4 = true for i = 1 to n by 1 upflagDownFrontier := upflagDownFrontier and high [ n - i ] < high [ n ] upflagUpFrontier0 := upflagUpFrontier0 and high [ n + i ] < high [ n ] upflagUpFrontier1 := upflagUpFrontier1 and high [ n + 1 ] <= high [ n ] and high [ n + i + 1 ] < high [ n ] upflagUpFrontier2 := upflagUpFrontier2 and high [ n + 1 ] <= high [ n ] and high [ n + 2 ] <= high [ n ] and high [ n + i + 2 ] < high [ n ] upflagUpFrontier3 := upflagUpFrontier3 and high [ n + 1 ] <= high [ n ] and high [ n + 2 ] <= high [ n ] and high [ n + 3 ] <= high [ n ] and high [ n + i + 3 ] < high [ n ] upflagUpFrontier4 := upflagUpFrontier4 and high [ n + 1 ] <= high [ n ] and high [ n + 2 ] <= high [ n ] and high [ n + 3 ] <= high [ n ] and high [ n + 4 ] <= high [ n ] and high [ n + i + 4 ] < high [ n ] upflagUpFrontier4 flagUpFrontier = upflagUpFrontier0 or upflagUpFrontier1 or upflagUpFrontier2 or upflagUpFrontier3 or upflagUpFrontier4 upFractal = upflagDownFrontier and flagUpFrontier // downFractal bool downflagDownFrontier = true bool downflagUpFrontier0 = true bool downflagUpFrontier1 = true bool downflagUpFrontier2 = true bool downflagUpFrontier3 = true bool downflagUpFrontier4 = true for i = 1 to n by 1 downflagDownFrontier := downflagDownFrontier and low [ n - i ] > low [ n ] downflagUpFrontier0 := downflagUpFrontier0 and low [ n + i ] > low [ n ] downflagUpFrontier1 := downflagUpFrontier1 and low [ n + 1 ] >= low [ n ] and low [ n + i + 1 ] > low [ n ] downflagUpFrontier2 := downflagUpFrontier2 and low [ n + 1 ] >= low [ n ] and low [ n + 2 ] >= low [ n ] and low [ n + i + 2 ] > low [ n ] downflagUpFrontier3 := downflagUpFrontier3 and low [ n + 1 ] >= low [ n ] and low [ n + 2 ] >= low [ n ] and low [ n + 3 ] >= low [ n ] and low [ n + i + 3 ] > low [ n ] downflagUpFrontier4 := downflagUpFrontier4 and low [ n + 1 ] >= low [ n ] and low [ n + 2 ] >= low [ n ] and low [ n + 3 ] >= low [ n ] and low [ n + 4 ] >= low [ n ] and low [ n + i + 4 ] > low [ n ] downflagUpFrontier4 flagDownFrontier = downflagUpFrontier0 or downflagUpFrontier1 or downflagUpFrontier2 or downflagUpFrontier3 or downflagUpFrontier4 downFractal = downflagDownFrontier and flagDownFrontier //Plotting lastUpFractal = ta . valuewhen ( upFractal , high [ n ], 0 ) lastDownFractal = ta . valuewhen ( downFractal , low [ n ], 0 ) pFractalUp = plot ( showFractalSupport ? lastUpFractal : na , linewidth = 1 , style = plot . style_circles , offset =- n , color = color . rgb ( 0 , 0 , 0 ), title = 'last Up Fractal' ) pFractalDown = plot ( showFractalSupport ? lastDownFractal : na , linewidth = 1 , style = plot . style_circles , offset =- n , color = color . rgb ( 0 , 0 , 0 ), title = 'last Down Fractal' ) plotshape ( useFractals and downFractal , style = shape . triangledown , location = location . belowbar , size = size . tiny , offset =- n , color = color . rgb ( 0 , 0 , 0 , 80 ), title = \"Down Fractal\" ) plotshape ( useFractals and upFractal , style = shape . triangleup , location = location . abovebar , size = size . tiny , offset =- n , color = color . rgb ( 0 , 0 , 0 , 80 ), title = \"Up Fractal\" ) //----------] End of Williams Fractals // Plot Buy and Sell Signals // plotshape(useFractals and downFractal, title=\"Sell Signal\", color=color.green, style=shape.labeldown, location=location.belowbar, text=\"Buy\", offset=-n) // plotshape(useFractals and upFractal, title=\"Buy Signal\", color=color.red, style=shape.labelup, location=location.abovebar, text=\"Sell\", offset=-n) // Alert Fractals alertcondition ( downFractal , title = \"Down Fractal Alert\" , message = \"Down Fractal Detected. Consider Buying.\" ) alertcondition ( upFractal , title = \"Up Fractal Alert\" , message = \"Up Fractal Detected. Consider Selling.\" ) //----------[ Ichimoku Logical Parameters isTenkanAboveKijun = tenkanSen > kijunSen isCloseAboveTenkan = close > tenkanSen isCloseAboveKijun = close > kijunSen kumoTwistUp = ta . crossover ( senkouSpanA , senkouSpanB ) kumoTwistDown = ta . crossunder ( senkouSpanA , senkouSpanB ) tenkanCrossUp = ta . crossover ( close , tenkanSen ) tenkanCrossDown = ta . crossunder ( close , tenkanSen ) kijunCrossUp = ta . crossover ( close , kijunSen ) kijunCrossDown = ta . crossunder ( close , kijunSen ) isChikouAbovePrice = close > close [ 26 ] kumoTop = math . max ( senkouSpanA , senkouSpanB ) kumoBottom = math . min ( senkouSpanA , senkouSpanB ) kumoMiddle = ( kumoTop + kumoBottom ) / 2 kumoGreen = senkouSpanA > senkouSpanB kumoRed = senkouSpanA < senkouSpanB isPriceAboveKumo = close > kumoTop [ 25 ] isPriceBelowKumo = close < kumoBottom [ 25 ] kumoBreakUp = ta . crossover ( close , kumoTop [ 25 ]) kumoBreakDown = ta . crossunder ( close , kumoBottom [ 25 ]) //----------] End of Ichimoku Logical Parameters //----------[ Start of Kumo Signals bullTrend = isChikouAboveKijun and isPriceAboveKumo and kumoGreen and isTenkanAboveKijun and isChikouAbovePrice bearTrend = ( not isChikouAboveKijun ) and isPriceBelowKumo and kumoRed and ( not isTenkanAboveKijun ) and ( not isChikouAbovePrice ) kijunBounceUp = bullTrend and low < kijunSen * 1.0001 and close > kijunSen * 1.0001 kijunBounceDown = bearTrend and high > kijunSen * 0.9999 and close < kijunSen * 0.9999 //Plotting plotshape ( useChikouCross and isChikouAboveKijun and ( not isChikouAboveKijun [ 1 ]), style = shape . diamond , location = location . abovebar , size = size . small , color = color . rgb ( 0 , 100 , 0 , 10 ), title = \"Chikou crossover Kijun\" ) plotshape ( useChikouCross and isChikouAboveKijun [ 1 ] and ( not isChikouAboveKijun ), style = shape . diamond , location = location . abovebar , size = size . small , color = color . rgb ( 75 , 0 , 130 , 10 ), title = \"Chikou crossunder Kijun\" ) plotshape ( useKumoTwist and kumoTwistUp , style = shape . xcross , location = location . abovebar , size = size . small , color = color . rgb ( 0 , 100 , 0 , 10 ), title = \"Kumo twist up\" ) plotshape ( useKumoTwist and kumoTwistDown , style = shape . xcross , location = location . abovebar , size = size . small , color = color . rgb ( 75 , 0 , 130 , 10 ), title = \"Kumo twist down\" ) plotshape ( useTrendDetector and bullTrend , style = shape . circle , location = location . abovebar , size = size . tiny , color = color . rgb ( 0 , 100 , 0 , 80 ), title = \"Strong Bulltrend\" ) plotshape ( useTrendDetector and bearTrend , style = shape . circle , location = location . abovebar , size = size . tiny , color = color . rgb ( 75 , 0 , 130 , 80 ), title = \"Strong Beartrend\" ) plot ( useKijunBounce and kijunBounceUp ? kijunSen : na , style = plot . style_circles , linewidth = 8 , color = color . rgb ( 0 , 0 , 0 , 10 ), title = \"Kijun Bounce Up\" ) plot ( useKijunBounce and kijunBounceDown ? kijunSen : na , style = plot . style_circles , linewidth = 8 , color = color . rgb ( 0 , 0 , 0 , 10 ), title = \"Kijun Bounce Down\" ) plot ( useKijunBounce and kijunBounceUp ? kijunSen : na , style = plot . style_circles , linewidth = 6 , color = color . rgb ( 255 , 215 , 0 , 10 ), title = \"Kijun Bounce Up\" ) plot ( useKijunBounce and kijunBounceDown ? kijunSen : na , style = plot . style_circles , linewidth = 6 , color = color . rgb ( 255 , 215 , 0 , 10 ), title = \"Kijun Bounce Down\" ) plot ( useKumoMid ? kumoMiddle : na , offset = displacement - 1 , linewidth = 3 , color = chikouColor2 , title = 'kumoMiddle' ) barcolor ( useBarColor ? ( chikouColor ) : na ) // Signals alertcondition ( isChikouAboveKijun and ( not isChikouAboveKijun [ 1 ]), title = \"Chikou Crossover\" ) alertcondition ( isChikouAboveKijun [ 1 ] and ( not isChikouAboveKijun ), title = \"Chikou Crossunder\" ) alertcondition ( kumoTwistUp , title = \"Kumo Twist Up\" ) alertcondition ( kumoTwistDown , title = \"Kumo Twist Down\" ) alertcondition ( kijunBounceUp , title = \"Kijun Bounce Up\" ) alertcondition ( kijunBounceDown , title = \"Kijun Bounce Down\" ) alertcondition ( bullTrend and ( not bullTrend [ 1 ]), title = \"Bulltrend Start\" ) alertcondition ( bearTrend and ( not bearTrend [ 1 ]), title = \"BearTrend Start\" ) //----------[ End of Kumo Signals JavaScript //@version=3 // Copyright (c) 2018-present, Alex Orekhov (everget) // On Balance Volume script may be freely distributed under the MIT license. study ( \"On Balance Volume with SMA\" , shorttitle = \"OBV\" , precision = 0 ) showSignal = input ( title = \"Show Signal ?\" , type = bool , defval = true ) signalType = input ( title = \"Signal Smoothing Type\" , defval = \"SMA\" , options = [ \"EMA\" , \"SMA\" ]) signalLength = input ( title = \"Signal Smoothing Length\" , type = integer , defval = 21 ) highlightCrossovers = input ( title = \"Highlight Crossovers ?\" , type = bool , defval = false ) applyFilling = input ( title = \"Apply Ribbon Filling ?\" , type = bool , defval = true ) src = input ( title = \"Source\" , type = source , defval = close ) obv = cum ( sign ( change ( src )) * volume ) signal = signalType == \"EMA\" ? ema ( obv , signalLength ) : sma ( obv , signalLength ) trendColor = obv > signal ? # 0 ebb23 : red obvColor = applyFilling ? trendColor : # 00 94 ff signalColor = applyFilling ? trendColor : # ff6a00 obvPlot = plot ( obv , title = \"OBV with SMA\" , linewidth = 2 , color = obvColor , transp = 0 ) signalPlot = plot ( showSignal ? signal : na , title = \"Signal\" , color = signalColor , transp = 0 ) transparent = color ( white , 100 ) fillColor = applyFilling ? trendColor : transparent fill ( obvPlot , signalPlot , color = fillColor , transp = 70 ) plotshape ( crossover ( obv , signal ) and highlightCrossovers ? obv : na , title = \"Crossover\" , location = location . absolute , style = shape . circle , size = size . tiny , color = green , transp = 0 ) plotshape ( crossunder ( obv , signal ) and highlightCrossovers ? obv : na , title = \"Crossunder\" , location = location . absolute , style = shape . circle , size = size . tiny , color = red , transp = 0 ) JavaScript //@version=5 indicator ( title = \"Ichimoku Cloud Momentum & Trend Indicator \u00abNoaTrader\u00bb\" , shorttitle = \"Ichi MTI\" , overlay = false ) var is_weighted = input . bool ( true , \"Use weighted value for conditions\" , group = \"Indicator\" ) conversionPeriods = input . int ( 9 , minval = 1 , title = \"Conversion Line Length\" , group = \"Ichimoku\" ) basePeriods = input . int ( 26 , minval = 1 , title = \"Base Line Length\" , group = \"Ichimoku\" ) laggingSpan2Periods = input . int ( 52 , minval = 1 , title = \"Leading Span B Length\" , group = \"Ichimoku\" ) displacement = input . int ( 26 , minval = 1 , title = \"Lagging Span\" , group = \"Ichimoku\" ) donchian ( len ) => math . avg ( ta . lowest ( len ), ta . highest ( len )) conversionLine = donchian ( conversionPeriods ) baseLine = donchian ( basePeriods ) leadLine1 = math . avg ( conversionLine , baseLine ) leadLine2 = donchian ( laggingSpan2Periods ) var trend = 1 lag = displacement - 1 cloud_top = math . max ( leadLine1 [ lag ], leadLine2 [ lag ]) cloud_bot = math . min ( leadLine1 [ lag ], leadLine2 [ lag ]) total = 0 // momentun over cloud is_mom_over_top = close > cloud_top [ lag ] is_mom_under_bot = close < cloud_bot [ lag ] var mom_w = 5 if is_mom_over_top total := total + ( is_weighted ? mom_w : 1 ) else if is_mom_under_bot total := total - ( is_weighted ? mom_w : 1 ) // price over cloud is_price_over_top = close > cloud_top is_price_under_bot = close < cloud_bot var price_could_w = 5 if is_price_over_top total := total + ( is_weighted ? price_could_w : 1 ) else if is_price_under_bot total := total - ( is_weighted ? price_could_w : 1 ) // cloud is green or red var cloud_w = 4 if leadLine1 > leadLine2 total := total + ( is_weighted ? cloud_w : 1 ) else total := total - ( is_weighted ? cloud_w : 1 ) var lines_w = 3 if conversionLine > baseLine total := total + ( is_weighted ? lines_w : 1 ) else total := total - ( is_weighted ? lines_w : 1 ) var prive_conv_w = 1 if close > conversionLine total := total + ( is_weighted ? prive_conv_w : 1 ) else total := total - ( is_weighted ? prive_conv_w : 1 ) var price_base_w = 2 if close > baseLine total := total + ( is_weighted ? price_base_w : 1 ) else total := total - ( is_weighted ? price_base_w : 1 ) w_sum = is_weighted ? price_base_w + prive_conv_w + lines_w + cloud_w + price_could_w + mom_w : 6 plot ( total * 100 / w_sum , \"Total\" , color . from_gradient ( total , - w_sum , w_sum , color . red , color . green ), style = plot . style_columns ) if is_mom_over_top and is_price_over_top trend := 1 else if is_mom_under_bot and is_price_under_bot trend := - 1 else trend := 0 plot ( trend * 100 , \"Trend\" , chart . bg_color == color . white ? color . black : color . white , style = plot . style_stepline_diamond , linewidth = 2 ) JavaScript // This source code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/ // \u00a9 LonesomeTheBlue //@version=4 study ( \"Support Resistance - Dynamic v2\" , \"SRv2\" , overlay = true ) prd = input ( defval = 10 , title = \"Pivot Period\" , minval = 4 , maxval = 30 , group = \"Setup\" ) ppsrc = input ( defval = 'High/Low' , title = \"Source\" , options = [ 'High/Low' , 'Close/Open' ], group = \"Setup\" ) maxnumpp = input ( defval = 20 , title = \" Maximum Number of Pivot\" , minval = 5 , maxval = 100 , group = \"Setup\" ) ChannelW = input ( defval = 10 , title = \"Maximum Channel Width %\" , minval = 1 , group = \"Setup\" ) maxnumsr = input ( defval = 5 , title = \" Maximum Number of S/R\" , minval = 1 , maxval = 10 , group = \"Setup\" ) min_strength = input ( defval = 2 , title = \" Minimum Strength\" , minval = 1 , maxval = 10 , group = \"Setup\" ) labelloc = input ( defval = 20 , title = \"Label Location\" , group = \"Colors\" , tooltip = \"Positive numbers reference future bars, negative numbers reference histical bars\" ) linestyle = input ( defval = 'Dashed' , title = \"Line Style\" , options = [ 'Solid' , 'Dotted' , 'Dashed' ], group = \"Colors\" ) linewidth = input ( defval = 2 , title = \"Line Width\" , minval = 1 , maxval = 4 , group = \"Colors\" ) resistancecolor = input ( defval = color . red , title = \"Resistance Color\" , type = input . color , group = \"Colors\" ) supportcolor = input ( defval = color . lime , title = \"Support Color\" , type = input . color , group = \"Colors\" ) showpp = input ( false , title = \"Show Point Points\" ) float src1 = ppsrc == 'High/Low' ? high : max ( close , open ) float src2 = ppsrc == 'High/Low' ? low : min ( close , open ) float ph = pivothigh ( src1 , prd , prd ) float pl = pivotlow ( src2 , prd , prd ) plotshape ( ph and showpp , text = \"H\" , style = shape . labeldown , color = na , textcolor = color . red , location = location . abovebar , offset = - prd ) plotshape ( pl and showpp , text = \"L\" , style = shape . labelup , color = na , textcolor = color . lime , location = location . belowbar , offset = - prd ) Lstyle = linestyle == 'Dashed' ? line . style_dashed : linestyle == 'Solid' ? line . style_solid : line . style_dotted //calculate maximum S/R channel zone width prdhighest = highest ( 300 ) prdlowest = lowest ( 300 ) cwidth = ( prdhighest - prdlowest ) * ChannelW / 100 var pivotvals = array . new_float ( 0 ) if ph or pl array . unshift ( pivotvals , ph ? ph : pl ) if array . size ( pivotvals ) > maxnumpp // limit the array size array . pop ( pivotvals ) get_sr_vals ( ind )=> float lo = array . get ( pivotvals , ind ) float hi = lo int numpp = 0 for y = 0 to array . size ( pivotvals ) - 1 float cpp = array . get ( pivotvals , y ) float wdth = cpp <= lo ? hi - cpp : cpp - lo if wdth <= cwidth // fits the max channel width? lo := cpp <= lo ? cpp : lo hi := cpp > lo ? cpp : hi numpp += 1 [ hi , lo , numpp ] var sr_up_level = array . new_float ( 0 ) var sr_dn_level = array . new_float ( 0 ) sr_strength = array . new_float ( 0 ) find_loc ( strength )=> ret = array . size ( sr_strength ) for i = ( ret > 0 ? array . size ( sr_strength ) - 1 : na ) to 0 if strength <= array . get ( sr_strength , i ) break ret := i ret check_sr ( hi , lo , strength )=> ret = true for i = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) //included? if array . get ( sr_up_level , i ) >= lo and array . get ( sr_up_level , i ) <= hi or array . get ( sr_dn_level , i ) >= lo and array . get ( sr_dn_level , i ) <= hi if strength >= array . get ( sr_strength , i ) array . remove ( sr_strength , i ) array . remove ( sr_up_level , i ) array . remove ( sr_dn_level , i ) ret else ret := false break ret var sr_lines = array . new_line ( 11 , na ) var sr_labels = array . new_label ( 11 , na ) for x = 1 to 10 rate = 100 * ( label . get_y ( array . get ( sr_labels , x )) - close ) / close label . set_text ( array . get ( sr_labels , x ), text = tostring ( label . get_y ( array . get ( sr_labels , x ))) + \"(\" + tostring ( rate , '#.##' ) + \"%)\" ) label . set_x ( array . get ( sr_labels , x ), x = bar_index + labelloc ) label . set_color ( array . get ( sr_labels , x ), color = label . get_y ( array . get ( sr_labels , x )) >= close ? color . red : color . lime ) label . set_textcolor ( array . get ( sr_labels , x ), textcolor = label . get_y ( array . get ( sr_labels , x )) >= close ? color . white : color . black ) label . set_style ( array . get ( sr_labels , x ), style = label . get_y ( array . get ( sr_labels , x )) >= close ? label . style_labeldown : label . style_labelup ) line . set_color ( array . get ( sr_lines , x ), color = line . get_y1 ( array . get ( sr_lines , x )) >= close ? resistancecolor : supportcolor ) if ph or pl //because of new calculation, remove old S/R levels array . clear ( sr_up_level ) array . clear ( sr_dn_level ) array . clear ( sr_strength ) //find S/R zones for x = 0 to array . size ( pivotvals ) - 1 [ hi , lo , strength ] = get_sr_vals ( x ) if check_sr ( hi , lo , strength ) loc = find_loc ( strength ) // if strength is in first maxnumsr sr then insert it to the arrays if loc < maxnumsr and strength >= min_strength array . insert ( sr_strength , loc , strength ) array . insert ( sr_up_level , loc , hi ) array . insert ( sr_dn_level , loc , lo ) // keep size of the arrays = 5 if array . size ( sr_strength ) > maxnumsr array . pop ( sr_strength ) array . pop ( sr_up_level ) array . pop ( sr_dn_level ) for x = 1 to 10 line . delete ( array . get ( sr_lines , x )) label . delete ( array . get ( sr_labels , x )) for x = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) float mid = round_to_mintick (( array . get ( sr_up_level , x ) + array . get ( sr_dn_level , x )) / 2 ) rate = 100 * ( mid - close ) / close array . set ( sr_labels , x + 1 , label . new ( x = bar_index + labelloc , y = mid , text = tostring ( mid ) + \"(\" + tostring ( rate , '#.##' ) + \"%)\" , color = mid >= close ? color . red : color . lime , textcolor = mid >= close ? color . white : color . black , style = mid >= close ? label . style_labeldown : label . style_labelup )) array . set ( sr_lines , x + 1 , line . new ( x1 = bar_index , y1 = mid , x2 = bar_index - 1 , y2 = mid , extend = extend . both , color = mid >= close ? resistancecolor : supportcolor , style = Lstyle , width = linewidth )) f_crossed_over ()=> ret = false for x = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) float mid = round_to_mintick (( array . get ( sr_up_level , x ) + array . get ( sr_dn_level , x )) / 2 ) if close [ 1 ] <= mid and close > mid ret := true ret f_crossed_under ()=> ret = false for x = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) float mid = round_to_mintick (( array . get ( sr_up_level , x ) + array . get ( sr_dn_level , x )) / 2 ) if close [ 1 ] >= mid and close < mid ret := true ret alertcondition ( f_crossed_over (), title = 'Resistance Broken' , message = 'Resistance Broken' ) alertcondition ( f_crossed_under (), title = 'Support Broken' , message = 'Support Broken' )","title":"Ichimoku Cloud Master"},{"location":"trading/ichimoku_cloud/#ichimoku-cloud-master","text":"Ichimoku Cloud Master aims to provide the ichimoku trader with easy alert functionality to not miss out on valuable trade setups. The key purpose of this script is to better visualise crucial moments in Ichimoku trading. These alerts should not be used for botting in my opinion as they always need a human to confirm the ichimoku market structure. For example, is the Kijun-Sen flat and too far away from price? A good ichimoku trader will not enter at such a point in time.","title":"Ichimoku Cloud Master"},{"location":"trading/ichimoku_cloud/#explanation-of-script","text":"Chikou(lagging span): pink line, this is price plotted 26 bars ago. People ignore the power of this it is crucial to see how chikou behaves towards past price action as seen in the chart below where we got an entry at red arrow because chikou bounced from past fractal bottom. Kijun-Sen(base line): Black line or color coded line. This is the equilibrium of last 26 candles. To me this is the most important line in the system as it attracts price. Kijun = (Highest high of 26 periods + Lowest low of 26 periods) \u00f7 2 Tenkan-Sen(conversion line): Blue line. This is the equilibrium of last 9 candles. In a strong uptrend price stays above this line. Tenkan = (Highest high of 9 periods + Lowest low of 9 periods) \u00f7 2 Senkou A (Leading span A)= Pink cloud line, this is the average of the 2 components projected 26 bars in the future. Senkou A = (Tenkan + Kijun) \u00f7 2 Senkou B (Leading span B) = Green cloud line, this is the 52 day equilibrium projected 26 bars in the future. Senkou B = (Highest high of prior 52 periods + Lowest low of prior 52 periods) \u00f7 2 Notice how the distance between Chikou and the cloud is also 52 bars. This is all part of Hosoda's numbers which I am not going to explain here. Fractals: These are the black triangles you find at key turning point. If you want to know how they work reseach williams fractals. I've used fractals with a period of 9 as it is an ichimoku number. These fractals are useful when working with ichimoku wave theory. Again I will not explain that here but in further education Fractal Support: Ability to extend lines from the fractals which can be used as an entry/exit mechanism in your trading. For example wait for tenkan to cross kijun and then enter on fractal breakout.","title":"Explanation of script:"},{"location":"trading/ichimoku_cloud/#signals","text":"Crossing of Chikou (lagging span) with past Kijun-Sen: this will color code the Bars / Kijun-Sen (you can turn this off in options) The script also has a signal for this, this will be the green and purple diamonds. Where green is bullish and purple is bearish. wy is this important? When current price plotted 26 candles back (chikou) crosses over the past equilibrium (kijun-sen) this usualy means price has moved past resistance levels where sellers come in. This indicates a switch in market structure and price is bullish from this point, this is the same in the other direction. Kumo Twist: when the kumo cloud (future) has a crossover from for example green to red (bull to bear). The script plots these using the colored cross symbols as seen in the picture above. A chikou cross + a Kumo twist at same bar of next to eachother below the cloud can be a great entry sign: this would be an entry after cross in the chart above. Kijun Bounce: when in an uptrend the price retraces back to Kijun-Sen and starts to go back up. These are marked by the yellow circles as seen in chart below: low below Kijun-Sen and close above it trong Trend: when Tenkan is above Kijun, price above cloud, future cloud green, chikou above close, chikou above Kijun we establish a strong bullish trend. For bearish the exact opposite. The script has a function to send an alert at the start of such trends and to plot them with small colored circles above the bars.","title":"Signals:"},{"location":"trading/ichimoku_cloud/#customisation","text":"I've added options to disable specific aspects of the indicator for those traders who do not want to use all aspects of the indicator. In the customisation tab I've given each part a clear title so you can use your own colors/shapes.","title":"Customisation:"},{"location":"trading/ichimoku_cloud/#the-perfect-entry","text":"Further info: Look into my education pane, I will be adding education in the future. The chance of me making a more advanced version of the script including line forecasting etc is rather high so watch out for that. For those who want to master this system I recommend reading the book: How to make money with the ichimoku system by Balkrishna M. Sadekar Or the originals books by Hosoda the inventor of Ichimoku if you can get your hands on them and can read Japanese. Almost all info about the ichimoku system you find on the internet will lose you money because they reduce the system to simple signals that do not generate money. I will be providing educational material on tradingview using this indicator. JavaScript //@version=5 indicator ( title = 'Ichimoku Cloud Master' , shorttitle = 'Ichimoku Master' , overlay = true , timeframe = \"\" , timeframe_gaps = false ) //----------[ Ichimoku Basic Script //inputs tenkanPeriods = input . int ( 9 , minval = 1 , title = 'Tenkan-Sen Length' , group = 'Ichimoku' ) kijunPeriods = input . int ( 26 , minval = 1 , title = 'Kijun-Sen Length' , group = 'Ichimoku' ) laggingSpan2Periods = input . int ( 52 , minval = 1 , title = 'Senkou Span B Length' , group = 'Ichimoku' ) displacement = input . int ( 26 , minval = 1 , title = 'Displacement' , group = 'Ichimoku' ) useBarColor = input . bool ( true , title = 'Bar Colored by Chikou' , group = 'Ichimoku' ) usekijunChikouColors = input . bool ( false , title = \"Kijun Colored by Chikou\" , group = 'Ichimoku' ) useKumoMid = input . bool ( false , title = 'Kumo Middle' , group = 'Ichimoku' ) useCloseInstead = input . bool ( false , title = 'Use close Instead of Kijun for Chikou Crossover' , group = 'Ichimoku' ) useChikouCross = input . bool ( true , title = 'Chikou/Kijun Crossover' , group = 'Conditions' ) useKumoTwist = input . bool ( true , title = 'Kumo Twists' , group = 'Conditions' ) useKijunBounce = input . bool ( true , title = 'Kijun Bounce' , group = 'Conditions' ) useTrendDetector = input . bool ( false , title = 'Strong Trend' , group = 'Conditions' ) //Calculations donchian ( len ) => math . avg ( ta . lowest ( len ), ta . highest ( len )) tenkanSen = donchian ( tenkanPeriods ) kijunSen = donchian ( kijunPeriods ) senkouSpanA = math . avg ( tenkanSen , kijunSen ) senkouSpanB = donchian ( laggingSpan2Periods ) isChikouAboveKijun = useCloseInstead ? close > close [ 26 ] : close > kijunSen [ 26 ] chikouColor = isChikouAboveKijun and close > open ? color . rgb ( 0 , 255 , 0 ) : isChikouAboveKijun and close < open ? color . rgb ( 0 , 100 , 0 ) : ( not isChikouAboveKijun ) and close < open ? color . rgb ( 238 , 130 , 238 ) : ( not isChikouAboveKijun ) and close > open ? color . rgb ( 75 , 0 , 130 ) : na chikouColor2 = isChikouAboveKijun ? color . rgb ( 0 , 0 , 255 ) : color . rgb ( 255 , 0 , 0 ) //Plotting plot ( tenkanSen , linewidth = 1 , color = color . rgb ( 0 , 0 , 255 , 0 ), title = 'Tenkan-Sen' ) plot ( kijunSen , linewidth = 3 , color = usekijunChikouColors ? chikouColor2 : color . rgb ( 0 , 0 , 0 , 0 ), title = 'Kijun-Sen' ) plot ( close , linewidth = 3 , color = color . rgb ( 238 , 130 , 238 , 20 ), offset =- displacement + 1 , title = 'Chikou Span' ) p1 = plot ( senkouSpanA , linewidth = 3 , color = color . rgb ( 238 , 130 , 238 ), offset = displacement - 1 , title = 'Senkou Span A' ) p2 = plot ( senkouSpanB , linewidth = 3 , color = color . rgb ( 0 , 100 , 0 ), offset = displacement - 1 , title = 'Senkou Span B' ) kumoCloudFillUpColor = input . color ( color . rgb ( 189 , 209 , 186 , 70 ), title = \"Kumo Fill Up Color\" ) kumoCloudFillDownColor = input . color ( color . rgb ( 230 , 187 , 189 , 70 ), title = \"Kumo Fill Dowan Color\" ) fill ( p1 , p2 , color = senkouSpanA > senkouSpanB ? kumoCloudFillUpColor : kumoCloudFillDownColor ) //----------] End of Ichimoku Basic Script //----------[ Start of Williams Fractals // Define \"n\" as the number of periods and keep a minimum value of 2 for error handling. n = input . int ( defval = 9 , minval = 2 , title = 'Fractal Periods' , group = 'Williams Fractals' ) useFractals = input . bool ( true , title = \"Fractals\" , group = 'Williams Fractals' ) showFractalSupport = input ( false , title = 'show Fractal Support?' , group = 'Williams Fractals' ) // UpFractal bool upflagDownFrontier = true bool upflagUpFrontier0 = true bool upflagUpFrontier1 = true bool upflagUpFrontier2 = true bool upflagUpFrontier3 = true bool upflagUpFrontier4 = true for i = 1 to n by 1 upflagDownFrontier := upflagDownFrontier and high [ n - i ] < high [ n ] upflagUpFrontier0 := upflagUpFrontier0 and high [ n + i ] < high [ n ] upflagUpFrontier1 := upflagUpFrontier1 and high [ n + 1 ] <= high [ n ] and high [ n + i + 1 ] < high [ n ] upflagUpFrontier2 := upflagUpFrontier2 and high [ n + 1 ] <= high [ n ] and high [ n + 2 ] <= high [ n ] and high [ n + i + 2 ] < high [ n ] upflagUpFrontier3 := upflagUpFrontier3 and high [ n + 1 ] <= high [ n ] and high [ n + 2 ] <= high [ n ] and high [ n + 3 ] <= high [ n ] and high [ n + i + 3 ] < high [ n ] upflagUpFrontier4 := upflagUpFrontier4 and high [ n + 1 ] <= high [ n ] and high [ n + 2 ] <= high [ n ] and high [ n + 3 ] <= high [ n ] and high [ n + 4 ] <= high [ n ] and high [ n + i + 4 ] < high [ n ] upflagUpFrontier4 flagUpFrontier = upflagUpFrontier0 or upflagUpFrontier1 or upflagUpFrontier2 or upflagUpFrontier3 or upflagUpFrontier4 upFractal = upflagDownFrontier and flagUpFrontier // downFractal bool downflagDownFrontier = true bool downflagUpFrontier0 = true bool downflagUpFrontier1 = true bool downflagUpFrontier2 = true bool downflagUpFrontier3 = true bool downflagUpFrontier4 = true for i = 1 to n by 1 downflagDownFrontier := downflagDownFrontier and low [ n - i ] > low [ n ] downflagUpFrontier0 := downflagUpFrontier0 and low [ n + i ] > low [ n ] downflagUpFrontier1 := downflagUpFrontier1 and low [ n + 1 ] >= low [ n ] and low [ n + i + 1 ] > low [ n ] downflagUpFrontier2 := downflagUpFrontier2 and low [ n + 1 ] >= low [ n ] and low [ n + 2 ] >= low [ n ] and low [ n + i + 2 ] > low [ n ] downflagUpFrontier3 := downflagUpFrontier3 and low [ n + 1 ] >= low [ n ] and low [ n + 2 ] >= low [ n ] and low [ n + 3 ] >= low [ n ] and low [ n + i + 3 ] > low [ n ] downflagUpFrontier4 := downflagUpFrontier4 and low [ n + 1 ] >= low [ n ] and low [ n + 2 ] >= low [ n ] and low [ n + 3 ] >= low [ n ] and low [ n + 4 ] >= low [ n ] and low [ n + i + 4 ] > low [ n ] downflagUpFrontier4 flagDownFrontier = downflagUpFrontier0 or downflagUpFrontier1 or downflagUpFrontier2 or downflagUpFrontier3 or downflagUpFrontier4 downFractal = downflagDownFrontier and flagDownFrontier //Plotting lastUpFractal = ta . valuewhen ( upFractal , high [ n ], 0 ) lastDownFractal = ta . valuewhen ( downFractal , low [ n ], 0 ) pFractalUp = plot ( showFractalSupport ? lastUpFractal : na , linewidth = 1 , style = plot . style_circles , offset =- n , color = color . rgb ( 0 , 0 , 0 ), title = 'last Up Fractal' ) pFractalDown = plot ( showFractalSupport ? lastDownFractal : na , linewidth = 1 , style = plot . style_circles , offset =- n , color = color . rgb ( 0 , 0 , 0 ), title = 'last Down Fractal' ) plotshape ( useFractals and downFractal , style = shape . triangledown , location = location . belowbar , size = size . tiny , offset =- n , color = color . rgb ( 0 , 0 , 0 , 80 ), title = \"Down Fractal\" ) plotshape ( useFractals and upFractal , style = shape . triangleup , location = location . abovebar , size = size . tiny , offset =- n , color = color . rgb ( 0 , 0 , 0 , 80 ), title = \"Up Fractal\" ) //----------] End of Williams Fractals // Plot Buy and Sell Signals // plotshape(useFractals and downFractal, title=\"Sell Signal\", color=color.green, style=shape.labeldown, location=location.belowbar, text=\"Buy\", offset=-n) // plotshape(useFractals and upFractal, title=\"Buy Signal\", color=color.red, style=shape.labelup, location=location.abovebar, text=\"Sell\", offset=-n) // Alert Fractals alertcondition ( downFractal , title = \"Down Fractal Alert\" , message = \"Down Fractal Detected. Consider Buying.\" ) alertcondition ( upFractal , title = \"Up Fractal Alert\" , message = \"Up Fractal Detected. Consider Selling.\" ) //----------[ Ichimoku Logical Parameters isTenkanAboveKijun = tenkanSen > kijunSen isCloseAboveTenkan = close > tenkanSen isCloseAboveKijun = close > kijunSen kumoTwistUp = ta . crossover ( senkouSpanA , senkouSpanB ) kumoTwistDown = ta . crossunder ( senkouSpanA , senkouSpanB ) tenkanCrossUp = ta . crossover ( close , tenkanSen ) tenkanCrossDown = ta . crossunder ( close , tenkanSen ) kijunCrossUp = ta . crossover ( close , kijunSen ) kijunCrossDown = ta . crossunder ( close , kijunSen ) isChikouAbovePrice = close > close [ 26 ] kumoTop = math . max ( senkouSpanA , senkouSpanB ) kumoBottom = math . min ( senkouSpanA , senkouSpanB ) kumoMiddle = ( kumoTop + kumoBottom ) / 2 kumoGreen = senkouSpanA > senkouSpanB kumoRed = senkouSpanA < senkouSpanB isPriceAboveKumo = close > kumoTop [ 25 ] isPriceBelowKumo = close < kumoBottom [ 25 ] kumoBreakUp = ta . crossover ( close , kumoTop [ 25 ]) kumoBreakDown = ta . crossunder ( close , kumoBottom [ 25 ]) //----------] End of Ichimoku Logical Parameters //----------[ Start of Kumo Signals bullTrend = isChikouAboveKijun and isPriceAboveKumo and kumoGreen and isTenkanAboveKijun and isChikouAbovePrice bearTrend = ( not isChikouAboveKijun ) and isPriceBelowKumo and kumoRed and ( not isTenkanAboveKijun ) and ( not isChikouAbovePrice ) kijunBounceUp = bullTrend and low < kijunSen * 1.0001 and close > kijunSen * 1.0001 kijunBounceDown = bearTrend and high > kijunSen * 0.9999 and close < kijunSen * 0.9999 //Plotting plotshape ( useChikouCross and isChikouAboveKijun and ( not isChikouAboveKijun [ 1 ]), style = shape . diamond , location = location . abovebar , size = size . small , color = color . rgb ( 0 , 100 , 0 , 10 ), title = \"Chikou crossover Kijun\" ) plotshape ( useChikouCross and isChikouAboveKijun [ 1 ] and ( not isChikouAboveKijun ), style = shape . diamond , location = location . abovebar , size = size . small , color = color . rgb ( 75 , 0 , 130 , 10 ), title = \"Chikou crossunder Kijun\" ) plotshape ( useKumoTwist and kumoTwistUp , style = shape . xcross , location = location . abovebar , size = size . small , color = color . rgb ( 0 , 100 , 0 , 10 ), title = \"Kumo twist up\" ) plotshape ( useKumoTwist and kumoTwistDown , style = shape . xcross , location = location . abovebar , size = size . small , color = color . rgb ( 75 , 0 , 130 , 10 ), title = \"Kumo twist down\" ) plotshape ( useTrendDetector and bullTrend , style = shape . circle , location = location . abovebar , size = size . tiny , color = color . rgb ( 0 , 100 , 0 , 80 ), title = \"Strong Bulltrend\" ) plotshape ( useTrendDetector and bearTrend , style = shape . circle , location = location . abovebar , size = size . tiny , color = color . rgb ( 75 , 0 , 130 , 80 ), title = \"Strong Beartrend\" ) plot ( useKijunBounce and kijunBounceUp ? kijunSen : na , style = plot . style_circles , linewidth = 8 , color = color . rgb ( 0 , 0 , 0 , 10 ), title = \"Kijun Bounce Up\" ) plot ( useKijunBounce and kijunBounceDown ? kijunSen : na , style = plot . style_circles , linewidth = 8 , color = color . rgb ( 0 , 0 , 0 , 10 ), title = \"Kijun Bounce Down\" ) plot ( useKijunBounce and kijunBounceUp ? kijunSen : na , style = plot . style_circles , linewidth = 6 , color = color . rgb ( 255 , 215 , 0 , 10 ), title = \"Kijun Bounce Up\" ) plot ( useKijunBounce and kijunBounceDown ? kijunSen : na , style = plot . style_circles , linewidth = 6 , color = color . rgb ( 255 , 215 , 0 , 10 ), title = \"Kijun Bounce Down\" ) plot ( useKumoMid ? kumoMiddle : na , offset = displacement - 1 , linewidth = 3 , color = chikouColor2 , title = 'kumoMiddle' ) barcolor ( useBarColor ? ( chikouColor ) : na ) // Signals alertcondition ( isChikouAboveKijun and ( not isChikouAboveKijun [ 1 ]), title = \"Chikou Crossover\" ) alertcondition ( isChikouAboveKijun [ 1 ] and ( not isChikouAboveKijun ), title = \"Chikou Crossunder\" ) alertcondition ( kumoTwistUp , title = \"Kumo Twist Up\" ) alertcondition ( kumoTwistDown , title = \"Kumo Twist Down\" ) alertcondition ( kijunBounceUp , title = \"Kijun Bounce Up\" ) alertcondition ( kijunBounceDown , title = \"Kijun Bounce Down\" ) alertcondition ( bullTrend and ( not bullTrend [ 1 ]), title = \"Bulltrend Start\" ) alertcondition ( bearTrend and ( not bearTrend [ 1 ]), title = \"BearTrend Start\" ) //----------[ End of Kumo Signals JavaScript //@version=3 // Copyright (c) 2018-present, Alex Orekhov (everget) // On Balance Volume script may be freely distributed under the MIT license. study ( \"On Balance Volume with SMA\" , shorttitle = \"OBV\" , precision = 0 ) showSignal = input ( title = \"Show Signal ?\" , type = bool , defval = true ) signalType = input ( title = \"Signal Smoothing Type\" , defval = \"SMA\" , options = [ \"EMA\" , \"SMA\" ]) signalLength = input ( title = \"Signal Smoothing Length\" , type = integer , defval = 21 ) highlightCrossovers = input ( title = \"Highlight Crossovers ?\" , type = bool , defval = false ) applyFilling = input ( title = \"Apply Ribbon Filling ?\" , type = bool , defval = true ) src = input ( title = \"Source\" , type = source , defval = close ) obv = cum ( sign ( change ( src )) * volume ) signal = signalType == \"EMA\" ? ema ( obv , signalLength ) : sma ( obv , signalLength ) trendColor = obv > signal ? # 0 ebb23 : red obvColor = applyFilling ? trendColor : # 00 94 ff signalColor = applyFilling ? trendColor : # ff6a00 obvPlot = plot ( obv , title = \"OBV with SMA\" , linewidth = 2 , color = obvColor , transp = 0 ) signalPlot = plot ( showSignal ? signal : na , title = \"Signal\" , color = signalColor , transp = 0 ) transparent = color ( white , 100 ) fillColor = applyFilling ? trendColor : transparent fill ( obvPlot , signalPlot , color = fillColor , transp = 70 ) plotshape ( crossover ( obv , signal ) and highlightCrossovers ? obv : na , title = \"Crossover\" , location = location . absolute , style = shape . circle , size = size . tiny , color = green , transp = 0 ) plotshape ( crossunder ( obv , signal ) and highlightCrossovers ? obv : na , title = \"Crossunder\" , location = location . absolute , style = shape . circle , size = size . tiny , color = red , transp = 0 ) JavaScript //@version=5 indicator ( title = \"Ichimoku Cloud Momentum & Trend Indicator \u00abNoaTrader\u00bb\" , shorttitle = \"Ichi MTI\" , overlay = false ) var is_weighted = input . bool ( true , \"Use weighted value for conditions\" , group = \"Indicator\" ) conversionPeriods = input . int ( 9 , minval = 1 , title = \"Conversion Line Length\" , group = \"Ichimoku\" ) basePeriods = input . int ( 26 , minval = 1 , title = \"Base Line Length\" , group = \"Ichimoku\" ) laggingSpan2Periods = input . int ( 52 , minval = 1 , title = \"Leading Span B Length\" , group = \"Ichimoku\" ) displacement = input . int ( 26 , minval = 1 , title = \"Lagging Span\" , group = \"Ichimoku\" ) donchian ( len ) => math . avg ( ta . lowest ( len ), ta . highest ( len )) conversionLine = donchian ( conversionPeriods ) baseLine = donchian ( basePeriods ) leadLine1 = math . avg ( conversionLine , baseLine ) leadLine2 = donchian ( laggingSpan2Periods ) var trend = 1 lag = displacement - 1 cloud_top = math . max ( leadLine1 [ lag ], leadLine2 [ lag ]) cloud_bot = math . min ( leadLine1 [ lag ], leadLine2 [ lag ]) total = 0 // momentun over cloud is_mom_over_top = close > cloud_top [ lag ] is_mom_under_bot = close < cloud_bot [ lag ] var mom_w = 5 if is_mom_over_top total := total + ( is_weighted ? mom_w : 1 ) else if is_mom_under_bot total := total - ( is_weighted ? mom_w : 1 ) // price over cloud is_price_over_top = close > cloud_top is_price_under_bot = close < cloud_bot var price_could_w = 5 if is_price_over_top total := total + ( is_weighted ? price_could_w : 1 ) else if is_price_under_bot total := total - ( is_weighted ? price_could_w : 1 ) // cloud is green or red var cloud_w = 4 if leadLine1 > leadLine2 total := total + ( is_weighted ? cloud_w : 1 ) else total := total - ( is_weighted ? cloud_w : 1 ) var lines_w = 3 if conversionLine > baseLine total := total + ( is_weighted ? lines_w : 1 ) else total := total - ( is_weighted ? lines_w : 1 ) var prive_conv_w = 1 if close > conversionLine total := total + ( is_weighted ? prive_conv_w : 1 ) else total := total - ( is_weighted ? prive_conv_w : 1 ) var price_base_w = 2 if close > baseLine total := total + ( is_weighted ? price_base_w : 1 ) else total := total - ( is_weighted ? price_base_w : 1 ) w_sum = is_weighted ? price_base_w + prive_conv_w + lines_w + cloud_w + price_could_w + mom_w : 6 plot ( total * 100 / w_sum , \"Total\" , color . from_gradient ( total , - w_sum , w_sum , color . red , color . green ), style = plot . style_columns ) if is_mom_over_top and is_price_over_top trend := 1 else if is_mom_under_bot and is_price_under_bot trend := - 1 else trend := 0 plot ( trend * 100 , \"Trend\" , chart . bg_color == color . white ? color . black : color . white , style = plot . style_stepline_diamond , linewidth = 2 ) JavaScript // This source code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/ // \u00a9 LonesomeTheBlue //@version=4 study ( \"Support Resistance - Dynamic v2\" , \"SRv2\" , overlay = true ) prd = input ( defval = 10 , title = \"Pivot Period\" , minval = 4 , maxval = 30 , group = \"Setup\" ) ppsrc = input ( defval = 'High/Low' , title = \"Source\" , options = [ 'High/Low' , 'Close/Open' ], group = \"Setup\" ) maxnumpp = input ( defval = 20 , title = \" Maximum Number of Pivot\" , minval = 5 , maxval = 100 , group = \"Setup\" ) ChannelW = input ( defval = 10 , title = \"Maximum Channel Width %\" , minval = 1 , group = \"Setup\" ) maxnumsr = input ( defval = 5 , title = \" Maximum Number of S/R\" , minval = 1 , maxval = 10 , group = \"Setup\" ) min_strength = input ( defval = 2 , title = \" Minimum Strength\" , minval = 1 , maxval = 10 , group = \"Setup\" ) labelloc = input ( defval = 20 , title = \"Label Location\" , group = \"Colors\" , tooltip = \"Positive numbers reference future bars, negative numbers reference histical bars\" ) linestyle = input ( defval = 'Dashed' , title = \"Line Style\" , options = [ 'Solid' , 'Dotted' , 'Dashed' ], group = \"Colors\" ) linewidth = input ( defval = 2 , title = \"Line Width\" , minval = 1 , maxval = 4 , group = \"Colors\" ) resistancecolor = input ( defval = color . red , title = \"Resistance Color\" , type = input . color , group = \"Colors\" ) supportcolor = input ( defval = color . lime , title = \"Support Color\" , type = input . color , group = \"Colors\" ) showpp = input ( false , title = \"Show Point Points\" ) float src1 = ppsrc == 'High/Low' ? high : max ( close , open ) float src2 = ppsrc == 'High/Low' ? low : min ( close , open ) float ph = pivothigh ( src1 , prd , prd ) float pl = pivotlow ( src2 , prd , prd ) plotshape ( ph and showpp , text = \"H\" , style = shape . labeldown , color = na , textcolor = color . red , location = location . abovebar , offset = - prd ) plotshape ( pl and showpp , text = \"L\" , style = shape . labelup , color = na , textcolor = color . lime , location = location . belowbar , offset = - prd ) Lstyle = linestyle == 'Dashed' ? line . style_dashed : linestyle == 'Solid' ? line . style_solid : line . style_dotted //calculate maximum S/R channel zone width prdhighest = highest ( 300 ) prdlowest = lowest ( 300 ) cwidth = ( prdhighest - prdlowest ) * ChannelW / 100 var pivotvals = array . new_float ( 0 ) if ph or pl array . unshift ( pivotvals , ph ? ph : pl ) if array . size ( pivotvals ) > maxnumpp // limit the array size array . pop ( pivotvals ) get_sr_vals ( ind )=> float lo = array . get ( pivotvals , ind ) float hi = lo int numpp = 0 for y = 0 to array . size ( pivotvals ) - 1 float cpp = array . get ( pivotvals , y ) float wdth = cpp <= lo ? hi - cpp : cpp - lo if wdth <= cwidth // fits the max channel width? lo := cpp <= lo ? cpp : lo hi := cpp > lo ? cpp : hi numpp += 1 [ hi , lo , numpp ] var sr_up_level = array . new_float ( 0 ) var sr_dn_level = array . new_float ( 0 ) sr_strength = array . new_float ( 0 ) find_loc ( strength )=> ret = array . size ( sr_strength ) for i = ( ret > 0 ? array . size ( sr_strength ) - 1 : na ) to 0 if strength <= array . get ( sr_strength , i ) break ret := i ret check_sr ( hi , lo , strength )=> ret = true for i = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) //included? if array . get ( sr_up_level , i ) >= lo and array . get ( sr_up_level , i ) <= hi or array . get ( sr_dn_level , i ) >= lo and array . get ( sr_dn_level , i ) <= hi if strength >= array . get ( sr_strength , i ) array . remove ( sr_strength , i ) array . remove ( sr_up_level , i ) array . remove ( sr_dn_level , i ) ret else ret := false break ret var sr_lines = array . new_line ( 11 , na ) var sr_labels = array . new_label ( 11 , na ) for x = 1 to 10 rate = 100 * ( label . get_y ( array . get ( sr_labels , x )) - close ) / close label . set_text ( array . get ( sr_labels , x ), text = tostring ( label . get_y ( array . get ( sr_labels , x ))) + \"(\" + tostring ( rate , '#.##' ) + \"%)\" ) label . set_x ( array . get ( sr_labels , x ), x = bar_index + labelloc ) label . set_color ( array . get ( sr_labels , x ), color = label . get_y ( array . get ( sr_labels , x )) >= close ? color . red : color . lime ) label . set_textcolor ( array . get ( sr_labels , x ), textcolor = label . get_y ( array . get ( sr_labels , x )) >= close ? color . white : color . black ) label . set_style ( array . get ( sr_labels , x ), style = label . get_y ( array . get ( sr_labels , x )) >= close ? label . style_labeldown : label . style_labelup ) line . set_color ( array . get ( sr_lines , x ), color = line . get_y1 ( array . get ( sr_lines , x )) >= close ? resistancecolor : supportcolor ) if ph or pl //because of new calculation, remove old S/R levels array . clear ( sr_up_level ) array . clear ( sr_dn_level ) array . clear ( sr_strength ) //find S/R zones for x = 0 to array . size ( pivotvals ) - 1 [ hi , lo , strength ] = get_sr_vals ( x ) if check_sr ( hi , lo , strength ) loc = find_loc ( strength ) // if strength is in first maxnumsr sr then insert it to the arrays if loc < maxnumsr and strength >= min_strength array . insert ( sr_strength , loc , strength ) array . insert ( sr_up_level , loc , hi ) array . insert ( sr_dn_level , loc , lo ) // keep size of the arrays = 5 if array . size ( sr_strength ) > maxnumsr array . pop ( sr_strength ) array . pop ( sr_up_level ) array . pop ( sr_dn_level ) for x = 1 to 10 line . delete ( array . get ( sr_lines , x )) label . delete ( array . get ( sr_labels , x )) for x = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) float mid = round_to_mintick (( array . get ( sr_up_level , x ) + array . get ( sr_dn_level , x )) / 2 ) rate = 100 * ( mid - close ) / close array . set ( sr_labels , x + 1 , label . new ( x = bar_index + labelloc , y = mid , text = tostring ( mid ) + \"(\" + tostring ( rate , '#.##' ) + \"%)\" , color = mid >= close ? color . red : color . lime , textcolor = mid >= close ? color . white : color . black , style = mid >= close ? label . style_labeldown : label . style_labelup )) array . set ( sr_lines , x + 1 , line . new ( x1 = bar_index , y1 = mid , x2 = bar_index - 1 , y2 = mid , extend = extend . both , color = mid >= close ? resistancecolor : supportcolor , style = Lstyle , width = linewidth )) f_crossed_over ()=> ret = false for x = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) float mid = round_to_mintick (( array . get ( sr_up_level , x ) + array . get ( sr_dn_level , x )) / 2 ) if close [ 1 ] <= mid and close > mid ret := true ret f_crossed_under ()=> ret = false for x = 0 to ( array . size ( sr_up_level ) > 0 ? array . size ( sr_up_level ) - 1 : na ) float mid = round_to_mintick (( array . get ( sr_up_level , x ) + array . get ( sr_dn_level , x )) / 2 ) if close [ 1 ] >= mid and close < mid ret := true ret alertcondition ( f_crossed_over (), title = 'Resistance Broken' , message = 'Resistance Broken' ) alertcondition ( f_crossed_under (), title = 'Support Broken' , message = 'Support Broken' )","title":"The perfect entry?"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: git \u00b6 Create Git PAT token with limited access. Multiple remote git configuration for repository","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#git","text":"Create Git PAT token with limited access. Multiple remote git configuration for repository","title":"git"}]}